{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tr_J983cibq"
   },
   "source": [
    "توضیحات بخش ۲ و ۳\n",
    "\n",
    "سینا کاظمی ۹۶۱۰۶۰۱۱\n",
    "\n",
    "نیما جمالی ۹۶۱۰۵۶۶۱\n",
    "\n",
    "سپهر فعلی ۹۶۱۰۵۹۵۹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2UKCEqbLpzXj",
    "outputId": "c26c9714-2bb7-4350-d811-dfe34cc51ba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
      "Suggested packages:\n",
      "  webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin\n",
      "The following NEW packages will be installed:\n",
      "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
      "  chromium-codecs-ffmpeg-extra\n",
      "0 upgraded, 4 newly installed, 0 to remove and 15 not upgraded.\n",
      "Need to get 81.0 MB of archives.\n",
      "After this operation, 273 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 87.0.4280.66-0ubuntu0.18.04.1 [1,122 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 87.0.4280.66-0ubuntu0.18.04.1 [71.7 MB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 87.0.4280.66-0ubuntu0.18.04.1 [3,716 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 87.0.4280.66-0ubuntu0.18.04.1 [4,488 kB]\n",
      "Fetched 81.0 MB in 4s (19.1 MB/s)\n",
      "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
      "(Reading database ... 146442 files and directories currently installed.)\n",
      "Preparing to unpack .../chromium-codecs-ffmpeg-extra_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\n",
      "Unpacking chromium-codecs-ffmpeg-extra (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
      "Selecting previously unselected package chromium-browser.\n",
      "Preparing to unpack .../chromium-browser_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\n",
      "Unpacking chromium-browser (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
      "Selecting previously unselected package chromium-browser-l10n.\n",
      "Preparing to unpack .../chromium-browser-l10n_87.0.4280.66-0ubuntu0.18.04.1_all.deb ...\n",
      "Unpacking chromium-browser-l10n (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
      "Selecting previously unselected package chromium-chromedriver.\n",
      "Preparing to unpack .../chromium-chromedriver_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\n",
      "Unpacking chromium-chromedriver (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
      "Setting up chromium-codecs-ffmpeg-extra (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
      "Setting up chromium-browser (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
      "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
      "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
      "Setting up chromium-chromedriver (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
      "Setting up chromium-browser-l10n (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
      "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
      "Processing triggers for mime-support (3.60ubuntu1) ...\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
      "Collecting selenium\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
      "\u001b[K     |████████████████████████████████| 911kB 4.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
      "Installing collected packages: selenium\n",
      "Successfully installed selenium-3.141.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: use options instead of chrome_options\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "!apt install chromium-chromedriver\n",
    "!pip install selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.add_argument('--user-agent=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) ' + 'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\"')\n",
    "driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "\n",
    "\n",
    "urls = [\n",
    "    'https://academic.microsoft.com/paper/2981549002',\n",
    "    'https://academic.microsoft.com/paper/3105081694',\n",
    "    'https://academic.microsoft.com/paper/2950893734'\n",
    "]\n",
    "papers = []\n",
    "papers_id = []\n",
    "do_I_array = []\n",
    "\n",
    "class Paper:\n",
    "    def __init__(self,identifier):\n",
    "        self.identifier = identifier\n",
    "        self.title = None\n",
    "        self.abstract = None\n",
    "        self.date = None\n",
    "        self.authors = []\n",
    "        self.references = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGljRfdDdHsS"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "عملیات crawling در این فاز با استفاده از کتابخانه selenium انجام شده است.\n",
    "آپشن headless گذاشته شده تا بار پردازشی بر روی cpu , gpu کاهش یابد.\n",
    "\n",
    "در ضمن با تعریف user_agent برای web_driver کروم از بلاک شدن درخواست توسط سایت academia جلوگیری کردیم.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bBTIOhNmp3e2"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def crawl(num_of_doc):\n",
    "    i = 0\n",
    "    while i < num_of_doc: \n",
    "        print(len(papers))\n",
    "        url = urls.pop(random.randrange(len(urls)))\n",
    "        paper_id = (url.split(\"/\"))[4]\n",
    "        if paper_id in papers_id:\n",
    "            continue\n",
    "        else:\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            try:\n",
    "                do_i = driver.find_element_by_xpath('//*[@id=\"mainArea\"]/router-view/div/div/div/div/a[2]')\n",
    "                if do_i.text in do_I_array:\n",
    "                    continue\n",
    "                else:\n",
    "                    do_I_array.append(do_i.text)\n",
    "            except:\n",
    "                pass\n",
    "            i += 1\n",
    "            try:\n",
    "                papers_id.append(paper_id)\n",
    "                paper = Paper(paper_id)\n",
    "                title = driver.find_element_by_xpath('//*[@id=\"mainArea\"]/router-view/div/div/div/div/h1')\n",
    "                paper.title = title.text\n",
    "                abstract = driver.find_element_by_xpath('//*[@id=\"mainArea\"]/router-view/div/div/div/div/p')\n",
    "                paper.abstract = abstract.text\n",
    "                date = driver.find_element_by_xpath('//*[@id=\"mainArea\"]/router-view/div/div/div/div/a/span[1]')\n",
    "                paper.date = date.text\n",
    "                authors = driver.find_element_by_xpath('//*[@id=\"mainArea\"]/router-view/div/div/div/div/ma-author-string-collection/div/div')\n",
    "                paper.authors = authors.text.split(\",\")\n",
    "            except:\n",
    "                i -= 1\n",
    "                continue\n",
    "            for x in range(1,11):\n",
    "                try:\n",
    "                    ref = driver.find_element_by_xpath(f'//*[@id=\"mainArea\"]/router-view/router-view/ma-edp-serp/div/div[2]/div/compose/div/div[2]/ma-card[{x}]/div/compose/div/div[1]/a[1]')\n",
    "                    ref_url = ref.get_attribute(\"href\")\n",
    "                    urls.append(ref_url)\n",
    "                    ref_id = (ref_url.split(\"/\"))[4]\n",
    "                    paper.references.append(ref_id)\n",
    "                except:\n",
    "                    break\n",
    "            papers.append(paper.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUC7y46yflJi"
   },
   "outputs": [],
   "source": [
    "<div dir=\"rtl\">\n",
    "سپس \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHf-zrlrfsNi"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "سپس یک کلاس paper ایجاد کرده تا هر مقاله ی بررسی شده را به عنوان یک آبجکت ایجاد کنیم و بعد از آن آرایه ای از آبجکت هارو با پسوند json. ذخیره سازی کنیم.\n",
    "\n",
    "ابتدا با url هایی که در فایل start.txt وجود داشتند شروع کردیم و سپس برای کراول کردن هر دفعه یکی از url هایی که درون آرایه url ها وجود داشت را به طور رندوم برداشتیم و در صورت تکراری نبودن مقاله شروع به استخراج اطلاعات نمودیم.\n",
    "\n",
    "تکراری نبودن مقاله را هم با استفاده از paper_id مقالات که در url آنها آمده است بررسی کردیم و هم با استفاده از do_i که در برخی مقالات وجود داشت و در بعضی از مقالات این فیلد نبود که امکان تکراری اومدن آنها وجود دارد.\n",
    "\n",
    "ورودی تابع crawl تعداد مقالاتی است که میخواهیم بررسی نماییم.\n",
    "\n",
    "مشخصاتی که از هر مقاله میخواستیم دریافت نماییم را با استفاده از آدرس xpath فایل html استخراج کردیم.\n",
    "\n",
    "برای اینکه قواعد اخلاقی در crawling را رعایت کرده باشیم بعد از هر بررسی مقاله ۲ ثانیه صبر می کنیم و سپس به سراغ مقاله بعدی می رویم.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "4K5HkgjBreBH",
    "outputId": "1312342c-9606-402b-9167-89395e3ce5cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "1778\n",
      "1778\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1786\n",
      "1787\n",
      "1787\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1804\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1820\n",
      "1820\n",
      "1820\n",
      "1820\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1824\n",
      "1824\n",
      "1825\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1840\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1847\n",
      "1847\n",
      "1847\n",
      "1847\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1854\n",
      "1854\n",
      "1854\n",
      "1855\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1858\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1863\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1869\n",
      "1870\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1882\n",
      "1882\n",
      "1883\n",
      "1883\n",
      "1883\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1892\n",
      "1893\n",
      "1893\n",
      "1894\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1902\n",
      "1903\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1905\n",
      "1905\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1913\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1922\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1930\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1940\n",
      "1941\n",
      "1941\n",
      "1941\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1949\n",
      "1950\n",
      "1950\n",
      "1950\n",
      "1950\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1952\n",
      "1953\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1955\n",
      "1956\n",
      "1956\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1988\n",
      "1988\n",
      "1989\n",
      "1989\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2011\n",
      "2011\n",
      "2012\n",
      "2012\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2017\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2022\n",
      "2023\n",
      "2023\n",
      "2023\n",
      "2024\n",
      "2024\n",
      "2025\n",
      "2025\n",
      "2025\n",
      "2026\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2040\n",
      "2041\n",
      "2041\n",
      "2042\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2054\n",
      "2055\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2077\n",
      "2078\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2082\n",
      "2082\n",
      "2082\n",
      "2082\n",
      "2082\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2085\n",
      "2085\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2087\n",
      "2088\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2092\n",
      "2093\n",
      "2093\n",
      "2093\n",
      "2093\n",
      "2093\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2096\n",
      "2096\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2102\n",
      "2102\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2113\n",
      "2113\n",
      "2114\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2117\n",
      "2118\n",
      "2118\n",
      "2118\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2122\n",
      "2122\n",
      "2122\n",
      "2123\n",
      "2123\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2135\n",
      "2136\n",
      "2136\n",
      "2136\n",
      "2137\n",
      "2137\n",
      "2137\n",
      "2138\n",
      "2138\n",
      "2139\n",
      "2139\n",
      "2139\n",
      "2139\n",
      "2139\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2141\n",
      "2142\n",
      "2142\n",
      "2142\n",
      "2142\n",
      "2142\n",
      "2142\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2149\n",
      "2149\n",
      "2149\n",
      "2150\n",
      "2150\n",
      "2150\n",
      "2151\n",
      "2151\n",
      "2152\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2162\n",
      "2162\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "2171\n",
      "2171\n",
      "2172\n",
      "2173\n",
      "2174\n",
      "2174\n",
      "2175\n",
      "2175\n",
      "2176\n",
      "2176\n",
      "2176\n",
      "2177\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2180\n",
      "2180\n",
      "2181\n",
      "2182\n",
      "2182\n",
      "2182\n",
      "2182\n",
      "2183\n",
      "2184\n",
      "2185\n",
      "2186\n",
      "2187\n",
      "2188\n",
      "2189\n",
      "2190\n",
      "2191\n",
      "2191\n",
      "2192\n",
      "2193\n",
      "2194\n",
      "2195\n",
      "2195\n",
      "2195\n",
      "2196\n",
      "2197\n",
      "2198\n",
      "2198\n",
      "2199\n",
      "2200\n",
      "2201\n",
      "2202\n",
      "2202\n",
      "2203\n",
      "2203\n",
      "2203\n",
      "2204\n",
      "2205\n",
      "2206\n",
      "2207\n",
      "2207\n",
      "2207\n",
      "2208\n",
      "2209\n",
      "2209\n",
      "2209\n",
      "2210\n",
      "2211\n",
      "2211\n",
      "2212\n",
      "2212\n",
      "2212\n",
      "2213\n",
      "2214\n",
      "2214\n",
      "2214\n",
      "2215\n",
      "2216\n",
      "2217\n",
      "2218\n",
      "2219\n",
      "2220\n",
      "2221\n",
      "2221\n",
      "2222\n",
      "2222\n",
      "2223\n",
      "2223\n",
      "2223\n",
      "2223\n",
      "2223\n",
      "2224\n",
      "2225\n",
      "2226\n",
      "2227\n",
      "2228\n",
      "2228\n",
      "2229\n",
      "2230\n",
      "2230\n",
      "2230\n",
      "2231\n",
      "2232\n",
      "2233\n",
      "2233\n",
      "2233\n",
      "2234\n",
      "2234\n",
      "2234\n",
      "2235\n",
      "2236\n",
      "2236\n",
      "2236\n",
      "2236\n",
      "2237\n",
      "2238\n",
      "2239\n",
      "2239\n",
      "2239\n",
      "2240\n",
      "2241\n",
      "2241\n",
      "2241\n",
      "2241\n",
      "2241\n",
      "2242\n",
      "2242\n",
      "2242\n",
      "2243\n",
      "2243\n",
      "2243\n",
      "2243\n",
      "2244\n",
      "2244\n",
      "2245\n",
      "2245\n",
      "2246\n",
      "2247\n",
      "2248\n",
      "2248\n",
      "2249\n",
      "2250\n",
      "2251\n",
      "2252\n",
      "2253\n",
      "2254\n",
      "2255\n",
      "2255\n",
      "2256\n",
      "2257\n",
      "2258\n",
      "2259\n",
      "2260\n",
      "2260\n",
      "2260\n",
      "2260\n",
      "2261\n",
      "2262\n",
      "2263\n",
      "2263\n",
      "2263\n",
      "2263\n",
      "2263\n",
      "2264\n",
      "2265\n",
      "2266\n",
      "2267\n",
      "2268\n",
      "2269\n",
      "2270\n",
      "2271\n",
      "2272\n",
      "2273\n",
      "2274\n",
      "2275\n",
      "2276\n",
      "2277\n",
      "2278\n",
      "2279\n",
      "2280\n",
      "2280\n",
      "2281\n",
      "2282\n",
      "2283\n",
      "2283\n",
      "2284\n",
      "2284\n",
      "2284\n",
      "2285\n",
      "2286\n",
      "2287\n",
      "2288\n",
      "2289\n",
      "2289\n",
      "2289\n",
      "2290\n",
      "2291\n",
      "2291\n",
      "2291\n",
      "2292\n",
      "2292\n",
      "2293\n",
      "2294\n",
      "2295\n",
      "2296\n",
      "2297\n",
      "2297\n",
      "2297\n",
      "2298\n",
      "2299\n",
      "2300\n",
      "2301\n",
      "2302\n",
      "2302\n",
      "2303\n",
      "2304\n",
      "2305\n",
      "2306\n",
      "2306\n",
      "2306\n",
      "2307\n",
      "2308\n",
      "2309\n",
      "2310\n",
      "2310\n",
      "2310\n",
      "2311\n",
      "2312\n",
      "2313\n",
      "2314\n",
      "2314\n",
      "2315\n",
      "2315\n",
      "2316\n",
      "2317\n",
      "2318\n",
      "2319\n",
      "2320\n",
      "2321\n",
      "2321\n",
      "2321\n",
      "2321\n",
      "2322\n",
      "2323\n",
      "2324\n",
      "2325\n",
      "2326\n",
      "2327\n",
      "2327\n",
      "2328\n",
      "2328\n",
      "2328\n",
      "2329\n",
      "2329\n",
      "2329\n",
      "2329\n",
      "2329\n",
      "2329\n",
      "2330\n",
      "2331\n",
      "2332\n",
      "2333\n",
      "2333\n",
      "2333\n",
      "2334\n",
      "2334\n",
      "2334\n",
      "2335\n",
      "2335\n",
      "2336\n",
      "2337\n",
      "2337\n",
      "2338\n",
      "2339\n",
      "2339\n",
      "2340\n",
      "2341\n",
      "2342\n",
      "2343\n",
      "2344\n",
      "2345\n",
      "2346\n",
      "2347\n",
      "2348\n",
      "2348\n",
      "2349\n",
      "2350\n",
      "2351\n",
      "2351\n",
      "2352\n",
      "2353\n",
      "2353\n",
      "2353\n",
      "2353\n",
      "2354\n",
      "2355\n",
      "2355\n",
      "2356\n",
      "2356\n",
      "2357\n",
      "2358\n",
      "2359\n",
      "2360\n",
      "2361\n",
      "2361\n",
      "2362\n",
      "2363\n",
      "2364\n",
      "2365\n",
      "2366\n",
      "2367\n",
      "2367\n",
      "2368\n",
      "2368\n",
      "2369\n",
      "2369\n",
      "2370\n",
      "2371\n",
      "2372\n",
      "2373\n",
      "2373\n",
      "2373\n",
      "2373\n",
      "2374\n",
      "2374\n",
      "2374\n",
      "2374\n",
      "2374\n",
      "2375\n",
      "2376\n",
      "2377\n",
      "2378\n",
      "2378\n",
      "2379\n",
      "2380\n",
      "2381\n",
      "2381\n",
      "2381\n",
      "2382\n",
      "2383\n",
      "2383\n",
      "2384\n",
      "2385\n",
      "2386\n",
      "2386\n",
      "2387\n",
      "2387\n",
      "2388\n",
      "2388\n",
      "2388\n",
      "2388\n",
      "2388\n",
      "2389\n",
      "2389\n",
      "2390\n",
      "2391\n",
      "2391\n",
      "2392\n",
      "2392\n",
      "2392\n",
      "2393\n",
      "2394\n",
      "2394\n",
      "2395\n",
      "2396\n",
      "2396\n",
      "2397\n",
      "2397\n",
      "2398\n",
      "2399\n",
      "2399\n",
      "2399\n",
      "2399\n",
      "2399\n",
      "2400\n",
      "2401\n",
      "2402\n",
      "2403\n",
      "2404\n",
      "2404\n",
      "2405\n",
      "2406\n",
      "2406\n",
      "2407\n",
      "2408\n",
      "2409\n",
      "2409\n",
      "2410\n",
      "2411\n",
      "2412\n",
      "2412\n",
      "2413\n",
      "2414\n",
      "2415\n",
      "2416\n",
      "2417\n",
      "2417\n",
      "2417\n",
      "2417\n",
      "2418\n",
      "2419\n",
      "2420\n",
      "2421\n",
      "2421\n",
      "2422\n",
      "2422\n",
      "2423\n",
      "2423\n",
      "2424\n",
      "2425\n",
      "2425\n",
      "2425\n",
      "2426\n",
      "2427\n",
      "2427\n",
      "2427\n",
      "2428\n",
      "2429\n",
      "2430\n",
      "2430\n",
      "2431\n",
      "2432\n",
      "2432\n",
      "2433\n",
      "2433\n",
      "2434\n",
      "2435\n",
      "2436\n",
      "2437\n",
      "2437\n",
      "2437\n",
      "2437\n",
      "2437\n",
      "2437\n",
      "2438\n",
      "2439\n",
      "2439\n",
      "2439\n",
      "2440\n",
      "2441\n",
      "2442\n",
      "2443\n",
      "2444\n",
      "2445\n",
      "2446\n",
      "2446\n",
      "2447\n",
      "2448\n",
      "2449\n",
      "2449\n",
      "2450\n",
      "2451\n",
      "2452\n",
      "2453\n",
      "2454\n",
      "2455\n",
      "2456\n",
      "2457\n",
      "2457\n",
      "2457\n",
      "2457\n",
      "2457\n",
      "2457\n",
      "2457\n",
      "2458\n",
      "2459\n",
      "2460\n",
      "2460\n",
      "2461\n",
      "2462\n",
      "2462\n",
      "2463\n",
      "2464\n",
      "2465\n",
      "2465\n",
      "2465\n",
      "2465\n",
      "2466\n",
      "2467\n",
      "2468\n",
      "2468\n",
      "2468\n",
      "2469\n",
      "2470\n",
      "2471\n",
      "2471\n",
      "2472\n",
      "2473\n",
      "2474\n",
      "2475\n",
      "2476\n",
      "2477\n",
      "2477\n",
      "2477\n",
      "2477\n",
      "2477\n",
      "2477\n",
      "2478\n",
      "2479\n",
      "2480\n",
      "2481\n",
      "2482\n",
      "2483\n",
      "2483\n",
      "2484\n",
      "2484\n",
      "2485\n",
      "2486\n",
      "2487\n",
      "2487\n",
      "2488\n",
      "2489\n",
      "2490\n",
      "2490\n",
      "2490\n",
      "2491\n",
      "2491\n",
      "2492\n",
      "2492\n",
      "2492\n",
      "2492\n",
      "2493\n",
      "2493\n",
      "2494\n",
      "2495\n",
      "2496\n",
      "2497\n",
      "2498\n",
      "2499\n",
      "2499\n",
      "2499\n",
      "2500\n",
      "2501\n",
      "2502\n",
      "2503\n",
      "2503\n",
      "2504\n",
      "2505\n",
      "2505\n",
      "2505\n",
      "2506\n",
      "2506\n",
      "2507\n",
      "2507\n",
      "2507\n",
      "2508\n",
      "2508\n",
      "2509\n",
      "2510\n",
      "2511\n",
      "2512\n",
      "2513\n",
      "2514\n",
      "2515\n",
      "2516\n",
      "2517\n",
      "2518\n",
      "2518\n",
      "2518\n",
      "2519\n",
      "2519\n",
      "2520\n",
      "2520\n",
      "2520\n",
      "2521\n",
      "2522\n",
      "2522\n",
      "2522\n",
      "2522\n",
      "2523\n",
      "2524\n",
      "2525\n",
      "2525\n",
      "2526\n",
      "2527\n",
      "2527\n",
      "2528\n",
      "2529\n",
      "2530\n",
      "2530\n",
      "2531\n",
      "2531\n",
      "2532\n",
      "2533\n",
      "2534\n",
      "2535\n",
      "2535\n",
      "2536\n",
      "2537\n",
      "2538\n",
      "2539\n",
      "2540\n",
      "2541\n",
      "2542\n",
      "2542\n",
      "2543\n",
      "2544\n",
      "2544\n",
      "2545\n",
      "2546\n",
      "2547\n",
      "2548\n",
      "2549\n",
      "2550\n",
      "2551\n",
      "2551\n",
      "2552\n",
      "2552\n",
      "2553\n",
      "2554\n",
      "2554\n",
      "2555\n",
      "2556\n",
      "2557\n",
      "2557\n",
      "2558\n",
      "2559\n",
      "2560\n",
      "2561\n",
      "2562\n",
      "2562\n",
      "2562\n",
      "2562\n",
      "2562\n",
      "2563\n",
      "2563\n",
      "2564\n",
      "2565\n",
      "2566\n",
      "2567\n",
      "2568\n",
      "2569\n",
      "2569\n",
      "2569\n",
      "2569\n",
      "2569\n",
      "2570\n",
      "2571\n",
      "2572\n",
      "2573\n",
      "2573\n",
      "2574\n",
      "2574\n",
      "2575\n",
      "2575\n",
      "2576\n",
      "2577\n",
      "2578\n",
      "2578\n",
      "2578\n",
      "2578\n",
      "2579\n",
      "2580\n",
      "2581\n",
      "2582\n",
      "2583\n",
      "2584\n",
      "2585\n",
      "2586\n",
      "2586\n",
      "2587\n",
      "2588\n",
      "2589\n",
      "2590\n",
      "2590\n",
      "2591\n",
      "2592\n",
      "2593\n",
      "2593\n",
      "2594\n",
      "2595\n",
      "2596\n",
      "2596\n",
      "2597\n",
      "2598\n",
      "2599\n",
      "2599\n",
      "2600\n",
      "2600\n",
      "2601\n",
      "2601\n",
      "2601\n",
      "2602\n",
      "2603\n",
      "2604\n",
      "2605\n",
      "2606\n",
      "2607\n",
      "2607\n",
      "2608\n",
      "2609\n",
      "2609\n",
      "2610\n",
      "2611\n",
      "2612\n",
      "2612\n",
      "2613\n",
      "2614\n",
      "2615\n",
      "2615\n",
      "2616\n",
      "2617\n",
      "2618\n",
      "2618\n",
      "2618\n",
      "2619\n",
      "2620\n",
      "2620\n",
      "2620\n",
      "2621\n",
      "2622\n",
      "2623\n",
      "2624\n",
      "2625\n",
      "2626\n",
      "2627\n",
      "2628\n",
      "2628\n",
      "2629\n",
      "2629\n",
      "2630\n",
      "2631\n",
      "2632\n",
      "2632\n",
      "2632\n",
      "2633\n",
      "2633\n",
      "2633\n",
      "2633\n",
      "2633\n",
      "2633\n",
      "2633\n",
      "2634\n",
      "2635\n",
      "2636\n",
      "2637\n",
      "2637\n",
      "2638\n",
      "2639\n",
      "2640\n",
      "2641\n",
      "2642\n",
      "2643\n",
      "2644\n",
      "2645\n",
      "2646\n",
      "2647\n",
      "2647\n",
      "2648\n",
      "2649\n",
      "2650\n",
      "2651\n",
      "2652\n",
      "2653\n",
      "2653\n",
      "2653\n",
      "2653\n",
      "2653\n",
      "2654\n",
      "2654\n",
      "2655\n",
      "2655\n",
      "2656\n",
      "2657\n",
      "2657\n",
      "2658\n",
      "2659\n",
      "2659\n",
      "2660\n",
      "2661\n",
      "2662\n",
      "2662\n",
      "2662\n",
      "2662\n",
      "2663\n",
      "2664\n",
      "2665\n",
      "2666\n",
      "2667\n",
      "2668\n",
      "2669\n",
      "2670\n",
      "2670\n",
      "2671\n",
      "2671\n",
      "2671\n",
      "2672\n",
      "2673\n",
      "2673\n",
      "2673\n",
      "2674\n",
      "2675\n",
      "2675\n",
      "2675\n",
      "2675\n",
      "2676\n",
      "2677\n",
      "2678\n",
      "2679\n",
      "2680\n",
      "2681\n",
      "2682\n",
      "2683\n",
      "2683\n",
      "2683\n",
      "2684\n",
      "2685\n",
      "2686\n",
      "2687\n",
      "2687\n",
      "2688\n",
      "2689\n",
      "2690\n",
      "2691\n",
      "2692\n",
      "2693\n",
      "2694\n",
      "2695\n",
      "2696\n",
      "2697\n",
      "2698\n",
      "2699\n",
      "2700\n",
      "2701\n",
      "2701\n",
      "2702\n",
      "2703\n",
      "2703\n",
      "2704\n",
      "2705\n",
      "2706\n",
      "2707\n",
      "2708\n",
      "2709\n",
      "2710\n",
      "2710\n",
      "2711\n",
      "2712\n",
      "2713\n",
      "2714\n",
      "2714\n",
      "2714\n",
      "2714\n",
      "2715\n",
      "2715\n",
      "2716\n",
      "2716\n",
      "2717\n",
      "2717\n",
      "2718\n",
      "2718\n",
      "2719\n",
      "2720\n",
      "2721\n",
      "2722\n",
      "2722\n",
      "2723\n",
      "2723\n",
      "2724\n",
      "2724\n",
      "2725\n",
      "2726\n",
      "2726\n",
      "2726\n",
      "2727\n",
      "2727\n",
      "2728\n",
      "2729\n",
      "2729\n",
      "2730\n",
      "2731\n",
      "2732\n",
      "2732\n",
      "2732\n",
      "2733\n",
      "2734\n",
      "2735\n",
      "2736\n",
      "2737\n",
      "2738\n",
      "2739\n",
      "2740\n",
      "2740\n",
      "2740\n",
      "2740\n",
      "2741\n",
      "2742\n",
      "2742\n",
      "2743\n",
      "2744\n",
      "2745\n",
      "2745\n",
      "2745\n",
      "2746\n",
      "2747\n",
      "2748\n",
      "2749\n",
      "2750\n",
      "2751\n",
      "2751\n",
      "2752\n",
      "2753\n",
      "2754\n",
      "2755\n",
      "2756\n",
      "2757\n",
      "2758\n",
      "2759\n",
      "2760\n",
      "2761\n",
      "2761\n",
      "2761\n",
      "2761\n",
      "2761\n",
      "2762\n",
      "2763\n",
      "2764\n",
      "2764\n",
      "2765\n",
      "2766\n",
      "2766\n",
      "2767\n",
      "2768\n",
      "2769\n",
      "2770\n",
      "2770\n",
      "2771\n",
      "2772\n",
      "2772\n",
      "2773\n",
      "2774\n",
      "2775\n",
      "2775\n",
      "2776\n",
      "2777\n",
      "2777\n",
      "2778\n",
      "2779\n",
      "2779\n",
      "2779\n",
      "2779\n",
      "2780\n",
      "2780\n",
      "2781\n",
      "2781\n",
      "2781\n",
      "2781\n",
      "2781\n",
      "2781\n",
      "2781\n",
      "2781\n",
      "2781\n",
      "2781\n",
      "2782\n",
      "2783\n",
      "2784\n",
      "2785\n",
      "2785\n",
      "2785\n",
      "2785\n",
      "2785\n",
      "2786\n",
      "2786\n",
      "2787\n",
      "2788\n",
      "2788\n",
      "2789\n",
      "2790\n",
      "2791\n",
      "2792\n",
      "2792\n",
      "2792\n",
      "2793\n",
      "2793\n",
      "2794\n",
      "2795\n",
      "2796\n",
      "2797\n",
      "2797\n",
      "2798\n",
      "2799\n",
      "2800\n",
      "2801\n",
      "2801\n",
      "2801\n",
      "2801\n",
      "2801\n",
      "2801\n",
      "2802\n",
      "2803\n",
      "2804\n",
      "2805\n",
      "2805\n",
      "2806\n",
      "2807\n",
      "2808\n",
      "2809\n",
      "2810\n",
      "2810\n",
      "2811\n",
      "2812\n",
      "2813\n",
      "2813\n",
      "2813\n",
      "2814\n",
      "2814\n",
      "2815\n",
      "2816\n",
      "2817\n",
      "2818\n",
      "2819\n",
      "2820\n",
      "2821\n",
      "2822\n",
      "2822\n",
      "2823\n",
      "2823\n",
      "2823\n",
      "2823\n",
      "2823\n",
      "2823\n",
      "2823\n",
      "2824\n",
      "2825\n",
      "2826\n",
      "2827\n",
      "2827\n",
      "2827\n",
      "2828\n",
      "2828\n",
      "2828\n",
      "2828\n",
      "2828\n",
      "2828\n",
      "2828\n",
      "2828\n",
      "2829\n",
      "2830\n",
      "2831\n",
      "2832\n",
      "2833\n",
      "2833\n",
      "2834\n",
      "2835\n",
      "2836\n",
      "2837\n",
      "2838\n",
      "2839\n",
      "2840\n",
      "2841\n",
      "2842\n",
      "2843\n",
      "2844\n",
      "2845\n",
      "2846\n",
      "2847\n",
      "2847\n",
      "2848\n",
      "2849\n",
      "2850\n",
      "2850\n",
      "2850\n",
      "2850\n",
      "2850\n",
      "2850\n",
      "2851\n",
      "2851\n",
      "2852\n",
      "2852\n",
      "2853\n",
      "2853\n",
      "2854\n",
      "2855\n",
      "2856\n",
      "2857\n",
      "2858\n",
      "2859\n",
      "2860\n",
      "2861\n",
      "2862\n",
      "2863\n",
      "2864\n",
      "2865\n",
      "2865\n",
      "2866\n",
      "2866\n",
      "2866\n",
      "2867\n",
      "2868\n",
      "2869\n",
      "2869\n",
      "2870\n",
      "2871\n",
      "2872\n",
      "2872\n",
      "2873\n",
      "2873\n",
      "2874\n",
      "2874\n",
      "2875\n",
      "2876\n",
      "2877\n",
      "2877\n",
      "2878\n",
      "2879\n",
      "2879\n",
      "2880\n",
      "2880\n",
      "2880\n",
      "2880\n",
      "2881\n",
      "2881\n",
      "2882\n",
      "2883\n",
      "2883\n",
      "2883\n",
      "2883\n",
      "2883\n",
      "2884\n",
      "2885\n",
      "2886\n",
      "2887\n",
      "2888\n",
      "2888\n",
      "2889\n",
      "2890\n",
      "2890\n",
      "2890\n",
      "2890\n",
      "2891\n",
      "2892\n",
      "2893\n",
      "2893\n",
      "2893\n",
      "2894\n",
      "2895\n",
      "2896\n",
      "2897\n",
      "2898\n",
      "2898\n",
      "2899\n",
      "2900\n",
      "2901\n",
      "2902\n",
      "2903\n",
      "2904\n",
      "2905\n",
      "2906\n",
      "2906\n",
      "2906\n",
      "2907\n",
      "2908\n",
      "2908\n",
      "2908\n",
      "2908\n",
      "2909\n",
      "2910\n",
      "2911\n",
      "2912\n",
      "2913\n",
      "2914\n",
      "2915\n",
      "2916\n",
      "2916\n",
      "2917\n",
      "2918\n",
      "2919\n",
      "2920\n",
      "2920\n",
      "2921\n",
      "2922\n",
      "2923\n",
      "2924\n",
      "2925\n",
      "2926\n",
      "2926\n",
      "2927\n",
      "2928\n",
      "2928\n",
      "2929\n",
      "2930\n",
      "2930\n",
      "2930\n",
      "2931\n",
      "2932\n",
      "2932\n",
      "2933\n",
      "2933\n",
      "2933\n",
      "2934\n",
      "2935\n",
      "2936\n",
      "2937\n",
      "2938\n",
      "2938\n",
      "2938\n",
      "2938\n",
      "2938\n",
      "2939\n",
      "2940\n",
      "2941\n",
      "2942\n",
      "2943\n",
      "2944\n",
      "2945\n",
      "2946\n",
      "2947\n",
      "2948\n",
      "2949\n",
      "2949\n",
      "2950\n",
      "2951\n",
      "2952\n",
      "2953\n",
      "2954\n",
      "2955\n",
      "2956\n",
      "2957\n",
      "2957\n",
      "2957\n",
      "2957\n",
      "2957\n",
      "2958\n",
      "2958\n",
      "2959\n",
      "2959\n",
      "2959\n",
      "2960\n",
      "2960\n",
      "2961\n",
      "2961\n",
      "2962\n",
      "2963\n",
      "2963\n",
      "2964\n",
      "2965\n",
      "2965\n",
      "2966\n",
      "2967\n",
      "2968\n",
      "2969\n",
      "2970\n",
      "2971\n",
      "2972\n",
      "2973\n",
      "2973\n",
      "2974\n",
      "2975\n",
      "2976\n",
      "2977\n",
      "2977\n",
      "2977\n",
      "2977\n",
      "2978\n",
      "2979\n",
      "2980\n",
      "2981\n",
      "2982\n",
      "2983\n",
      "2984\n",
      "2985\n",
      "2986\n",
      "2987\n",
      "2987\n",
      "2988\n",
      "2988\n",
      "2989\n",
      "2990\n",
      "2991\n",
      "2992\n",
      "2993\n",
      "2994\n",
      "2994\n",
      "2995\n",
      "2996\n",
      "2997\n",
      "2998\n",
      "2998\n",
      "2999\n",
      "3000\n",
      "3001\n",
      "3001\n",
      "3001\n",
      "3002\n",
      "3003\n",
      "3004\n",
      "3005\n",
      "3005\n",
      "3005\n",
      "3006\n",
      "3007\n",
      "3007\n",
      "3007\n",
      "3008\n",
      "3008\n",
      "3008\n",
      "3008\n",
      "3008\n",
      "3009\n",
      "3010\n",
      "3011\n",
      "3012\n",
      "3012\n",
      "3012\n",
      "3012\n",
      "3012\n",
      "3013\n",
      "3014\n",
      "3015\n",
      "3016\n",
      "3017\n",
      "3017\n",
      "3018\n",
      "3019\n",
      "3020\n",
      "3020\n",
      "3020\n",
      "3020\n",
      "3020\n",
      "3020\n",
      "3021\n",
      "3021\n",
      "3022\n",
      "3023\n",
      "3023\n",
      "3024\n",
      "3025\n",
      "3026\n",
      "3027\n",
      "3027\n",
      "3028\n",
      "3029\n",
      "3030\n",
      "3030\n",
      "3031\n",
      "3032\n",
      "3033\n",
      "3033\n",
      "3033\n",
      "3033\n",
      "3034\n",
      "3035\n",
      "3036\n",
      "3037\n",
      "3038\n",
      "3039\n",
      "3040\n",
      "3040\n",
      "3040\n",
      "3040\n",
      "3040\n",
      "3040\n",
      "3040\n",
      "3040\n",
      "3041\n",
      "3041\n",
      "3042\n",
      "3043\n",
      "3043\n",
      "3043\n",
      "3044\n",
      "3045\n",
      "3045\n",
      "3045\n",
      "3046\n",
      "3047\n",
      "3047\n",
      "3047\n",
      "3048\n",
      "3049\n",
      "3050\n",
      "3051\n",
      "3052\n",
      "3053\n",
      "3053\n",
      "3054\n",
      "3055\n",
      "3055\n",
      "3056\n",
      "3057\n",
      "3058\n",
      "3059\n",
      "3060\n",
      "3060\n",
      "3060\n",
      "3060\n",
      "3060\n",
      "3061\n",
      "3061\n",
      "3062\n",
      "3062\n",
      "3063\n",
      "3063\n",
      "3063\n",
      "3063\n",
      "3063\n",
      "3063\n",
      "3064\n",
      "3064\n",
      "3064\n",
      "3065\n",
      "3065\n",
      "3065\n",
      "3065\n",
      "3066\n",
      "3067\n",
      "3067\n",
      "3068\n",
      "3069\n",
      "3070\n",
      "3070\n",
      "3071\n",
      "3072\n",
      "3073\n",
      "3074\n",
      "3074\n",
      "3075\n",
      "3076\n",
      "3076\n",
      "3077\n",
      "3077\n",
      "3077\n",
      "3078\n",
      "3078\n",
      "3079\n",
      "3079\n",
      "3080\n",
      "3080\n",
      "3080\n",
      "3080\n",
      "3080\n",
      "3080\n",
      "3081\n",
      "3082\n",
      "3083\n",
      "3084\n",
      "3085\n",
      "3085\n",
      "3086\n",
      "3086\n",
      "3087\n",
      "3088\n",
      "3089\n",
      "3090\n",
      "3091\n",
      "3092\n",
      "3093\n",
      "3094\n",
      "3095\n",
      "3096\n",
      "3097\n",
      "3098\n",
      "3099\n",
      "3100\n",
      "3100\n",
      "3100\n",
      "3100\n",
      "3100\n",
      "3101\n",
      "3101\n",
      "3102\n",
      "3103\n",
      "3104\n",
      "3105\n",
      "3105\n",
      "3106\n",
      "3106\n",
      "3106\n",
      "3106\n",
      "3107\n",
      "3108\n",
      "3109\n",
      "3110\n",
      "3110\n",
      "3111\n",
      "3111\n",
      "3112\n",
      "3112\n",
      "3113\n",
      "3114\n",
      "3114\n",
      "3115\n",
      "3115\n",
      "3116\n",
      "3117\n",
      "3117\n",
      "3118\n",
      "3119\n",
      "3119\n",
      "3120\n",
      "3120\n",
      "3120\n",
      "3120\n",
      "3120\n",
      "3121\n",
      "3122\n",
      "3122\n",
      "3122\n",
      "3123\n",
      "3124\n",
      "3124\n",
      "3124\n",
      "3125\n",
      "3126\n",
      "3127\n",
      "3127\n",
      "3128\n",
      "3128\n",
      "3129\n",
      "3129\n",
      "3129\n",
      "3129\n",
      "3130\n",
      "3130\n",
      "3131\n",
      "3132\n",
      "3133\n",
      "3134\n",
      "3135\n",
      "3136\n",
      "3137\n",
      "3137\n",
      "3137\n",
      "3138\n",
      "3139\n",
      "3140\n",
      "3140\n",
      "3140\n",
      "3140\n",
      "3141\n",
      "3142\n",
      "3143\n",
      "3143\n",
      "3144\n",
      "3145\n",
      "3145\n",
      "3146\n",
      "3147\n",
      "3147\n",
      "3148\n",
      "3148\n",
      "3149\n",
      "3149\n",
      "3150\n",
      "3151\n",
      "3151\n",
      "3151\n",
      "3152\n",
      "3153\n",
      "3154\n",
      "3154\n",
      "3155\n",
      "3156\n",
      "3157\n",
      "3157\n",
      "3157\n",
      "3157\n",
      "3158\n",
      "3159\n",
      "3159\n",
      "3160\n",
      "3161\n",
      "3162\n",
      "3163\n",
      "3163\n",
      "3163\n",
      "3164\n",
      "3165\n",
      "3166\n",
      "3166\n",
      "3167\n",
      "3167\n",
      "3168\n",
      "3169\n",
      "3170\n",
      "3170\n",
      "3171\n",
      "3172\n",
      "3173\n",
      "3173\n",
      "3173\n",
      "3174\n",
      "3175\n",
      "3175\n",
      "3176\n",
      "3177\n",
      "3178\n",
      "3179\n",
      "3180\n",
      "3180\n",
      "3180\n",
      "3180\n",
      "3180\n",
      "3181\n",
      "3182\n",
      "3182\n",
      "3183\n",
      "3184\n",
      "3184\n",
      "3184\n",
      "3185\n",
      "3185\n",
      "3185\n",
      "3186\n",
      "3187\n",
      "3188\n",
      "3188\n",
      "3188\n",
      "3189\n",
      "3190\n",
      "3191\n",
      "3191\n",
      "3191\n",
      "3192\n",
      "3192\n",
      "3193\n",
      "3194\n",
      "3195\n",
      "3196\n",
      "3197\n",
      "3198\n",
      "3199\n",
      "3200\n",
      "3200\n",
      "3200\n",
      "3200\n",
      "3200\n",
      "3200\n",
      "3200\n",
      "3201\n",
      "3201\n",
      "3202\n",
      "3203\n",
      "3204\n",
      "3205\n",
      "3206\n",
      "3206\n",
      "3207\n",
      "3207\n",
      "3208\n",
      "3209\n",
      "3210\n",
      "3211\n",
      "3212\n",
      "3213\n",
      "3214\n",
      "3214\n",
      "3215\n",
      "3216\n",
      "3216\n",
      "3217\n",
      "3217\n",
      "3217\n",
      "3217\n",
      "3218\n",
      "3219\n",
      "3219\n",
      "3220\n",
      "3220\n",
      "3220\n",
      "3220\n",
      "3221\n",
      "3222\n",
      "3223\n",
      "3223\n",
      "3224\n",
      "3225\n",
      "3226\n",
      "3226\n",
      "3226\n",
      "3227\n",
      "3227\n",
      "3227\n",
      "3228\n",
      "3228\n",
      "3229\n",
      "3229\n",
      "3229\n",
      "3230\n",
      "3231\n",
      "3232\n",
      "3233\n",
      "3234\n",
      "3235\n",
      "3236\n",
      "3236\n",
      "3237\n",
      "3237\n",
      "3238\n",
      "3239\n",
      "3239\n",
      "3240\n",
      "3240\n",
      "3240\n",
      "3240\n",
      "3240\n",
      "3240\n",
      "3241\n",
      "3241\n",
      "3242\n",
      "3243\n",
      "3244\n",
      "3244\n",
      "3244\n",
      "3245\n",
      "3246\n",
      "3246\n",
      "3247\n",
      "3248\n",
      "3249\n",
      "3249\n",
      "3249\n",
      "3249\n",
      "3250\n",
      "3251\n",
      "3252\n",
      "3252\n",
      "3253\n",
      "3254\n",
      "3254\n",
      "3255\n",
      "3256\n",
      "3257\n",
      "3257\n",
      "3258\n",
      "3258\n",
      "3259\n",
      "3260\n",
      "3260\n",
      "3260\n",
      "3260\n",
      "3260\n",
      "3261\n",
      "3262\n",
      "3262\n",
      "3262\n",
      "3263\n",
      "3263\n",
      "3264\n",
      "3265\n",
      "3266\n",
      "3267\n",
      "3267\n",
      "3267\n",
      "3268\n",
      "3269\n",
      "3270\n",
      "3271\n",
      "3271\n",
      "3271\n",
      "3272\n",
      "3272\n",
      "3272\n",
      "3273\n",
      "3273\n",
      "3274\n",
      "3275\n",
      "3276\n",
      "3277\n",
      "3278\n",
      "3279\n",
      "3279\n",
      "3280\n",
      "3280\n",
      "3280\n",
      "3280\n",
      "3280\n",
      "3280\n",
      "3281\n",
      "3282\n",
      "3282\n",
      "3283\n",
      "3284\n",
      "3284\n",
      "3285\n",
      "3286\n",
      "3287\n",
      "3287\n",
      "3287\n",
      "3288\n",
      "3289\n",
      "3289\n",
      "3290\n",
      "3290\n",
      "3291\n",
      "3292\n",
      "3293\n",
      "3294\n",
      "3295\n",
      "3296\n",
      "3297\n",
      "3297\n",
      "3298\n",
      "3299\n",
      "3299\n",
      "3300\n",
      "3301\n",
      "3301\n",
      "3302\n",
      "3302\n",
      "3303\n",
      "3304\n",
      "3305\n",
      "3306\n",
      "3307\n",
      "3308\n",
      "3309\n",
      "3310\n",
      "3310\n",
      "3310\n",
      "3311\n",
      "3311\n",
      "3312\n",
      "3313\n",
      "3314\n",
      "3315\n",
      "3315\n",
      "3316\n",
      "3317\n",
      "3317\n",
      "3317\n",
      "3318\n",
      "3319\n",
      "3319\n",
      "3320\n",
      "3321\n",
      "3321\n",
      "3322\n",
      "3322\n",
      "3323\n",
      "3324\n",
      "3324\n",
      "3325\n",
      "3326\n",
      "3327\n",
      "3328\n",
      "3329\n",
      "3329\n",
      "3330\n",
      "3331\n",
      "3332\n",
      "3333\n",
      "3333\n",
      "3333\n",
      "3333\n",
      "3334\n",
      "3335\n",
      "3335\n",
      "3335\n",
      "3335\n",
      "3336\n",
      "3337\n",
      "3338\n",
      "3339\n",
      "3340\n",
      "3340\n",
      "3341\n",
      "3341\n",
      "3341\n",
      "3342\n",
      "3343\n",
      "3344\n",
      "3345\n",
      "3345\n",
      "3346\n",
      "3347\n",
      "3348\n",
      "3349\n",
      "3349\n",
      "3349\n",
      "3350\n",
      "3350\n",
      "3350\n",
      "3351\n",
      "3352\n",
      "3353\n",
      "3353\n",
      "3354\n",
      "3354\n",
      "3355\n",
      "3356\n",
      "3356\n",
      "3357\n",
      "3358\n",
      "3359\n",
      "3360\n",
      "3360\n",
      "3361\n",
      "3361\n",
      "3362\n",
      "3362\n",
      "3362\n",
      "3362\n",
      "3362\n",
      "3363\n",
      "3364\n",
      "3365\n",
      "3365\n",
      "3366\n",
      "3367\n",
      "3367\n",
      "3368\n",
      "3368\n",
      "3369\n",
      "3370\n",
      "3371\n",
      "3372\n",
      "3372\n",
      "3373\n",
      "3374\n",
      "3375\n",
      "3376\n",
      "3377\n",
      "3378\n",
      "3378\n",
      "3379\n",
      "3380\n",
      "3381\n",
      "3382\n",
      "3383\n",
      "3384\n",
      "3385\n",
      "3386\n",
      "3387\n",
      "3388\n",
      "3388\n",
      "3388\n",
      "3388\n",
      "3388\n",
      "3389\n",
      "3390\n",
      "3390\n",
      "3390\n",
      "3391\n",
      "3391\n",
      "3391\n",
      "3391\n",
      "3392\n",
      "3393\n",
      "3394\n",
      "3395\n",
      "3396\n",
      "3397\n",
      "3397\n",
      "3398\n",
      "3399\n",
      "3399\n",
      "3399\n",
      "3400\n",
      "3400\n",
      "3400\n",
      "3401\n",
      "3402\n",
      "3403\n",
      "3403\n",
      "3404\n",
      "3405\n",
      "3405\n",
      "3406\n",
      "3406\n",
      "3407\n",
      "3408\n",
      "3409\n",
      "3409\n",
      "3410\n",
      "3411\n",
      "3412\n",
      "3412\n",
      "3413\n",
      "3414\n",
      "3415\n",
      "3416\n",
      "3416\n",
      "3417\n",
      "3417\n",
      "3417\n",
      "3418\n",
      "3419\n",
      "3419\n",
      "3419\n",
      "3420\n",
      "3421\n",
      "3422\n",
      "3423\n",
      "3424\n",
      "3425\n",
      "3425\n",
      "3425\n",
      "3426\n",
      "3427\n",
      "3428\n",
      "3428\n",
      "3429\n",
      "3430\n",
      "3431\n",
      "3431\n",
      "3432\n",
      "3433\n",
      "3433\n",
      "3434\n",
      "3435\n",
      "3436\n",
      "3437\n",
      "3438\n",
      "3438\n",
      "3439\n",
      "3440\n",
      "3441\n",
      "3441\n",
      "3441\n",
      "3441\n",
      "3442\n",
      "3442\n",
      "3442\n",
      "3443\n",
      "3444\n",
      "3445\n",
      "3446\n",
      "3446\n",
      "3446\n",
      "3447\n",
      "3448\n",
      "3449\n",
      "3450\n",
      "3451\n",
      "3451\n",
      "3452\n",
      "3453\n",
      "3454\n",
      "3455\n",
      "3456\n",
      "3457\n",
      "3458\n",
      "3459\n",
      "3460\n",
      "3461\n",
      "3462\n",
      "3463\n",
      "3464\n",
      "3465\n",
      "3466\n",
      "3467\n",
      "3468\n",
      "3469\n",
      "3470\n",
      "3471\n",
      "3472\n",
      "3473\n",
      "3474\n",
      "3475\n",
      "3476\n",
      "3477\n",
      "3478\n",
      "3479\n",
      "3480\n",
      "3481\n",
      "3481\n",
      "3482\n",
      "3483\n",
      "3484\n",
      "3485\n",
      "3485\n",
      "3485\n",
      "3486\n",
      "3487\n",
      "3488\n",
      "3489\n",
      "3490\n",
      "3491\n",
      "3492\n",
      "3493\n",
      "3493\n",
      "3493\n",
      "3493\n",
      "3493\n",
      "3493\n",
      "3494\n",
      "3494\n",
      "3494\n",
      "3494\n",
      "3495\n",
      "3495\n",
      "3496\n",
      "3496\n",
      "3497\n",
      "3498\n",
      "3499\n",
      "3499\n",
      "3500\n",
      "3501\n",
      "3502\n",
      "3503\n",
      "3503\n",
      "3504\n",
      "3505\n",
      "3505\n",
      "3505\n",
      "3506\n",
      "3506\n",
      "3507\n",
      "3508\n",
      "3509\n",
      "3509\n",
      "3510\n",
      "3510\n",
      "3511\n",
      "3512\n",
      "3513\n",
      "3513\n",
      "3513\n",
      "3513\n",
      "3514\n",
      "3515\n",
      "3515\n",
      "3516\n",
      "3516\n",
      "3517\n",
      "3518\n",
      "3518\n",
      "3519\n",
      "3520\n",
      "3520\n",
      "3521\n",
      "3521\n",
      "3522\n",
      "3523\n",
      "3524\n",
      "3525\n",
      "3526\n",
      "3527\n",
      "3527\n",
      "3528\n",
      "3528\n",
      "3529\n",
      "3529\n",
      "3530\n",
      "3531\n",
      "3532\n",
      "3533\n",
      "3533\n",
      "3533\n",
      "3533\n",
      "3534\n",
      "3535\n",
      "3536\n",
      "3537\n",
      "3538\n",
      "3539\n",
      "3539\n",
      "3539\n",
      "3539\n",
      "3539\n",
      "3540\n",
      "3541\n",
      "3542\n",
      "3542\n",
      "3543\n",
      "3544\n",
      "3545\n",
      "3546\n",
      "3546\n",
      "3547\n",
      "3547\n",
      "3547\n",
      "3548\n",
      "3549\n",
      "3550\n",
      "3551\n",
      "3552\n",
      "3553\n",
      "3553\n",
      "3553\n",
      "3553\n",
      "3553\n",
      "3554\n",
      "3554\n",
      "3554\n",
      "3555\n",
      "3556\n",
      "3557\n",
      "3558\n",
      "3559\n",
      "3560\n",
      "3561\n",
      "3562\n",
      "3563\n",
      "3564\n",
      "3565\n",
      "3566\n",
      "3567\n",
      "3568\n",
      "3569\n",
      "3570\n",
      "3571\n",
      "3572\n",
      "3573\n",
      "3574\n",
      "3574\n",
      "3574\n",
      "3574\n",
      "3574\n",
      "3575\n",
      "3575\n",
      "3575\n",
      "3576\n",
      "3577\n",
      "3578\n",
      "3579\n",
      "3580\n",
      "3580\n",
      "3580\n",
      "3581\n",
      "3581\n",
      "3582\n",
      "3583\n",
      "3584\n",
      "3584\n",
      "3584\n",
      "3584\n",
      "3584\n",
      "3585\n",
      "3586\n",
      "3587\n",
      "3588\n",
      "3588\n",
      "3589\n",
      "3590\n",
      "3591\n",
      "3591\n",
      "3592\n",
      "3593\n",
      "3593\n",
      "3594\n",
      "3594\n",
      "3594\n",
      "3595\n",
      "3596\n",
      "3597\n",
      "3598\n",
      "3598\n",
      "3599\n",
      "3599\n",
      "3600\n",
      "3600\n",
      "3600\n",
      "3600\n",
      "3601\n",
      "3601\n",
      "3602\n",
      "3602\n",
      "3603\n",
      "3604\n",
      "3605\n",
      "3605\n",
      "3606\n",
      "3606\n",
      "3607\n",
      "3608\n",
      "3608\n",
      "3608\n",
      "3608\n",
      "3608\n",
      "3608\n",
      "3609\n",
      "3610\n",
      "3611\n",
      "3611\n",
      "3612\n",
      "3612\n",
      "3612\n",
      "3612\n",
      "3613\n",
      "3613\n",
      "3614\n",
      "3614\n",
      "3614\n",
      "3615\n",
      "3615\n",
      "3616\n",
      "3617\n",
      "3617\n",
      "3617\n",
      "3617\n",
      "3618\n",
      "3618\n",
      "3619\n",
      "3620\n",
      "3621\n",
      "3622\n",
      "3623\n",
      "3624\n",
      "3625\n",
      "3626\n",
      "3627\n",
      "3628\n",
      "3629\n",
      "3630\n",
      "3631\n",
      "3632\n",
      "3633\n",
      "3634\n",
      "3635\n",
      "3636\n",
      "3637\n",
      "3638\n",
      "3638\n",
      "3638\n",
      "3639\n",
      "3640\n",
      "3640\n",
      "3641\n",
      "3642\n",
      "3643\n",
      "3644\n",
      "3645\n",
      "3646\n",
      "3647\n",
      "3648\n",
      "3649\n",
      "3650\n",
      "3651\n",
      "3652\n",
      "3653\n",
      "3654\n",
      "3655\n",
      "3655\n",
      "3656\n",
      "3657\n",
      "3658\n",
      "3659\n",
      "3660\n",
      "3661\n",
      "3662\n",
      "3663\n",
      "3664\n",
      "3665\n",
      "3666\n",
      "3666\n",
      "3667\n",
      "3668\n",
      "3668\n",
      "3669\n",
      "3670\n",
      "3671\n",
      "3672\n",
      "3673\n",
      "3674\n",
      "3674\n",
      "3674\n",
      "3675\n",
      "3676\n",
      "3676\n",
      "3677\n",
      "3678\n",
      "3679\n",
      "3679\n",
      "3680\n",
      "3681\n",
      "3682\n",
      "3682\n",
      "3682\n",
      "3683\n",
      "3684\n",
      "3684\n",
      "3685\n",
      "3686\n",
      "3687\n",
      "3688\n",
      "3688\n",
      "3689\n",
      "3690\n",
      "3691\n",
      "3692\n",
      "3693\n",
      "3694\n",
      "3695\n",
      "3696\n",
      "3696\n",
      "3697\n",
      "3698\n",
      "3698\n",
      "3698\n",
      "3699\n",
      "3700\n",
      "3700\n",
      "3700\n",
      "3701\n",
      "3702\n",
      "3703\n",
      "3704\n",
      "3705\n",
      "3706\n",
      "3706\n",
      "3706\n",
      "3706\n",
      "3706\n",
      "3706\n",
      "3706\n",
      "3707\n",
      "3708\n",
      "3708\n",
      "3709\n",
      "3710\n",
      "3711\n",
      "3712\n",
      "3712\n",
      "3713\n",
      "3714\n",
      "3715\n",
      "3716\n",
      "3717\n",
      "3718\n",
      "3719\n",
      "3719\n",
      "3719\n",
      "3720\n",
      "3720\n",
      "3721\n",
      "3722\n",
      "3723\n",
      "3723\n",
      "3724\n",
      "3724\n",
      "3725\n",
      "3726\n",
      "3727\n",
      "3728\n",
      "3729\n",
      "3730\n",
      "3731\n",
      "3731\n",
      "3732\n",
      "3733\n",
      "3734\n",
      "3735\n",
      "3736\n",
      "3737\n",
      "3738\n",
      "3739\n",
      "3740\n",
      "3741\n",
      "3742\n",
      "3743\n",
      "3744\n",
      "3744\n",
      "3744\n",
      "3745\n",
      "3745\n",
      "3745\n",
      "3746\n",
      "3747\n",
      "3747\n",
      "3748\n",
      "3748\n",
      "3749\n",
      "3750\n",
      "3751\n",
      "3752\n",
      "3753\n",
      "3754\n",
      "3755\n",
      "3755\n",
      "3756\n",
      "3757\n",
      "3758\n",
      "3758\n",
      "3759\n",
      "3759\n",
      "3760\n",
      "3760\n",
      "3761\n",
      "3762\n",
      "3762\n",
      "3762\n",
      "3762\n",
      "3763\n",
      "3764\n",
      "3764\n",
      "3765\n",
      "3765\n",
      "3766\n",
      "3767\n",
      "3767\n",
      "3767\n",
      "3768\n",
      "3769\n",
      "3769\n",
      "3769\n",
      "3770\n",
      "3771\n",
      "3772\n",
      "3773\n",
      "3774\n",
      "3774\n",
      "3774\n",
      "3775\n",
      "3776\n",
      "3777\n",
      "3778\n",
      "3779\n",
      "3780\n",
      "3780\n",
      "3781\n",
      "3782\n",
      "3782\n",
      "3783\n",
      "3784\n",
      "3785\n",
      "3786\n",
      "3787\n",
      "3788\n",
      "3789\n",
      "3789\n",
      "3789\n",
      "3789\n",
      "3789\n",
      "3789\n",
      "3789\n",
      "3790\n",
      "3791\n",
      "3792\n",
      "3793\n",
      "3794\n",
      "3795\n",
      "3795\n",
      "3796\n",
      "3796\n",
      "3796\n",
      "3797\n",
      "3798\n",
      "3799\n",
      "3799\n",
      "3799\n",
      "3799\n",
      "3800\n",
      "3800\n",
      "3801\n",
      "3802\n",
      "3803\n",
      "3804\n",
      "3805\n",
      "3806\n",
      "3807\n",
      "3808\n",
      "3808\n",
      "3809\n",
      "3809\n",
      "3809\n",
      "3809\n",
      "3809\n",
      "3809\n",
      "3810\n",
      "3811\n",
      "3812\n",
      "3812\n",
      "3812\n",
      "3813\n",
      "3814\n",
      "3815\n",
      "3816\n",
      "3817\n",
      "3818\n",
      "3819\n",
      "3820\n",
      "3820\n",
      "3821\n",
      "3821\n",
      "3821\n",
      "3821\n",
      "3822\n",
      "3823\n",
      "3824\n",
      "3825\n",
      "3826\n",
      "3827\n",
      "3828\n",
      "3828\n",
      "3829\n",
      "3829\n",
      "3829\n",
      "3829\n",
      "3830\n",
      "3831\n",
      "3832\n",
      "3833\n",
      "3834\n",
      "3835\n",
      "3836\n",
      "3837\n",
      "3838\n",
      "3839\n",
      "3840\n",
      "3840\n",
      "3841\n",
      "3842\n",
      "3842\n",
      "3843\n",
      "3844\n",
      "3844\n",
      "3845\n",
      "3845\n",
      "3845\n",
      "3845\n",
      "3846\n",
      "3847\n",
      "3848\n",
      "3849\n",
      "3849\n",
      "3849\n",
      "3849\n",
      "3849\n",
      "3849\n",
      "3849\n",
      "3850\n",
      "3851\n",
      "3852\n",
      "3852\n",
      "3852\n",
      "3853\n",
      "3854\n",
      "3854\n",
      "3855\n",
      "3855\n",
      "3855\n",
      "3855\n",
      "3856\n",
      "3857\n",
      "3858\n",
      "3859\n",
      "3860\n",
      "3860\n",
      "3861\n",
      "3862\n",
      "3863\n",
      "3864\n",
      "3865\n",
      "3866\n",
      "3867\n",
      "3868\n",
      "3869\n",
      "3869\n",
      "3869\n",
      "3869\n",
      "3869\n",
      "3870\n",
      "3871\n",
      "3871\n",
      "3871\n",
      "3872\n",
      "3873\n",
      "3874\n",
      "3875\n",
      "3876\n",
      "3877\n",
      "3878\n",
      "3879\n",
      "3880\n",
      "3881\n",
      "3881\n",
      "3882\n",
      "3883\n",
      "3884\n",
      "3884\n",
      "3884\n",
      "3885\n",
      "3886\n",
      "3887\n",
      "3887\n",
      "3888\n",
      "3889\n",
      "3889\n",
      "3889\n",
      "3889\n",
      "3889\n",
      "3889\n",
      "3889\n",
      "3890\n",
      "3891\n",
      "3892\n",
      "3893\n",
      "3894\n",
      "3895\n",
      "3896\n",
      "3897\n",
      "3898\n",
      "3898\n",
      "3898\n",
      "3899\n",
      "3900\n",
      "3901\n",
      "3902\n",
      "3902\n",
      "3903\n",
      "3904\n",
      "3905\n",
      "3905\n",
      "3906\n",
      "3906\n",
      "3906\n",
      "3907\n",
      "3908\n",
      "3909\n",
      "3909\n",
      "3909\n",
      "3909\n",
      "3909\n",
      "3909\n",
      "3910\n",
      "3911\n",
      "3912\n",
      "3913\n",
      "3914\n",
      "3914\n",
      "3915\n",
      "3916\n",
      "3917\n",
      "3918\n",
      "3919\n",
      "3920\n",
      "3920\n",
      "3921\n",
      "3922\n",
      "3922\n",
      "3923\n",
      "3924\n",
      "3924\n",
      "3925\n",
      "3926\n",
      "3927\n",
      "3928\n",
      "3929\n",
      "3929\n",
      "3929\n",
      "3929\n",
      "3930\n",
      "3930\n",
      "3930\n",
      "3931\n",
      "3931\n",
      "3932\n",
      "3933\n",
      "3934\n",
      "3934\n",
      "3935\n",
      "3936\n",
      "3937\n",
      "3937\n",
      "3938\n",
      "3938\n",
      "3939\n",
      "3940\n",
      "3940\n",
      "3941\n",
      "3942\n",
      "3943\n",
      "3944\n",
      "3945\n",
      "3946\n",
      "3946\n",
      "3946\n",
      "3947\n",
      "3947\n",
      "3948\n",
      "3949\n",
      "3950\n",
      "3950\n",
      "3951\n",
      "3952\n",
      "3952\n",
      "3953\n",
      "3954\n",
      "3955\n",
      "3956\n",
      "3957\n",
      "3958\n",
      "3958\n",
      "3959\n",
      "3960\n",
      "3960\n",
      "3961\n",
      "3962\n",
      "3963\n",
      "3963\n",
      "3964\n",
      "3965\n",
      "3966\n",
      "3967\n",
      "3968\n",
      "3969\n",
      "3969\n",
      "3970\n",
      "3971\n",
      "3971\n",
      "3972\n",
      "3973\n",
      "3974\n",
      "3975\n",
      "3976\n",
      "3976\n",
      "3976\n",
      "3977\n",
      "3978\n",
      "3979\n",
      "3980\n",
      "3981\n",
      "3981\n",
      "3982\n",
      "3983\n",
      "3984\n",
      "3985\n",
      "3985\n",
      "3986\n",
      "3987\n",
      "3988\n",
      "3989\n",
      "3990\n",
      "3991\n",
      "3991\n",
      "3992\n",
      "3993\n",
      "3994\n",
      "3995\n",
      "3995\n",
      "3996\n",
      "3997\n",
      "3998\n",
      "3998\n",
      "3999\n",
      "4000\n",
      "4001\n",
      "4001\n",
      "4002\n",
      "4002\n",
      "4003\n",
      "4004\n",
      "4005\n",
      "4005\n",
      "4005\n",
      "4006\n",
      "4007\n",
      "4008\n",
      "4009\n",
      "4010\n",
      "4011\n",
      "4012\n",
      "4012\n",
      "4012\n",
      "4012\n",
      "4013\n",
      "4014\n",
      "4015\n",
      "4015\n",
      "4016\n",
      "4016\n",
      "4017\n",
      "4018\n",
      "4018\n",
      "4019\n",
      "4019\n",
      "4020\n",
      "4020\n",
      "4021\n",
      "4021\n",
      "4021\n",
      "4022\n",
      "4023\n",
      "4024\n",
      "4025\n",
      "4026\n",
      "4026\n",
      "4027\n",
      "4028\n",
      "4028\n",
      "4029\n",
      "4029\n",
      "4030\n",
      "4031\n",
      "4032\n",
      "4033\n",
      "4033\n",
      "4034\n",
      "4035\n",
      "4036\n",
      "4036\n",
      "4036\n",
      "4036\n",
      "4037\n",
      "4037\n",
      "4038\n",
      "4039\n",
      "4039\n",
      "4040\n",
      "4041\n",
      "4042\n",
      "4042\n",
      "4042\n",
      "4043\n",
      "4044\n",
      "4045\n",
      "4046\n",
      "4046\n",
      "4047\n",
      "4048\n",
      "4049\n",
      "4050\n",
      "4051\n",
      "4052\n",
      "4053\n",
      "4053\n",
      "4054\n",
      "4054\n",
      "4055\n",
      "4056\n",
      "4056\n",
      "4057\n",
      "4058\n",
      "4059\n",
      "4060\n",
      "4061\n",
      "4061\n",
      "4062\n",
      "4062\n",
      "4063\n",
      "4064\n",
      "4065\n",
      "4066\n",
      "4067\n",
      "4068\n",
      "4069\n",
      "4070\n",
      "4071\n",
      "4071\n",
      "4071\n",
      "4071\n",
      "4072\n",
      "4073\n",
      "4074\n",
      "4075\n",
      "4076\n",
      "4076\n",
      "4077\n",
      "4077\n",
      "4077\n",
      "4078\n",
      "4079\n",
      "4079\n",
      "4079\n",
      "4079\n",
      "4079\n",
      "4079\n",
      "4080\n",
      "4081\n",
      "4082\n",
      "4083\n",
      "4084\n",
      "4085\n",
      "4085\n",
      "4085\n",
      "4085\n",
      "4085\n",
      "4086\n",
      "4086\n",
      "4087\n",
      "4088\n",
      "4089\n",
      "4089\n",
      "4089\n",
      "4090\n",
      "4091\n",
      "4092\n",
      "4093\n",
      "4093\n",
      "4094\n",
      "4095\n",
      "4096\n",
      "4097\n",
      "4097\n",
      "4097\n",
      "4098\n",
      "4099\n",
      "4099\n",
      "4099\n",
      "4100\n",
      "4101\n",
      "4101\n",
      "4102\n",
      "4102\n",
      "4102\n",
      "4102\n",
      "4103\n",
      "4104\n",
      "4105\n",
      "4105\n",
      "4105\n",
      "4105\n",
      "4105\n",
      "4106\n",
      "4107\n",
      "4108\n",
      "4108\n",
      "4109\n",
      "4110\n",
      "4110\n",
      "4111\n",
      "4111\n",
      "4111\n",
      "4112\n",
      "4113\n",
      "4114\n",
      "4114\n",
      "4115\n",
      "4116\n",
      "4116\n",
      "4116\n",
      "4116\n",
      "4116\n",
      "4117\n",
      "4117\n",
      "4118\n",
      "4118\n",
      "4118\n",
      "4119\n",
      "4119\n",
      "4120\n",
      "4121\n",
      "4122\n",
      "4123\n",
      "4124\n",
      "4125\n",
      "4125\n",
      "4125\n",
      "4125\n",
      "4125\n",
      "4125\n",
      "4126\n",
      "4127\n",
      "4127\n",
      "4128\n",
      "4128\n",
      "4129\n",
      "4129\n",
      "4130\n",
      "4131\n",
      "4132\n",
      "4133\n",
      "4134\n",
      "4135\n",
      "4136\n",
      "4137\n",
      "4138\n",
      "4139\n",
      "4140\n",
      "4141\n",
      "4141\n",
      "4142\n",
      "4143\n",
      "4143\n",
      "4144\n",
      "4145\n",
      "4145\n",
      "4145\n",
      "4145\n",
      "4145\n",
      "4146\n",
      "4147\n",
      "4148\n",
      "4149\n",
      "4149\n",
      "4150\n",
      "4150\n",
      "4151\n",
      "4151\n",
      "4151\n",
      "4151\n",
      "4151\n",
      "4152\n",
      "4153\n",
      "4153\n",
      "4153\n",
      "4153\n",
      "4154\n",
      "4154\n",
      "4155\n",
      "4156\n",
      "4156\n",
      "4157\n",
      "4157\n",
      "4158\n",
      "4158\n",
      "4159\n",
      "4160\n",
      "4160\n",
      "4161\n",
      "4162\n",
      "4163\n",
      "4163\n",
      "4163\n",
      "4164\n",
      "4165\n",
      "4165\n",
      "4165\n",
      "4165\n",
      "4165\n",
      "4165\n",
      "4165\n",
      "4166\n",
      "4166\n",
      "4167\n",
      "4168\n",
      "4169\n",
      "4170\n",
      "4171\n",
      "4171\n",
      "4172\n",
      "4173\n",
      "4173\n",
      "4174\n",
      "4174\n",
      "4175\n",
      "4176\n",
      "4176\n",
      "4176\n",
      "4176\n",
      "4176\n",
      "4177\n",
      "4178\n",
      "4179\n",
      "4180\n",
      "4181\n",
      "4181\n",
      "4182\n",
      "4183\n",
      "4184\n",
      "4184\n",
      "4185\n",
      "4185\n",
      "4185\n",
      "4185\n",
      "4185\n",
      "4185\n",
      "4186\n",
      "4187\n",
      "4188\n",
      "4189\n",
      "4190\n",
      "4191\n",
      "4192\n",
      "4193\n",
      "4193\n",
      "4194\n",
      "4195\n",
      "4196\n",
      "4197\n",
      "4198\n",
      "4199\n",
      "4200\n",
      "4200\n",
      "4200\n",
      "4201\n",
      "4202\n",
      "4203\n",
      "4204\n",
      "4205\n",
      "4205\n",
      "4205\n",
      "4205\n",
      "4205\n",
      "4206\n",
      "4207\n",
      "4207\n",
      "4208\n",
      "4209\n",
      "4209\n",
      "4210\n",
      "4210\n",
      "4211\n",
      "4212\n",
      "4213\n",
      "4214\n",
      "4214\n",
      "4214\n",
      "4215\n",
      "4216\n",
      "4217\n",
      "4218\n",
      "4219\n",
      "4220\n",
      "4221\n",
      "4221\n",
      "4222\n",
      "4222\n",
      "4223\n",
      "4223\n",
      "4224\n",
      "4224\n",
      "4224\n",
      "4225\n",
      "4225\n",
      "4226\n",
      "4227\n",
      "4228\n",
      "4228\n",
      "4229\n",
      "4230\n",
      "4230\n",
      "4231\n",
      "4231\n",
      "4232\n",
      "4232\n",
      "4232\n",
      "4233\n",
      "4233\n",
      "4234\n",
      "4235\n",
      "4236\n",
      "4236\n",
      "4236\n",
      "4237\n",
      "4237\n",
      "4238\n",
      "4238\n",
      "4238\n",
      "4239\n",
      "4239\n",
      "4239\n",
      "4239\n",
      "4240\n",
      "4241\n",
      "4242\n",
      "4243\n",
      "4244\n",
      "4245\n",
      "4246\n",
      "4246\n",
      "4247\n",
      "4247\n",
      "4248\n",
      "4248\n",
      "4249\n",
      "4250\n",
      "4250\n",
      "4251\n",
      "4251\n",
      "4252\n",
      "4253\n",
      "4253\n",
      "4254\n",
      "4255\n",
      "4256\n",
      "4256\n",
      "4257\n",
      "4258\n",
      "4259\n",
      "4260\n",
      "4261\n",
      "4262\n",
      "4263\n",
      "4264\n",
      "4265\n",
      "4266\n",
      "4267\n",
      "4268\n",
      "4269\n",
      "4270\n",
      "4271\n",
      "4272\n",
      "4272\n",
      "4273\n",
      "4274\n",
      "4274\n",
      "4275\n",
      "4276\n",
      "4276\n",
      "4276\n",
      "4277\n",
      "4278\n",
      "4278\n",
      "4278\n",
      "4278\n",
      "4279\n",
      "4280\n",
      "4281\n",
      "4282\n",
      "4283\n",
      "4284\n",
      "4284\n",
      "4285\n",
      "4286\n",
      "4287\n",
      "4287\n",
      "4288\n",
      "4289\n",
      "4290\n",
      "4291\n",
      "4292\n",
      "4292\n",
      "4293\n",
      "4294\n",
      "4295\n",
      "4295\n",
      "4295\n",
      "4295\n",
      "4296\n",
      "4296\n",
      "4297\n",
      "4297\n",
      "4298\n",
      "4299\n",
      "4299\n",
      "4300\n",
      "4301\n",
      "4301\n",
      "4301\n",
      "4301\n",
      "4301\n",
      "4301\n",
      "4302\n",
      "4303\n",
      "4304\n",
      "4304\n",
      "4305\n",
      "4305\n",
      "4305\n",
      "4305\n",
      "4305\n",
      "4305\n",
      "4305\n",
      "4306\n",
      "4307\n",
      "4308\n",
      "4309\n",
      "4309\n",
      "4309\n",
      "4310\n",
      "4311\n",
      "4312\n",
      "4313\n",
      "4313\n",
      "4314\n",
      "4315\n",
      "4316\n",
      "4317\n",
      "4317\n",
      "4318\n",
      "4319\n",
      "4320\n",
      "4321\n",
      "4321\n",
      "4321\n",
      "4321\n",
      "4322\n",
      "4323\n",
      "4323\n",
      "4324\n",
      "4324\n",
      "4325\n",
      "4326\n",
      "4327\n",
      "4328\n",
      "4328\n",
      "4329\n",
      "4329\n",
      "4330\n",
      "4331\n",
      "4332\n",
      "4333\n",
      "4334\n",
      "4335\n",
      "4336\n",
      "4336\n",
      "4337\n",
      "4337\n",
      "4338\n",
      "4339\n",
      "4340\n",
      "4341\n",
      "4342\n",
      "4342\n",
      "4343\n",
      "4344\n",
      "4345\n",
      "4345\n",
      "4346\n",
      "4347\n",
      "4347\n",
      "4348\n",
      "4349\n",
      "4350\n",
      "4351\n",
      "4352\n",
      "4353\n",
      "4354\n",
      "4355\n",
      "4355\n",
      "4355\n",
      "4356\n",
      "4356\n",
      "4357\n",
      "4358\n",
      "4358\n",
      "4359\n",
      "4360\n",
      "4361\n",
      "4362\n",
      "4362\n",
      "4363\n",
      "4363\n",
      "4364\n",
      "4364\n",
      "4365\n",
      "4366\n",
      "4367\n",
      "4367\n",
      "4368\n",
      "4369\n",
      "4370\n",
      "4371\n",
      "4371\n",
      "4371\n",
      "4372\n",
      "4372\n",
      "4373\n",
      "4374\n",
      "4375\n",
      "4376\n",
      "4377\n",
      "4378\n",
      "4378\n",
      "4379\n",
      "4380\n",
      "4381\n",
      "4382\n",
      "4382\n",
      "4383\n",
      "4384\n",
      "4384\n",
      "4385\n",
      "4386\n",
      "4386\n",
      "4387\n",
      "4388\n",
      "4388\n",
      "4389\n",
      "4390\n",
      "4390\n",
      "4391\n",
      "4392\n",
      "4393\n",
      "4394\n",
      "4395\n",
      "4396\n",
      "4396\n",
      "4397\n",
      "4398\n",
      "4399\n",
      "4400\n",
      "4401\n",
      "4401\n",
      "4401\n",
      "4402\n",
      "4403\n",
      "4404\n",
      "4405\n",
      "4405\n",
      "4406\n",
      "4407\n",
      "4407\n",
      "4407\n",
      "4407\n",
      "4407\n",
      "4407\n",
      "4407\n",
      "4408\n",
      "4409\n",
      "4409\n",
      "4409\n",
      "4410\n",
      "4411\n",
      "4412\n",
      "4413\n",
      "4414\n",
      "4415\n",
      "4416\n",
      "4416\n",
      "4417\n",
      "4417\n",
      "4418\n",
      "4418\n",
      "4418\n",
      "4419\n",
      "4420\n",
      "4421\n",
      "4422\n",
      "4422\n",
      "4423\n",
      "4423\n",
      "4424\n",
      "4424\n",
      "4425\n",
      "4425\n",
      "4425\n",
      "4426\n",
      "4427\n",
      "4427\n",
      "4427\n",
      "4427\n",
      "4427\n",
      "4428\n",
      "4429\n",
      "4429\n",
      "4430\n",
      "4431\n",
      "4431\n",
      "4432\n",
      "4433\n",
      "4434\n",
      "4435\n",
      "4436\n",
      "4436\n",
      "4436\n",
      "4436\n",
      "4437\n",
      "4438\n",
      "4439\n",
      "4439\n",
      "4440\n",
      "4440\n",
      "4441\n",
      "4442\n",
      "4443\n",
      "4443\n",
      "4444\n",
      "4445\n",
      "4446\n",
      "4447\n",
      "4447\n",
      "4447\n",
      "4448\n",
      "4449\n",
      "4449\n",
      "4450\n",
      "4451\n",
      "4452\n",
      "4453\n",
      "4454\n",
      "4455\n",
      "4456\n",
      "4456\n",
      "4457\n",
      "4458\n",
      "4459\n",
      "4460\n",
      "4461\n",
      "4462\n",
      "4463\n",
      "4463\n",
      "4463\n",
      "4464\n",
      "4465\n",
      "4465\n",
      "4466\n",
      "4466\n",
      "4467\n",
      "4467\n",
      "4468\n",
      "4469\n",
      "4469\n",
      "4469\n",
      "4469\n",
      "4469\n",
      "4470\n",
      "4471\n",
      "4471\n",
      "4472\n",
      "4472\n",
      "4472\n",
      "4473\n",
      "4474\n",
      "4474\n",
      "4474\n",
      "4475\n",
      "4476\n",
      "4477\n",
      "4478\n",
      "4479\n",
      "4480\n",
      "4481\n",
      "4482\n",
      "4483\n",
      "4483\n",
      "4484\n",
      "4485\n",
      "4486\n",
      "4487\n",
      "4488\n",
      "4489\n",
      "4490\n",
      "4491\n",
      "4491\n",
      "4491\n",
      "4492\n",
      "4493\n",
      "4494\n",
      "4495\n",
      "4496\n",
      "4496\n",
      "4496\n",
      "4497\n",
      "4497\n",
      "4498\n",
      "4498\n",
      "4498\n",
      "4499\n",
      "4500\n",
      "4501\n",
      "4501\n",
      "4502\n",
      "4502\n",
      "4503\n",
      "4503\n",
      "4504\n",
      "4505\n",
      "4505\n",
      "4506\n",
      "4506\n",
      "4507\n",
      "4508\n",
      "4509\n",
      "4509\n",
      "4509\n",
      "4510\n",
      "4511\n",
      "4511\n",
      "4512\n",
      "4513\n",
      "4514\n",
      "4515\n",
      "4516\n",
      "4517\n",
      "4517\n",
      "4517\n",
      "4517\n",
      "4518\n",
      "4519\n",
      "4519\n",
      "4520\n",
      "4521\n",
      "4522\n",
      "4523\n",
      "4523\n",
      "4524\n",
      "4525\n",
      "4526\n",
      "4527\n",
      "4528\n",
      "4529\n",
      "4530\n",
      "4531\n",
      "4532\n",
      "4532\n",
      "4533\n",
      "4534\n",
      "4534\n",
      "4535\n",
      "4536\n",
      "4537\n",
      "4538\n",
      "4538\n",
      "4538\n",
      "4538\n",
      "4538\n",
      "4539\n",
      "4539\n",
      "4540\n",
      "4540\n",
      "4541\n",
      "4542\n",
      "4543\n",
      "4544\n",
      "4545\n",
      "4546\n",
      "4547\n",
      "4548\n",
      "4548\n",
      "4549\n",
      "4550\n",
      "4550\n",
      "4551\n",
      "4551\n",
      "4551\n",
      "4552\n",
      "4553\n",
      "4553\n",
      "4554\n",
      "4555\n",
      "4556\n",
      "4557\n",
      "4558\n",
      "4558\n",
      "4558\n",
      "4558\n",
      "4558\n",
      "4558\n",
      "4558\n",
      "4559\n",
      "4559\n",
      "4560\n",
      "4560\n",
      "4560\n",
      "4560\n",
      "4561\n",
      "4562\n",
      "4562\n",
      "4563\n",
      "4564\n",
      "4565\n",
      "4565\n",
      "4566\n",
      "4566\n",
      "4567\n",
      "4568\n",
      "4569\n",
      "4570\n",
      "4571\n",
      "4571\n",
      "4572\n",
      "4572\n",
      "4572\n",
      "4573\n",
      "4574\n",
      "4575\n",
      "4576\n",
      "4577\n",
      "4578\n",
      "4578\n",
      "4578\n",
      "4578\n",
      "4579\n",
      "4580\n",
      "4580\n",
      "4581\n",
      "4581\n",
      "4582\n",
      "4582\n",
      "4582\n",
      "4582\n",
      "4582\n",
      "4583\n",
      "4584\n",
      "4585\n",
      "4585\n",
      "4585\n",
      "4586\n",
      "4587\n",
      "4587\n",
      "4588\n",
      "4588\n",
      "4589\n",
      "4589\n",
      "4590\n",
      "4591\n",
      "4592\n",
      "4593\n",
      "4593\n",
      "4594\n",
      "4595\n",
      "4596\n",
      "4597\n",
      "4597\n",
      "4598\n",
      "4598\n",
      "4598\n",
      "4598\n",
      "4598\n",
      "4599\n",
      "4600\n",
      "4601\n",
      "4601\n",
      "4602\n",
      "4603\n",
      "4604\n",
      "4605\n",
      "4605\n",
      "4605\n",
      "4605\n",
      "4605\n",
      "4606\n",
      "4607\n",
      "4608\n",
      "4609\n",
      "4610\n",
      "4611\n",
      "4611\n",
      "4612\n",
      "4613\n",
      "4614\n",
      "4615\n",
      "4616\n",
      "4616\n",
      "4616\n",
      "4616\n",
      "4617\n",
      "4618\n",
      "4618\n",
      "4618\n",
      "4618\n",
      "4618\n",
      "4619\n",
      "4619\n",
      "4620\n",
      "4621\n",
      "4622\n",
      "4623\n",
      "4624\n",
      "4625\n",
      "4626\n",
      "4627\n",
      "4628\n",
      "4629\n",
      "4629\n",
      "4630\n",
      "4631\n",
      "4632\n",
      "4633\n",
      "4634\n",
      "4635\n",
      "4635\n",
      "4635\n",
      "4635\n",
      "4636\n",
      "4637\n",
      "4637\n",
      "4638\n",
      "4638\n",
      "4638\n",
      "4638\n",
      "4638\n",
      "4639\n",
      "4639\n",
      "4639\n",
      "4640\n",
      "4641\n",
      "4642\n",
      "4642\n",
      "4643\n",
      "4643\n",
      "4644\n",
      "4645\n",
      "4646\n",
      "4647\n",
      "4648\n",
      "4649\n",
      "4649\n",
      "4650\n",
      "4651\n",
      "4652\n",
      "4653\n",
      "4654\n",
      "4655\n",
      "4656\n",
      "4657\n",
      "4657\n",
      "4658\n",
      "4658\n",
      "4658\n",
      "4658\n",
      "4658\n",
      "4659\n",
      "4660\n",
      "4660\n",
      "4661\n",
      "4661\n",
      "4662\n",
      "4662\n",
      "4662\n",
      "4663\n",
      "4664\n",
      "4665\n",
      "4666\n",
      "4667\n",
      "4668\n",
      "4669\n",
      "4669\n",
      "4670\n",
      "4671\n",
      "4672\n",
      "4673\n",
      "4674\n",
      "4675\n",
      "4676\n",
      "4676\n",
      "4676\n",
      "4677\n",
      "4678\n",
      "4678\n",
      "4678\n",
      "4678\n",
      "4678\n",
      "4679\n",
      "4679\n",
      "4680\n",
      "4680\n",
      "4681\n",
      "4682\n",
      "4683\n",
      "4684\n",
      "4685\n",
      "4685\n",
      "4685\n",
      "4685\n",
      "4685\n",
      "4686\n",
      "4686\n",
      "4687\n",
      "4687\n",
      "4688\n",
      "4689\n",
      "4689\n",
      "4689\n",
      "4689\n",
      "4690\n",
      "4690\n",
      "4690\n",
      "4690\n",
      "4690\n",
      "4691\n",
      "4692\n",
      "4692\n",
      "4693\n",
      "4693\n",
      "4694\n",
      "4695\n",
      "4696\n",
      "4696\n",
      "4697\n",
      "4698\n",
      "4698\n",
      "4698\n",
      "4698\n",
      "4698\n",
      "4698\n",
      "4698\n",
      "4698\n",
      "4699\n",
      "4700\n",
      "4701\n",
      "4702\n",
      "4703\n",
      "4703\n",
      "4704\n",
      "4705\n",
      "4706\n",
      "4707\n",
      "4708\n",
      "4709\n",
      "4709\n",
      "4709\n",
      "4709\n",
      "4710\n",
      "4711\n",
      "4712\n",
      "4713\n",
      "4714\n",
      "4714\n",
      "4715\n",
      "4715\n",
      "4715\n",
      "4716\n",
      "4717\n",
      "4717\n",
      "4717\n",
      "4718\n",
      "4718\n",
      "4718\n",
      "4718\n",
      "4718\n",
      "4718\n",
      "4719\n",
      "4719\n",
      "4720\n",
      "4721\n",
      "4721\n",
      "4721\n",
      "4721\n",
      "4722\n",
      "4723\n",
      "4724\n",
      "4725\n",
      "4726\n",
      "4727\n",
      "4727\n",
      "4728\n",
      "4728\n",
      "4728\n",
      "4728\n",
      "4729\n",
      "4730\n",
      "4730\n",
      "4731\n",
      "4732\n",
      "4732\n",
      "4733\n",
      "4734\n",
      "4734\n",
      "4734\n",
      "4734\n",
      "4734\n",
      "4735\n",
      "4736\n",
      "4737\n",
      "4738\n",
      "4738\n",
      "4738\n",
      "4738\n",
      "4738\n",
      "4739\n",
      "4739\n",
      "4740\n",
      "4741\n",
      "4742\n",
      "4743\n",
      "4744\n",
      "4745\n",
      "4746\n",
      "4747\n",
      "4748\n",
      "4749\n",
      "4750\n",
      "4751\n",
      "4751\n",
      "4751\n",
      "4752\n",
      "4752\n",
      "4753\n",
      "4754\n",
      "4755\n",
      "4756\n",
      "4757\n",
      "4758\n",
      "4758\n",
      "4758\n",
      "4759\n",
      "4760\n",
      "4761\n",
      "4762\n",
      "4762\n",
      "4763\n",
      "4764\n",
      "4764\n",
      "4765\n",
      "4766\n",
      "4766\n",
      "4766\n",
      "4766\n",
      "4766\n",
      "4767\n",
      "4767\n",
      "4768\n",
      "4769\n",
      "4769\n",
      "4770\n",
      "4771\n",
      "4771\n",
      "4771\n",
      "4772\n",
      "4773\n",
      "4774\n",
      "4775\n",
      "4776\n",
      "4777\n",
      "4778\n",
      "4778\n",
      "4778\n",
      "4778\n",
      "4779\n",
      "4779\n",
      "4780\n",
      "4781\n",
      "4782\n",
      "4783\n",
      "4784\n",
      "4785\n",
      "4786\n",
      "4787\n",
      "4788\n",
      "4789\n",
      "4789\n",
      "4789\n",
      "4789\n",
      "4790\n",
      "4791\n",
      "4792\n",
      "4793\n",
      "4794\n",
      "4794\n",
      "4795\n",
      "4796\n",
      "4797\n",
      "4798\n",
      "4798\n",
      "4798\n",
      "4798\n",
      "4799\n",
      "4800\n",
      "4800\n",
      "4801\n",
      "4802\n",
      "4803\n",
      "4804\n",
      "4804\n",
      "4805\n",
      "4806\n",
      "4807\n",
      "4808\n",
      "4809\n",
      "4810\n",
      "4810\n",
      "4811\n",
      "4812\n",
      "4813\n",
      "4813\n",
      "4814\n",
      "4815\n",
      "4816\n",
      "4817\n",
      "4817\n",
      "4818\n",
      "4818\n",
      "4818\n",
      "4818\n",
      "4818\n",
      "4819\n",
      "4819\n",
      "4820\n",
      "4820\n",
      "4820\n",
      "4821\n",
      "4821\n",
      "4822\n",
      "4823\n",
      "4823\n",
      "4823\n",
      "4824\n",
      "4825\n",
      "4826\n",
      "4827\n",
      "4827\n",
      "4827\n",
      "4828\n",
      "4829\n",
      "4829\n",
      "4829\n",
      "4829\n",
      "4830\n",
      "4831\n",
      "4832\n",
      "4833\n",
      "4834\n",
      "4835\n",
      "4836\n",
      "4837\n",
      "4837\n",
      "4838\n",
      "4838\n",
      "4838\n",
      "4838\n",
      "4839\n",
      "4839\n",
      "4840\n",
      "4841\n",
      "4842\n",
      "4842\n",
      "4843\n",
      "4844\n",
      "4845\n",
      "4846\n",
      "4847\n",
      "4848\n",
      "4849\n",
      "4849\n",
      "4850\n",
      "4851\n",
      "4851\n",
      "4852\n",
      "4853\n",
      "4853\n",
      "4854\n",
      "4855\n",
      "4855\n",
      "4856\n",
      "4857\n",
      "4857\n",
      "4858\n",
      "4858\n",
      "4858\n",
      "4858\n",
      "4858\n",
      "4858\n",
      "4858\n",
      "4859\n",
      "4860\n",
      "4860\n",
      "4861\n",
      "4862\n",
      "4863\n",
      "4863\n",
      "4864\n",
      "4865\n",
      "4865\n",
      "4866\n",
      "4867\n",
      "4868\n",
      "4868\n",
      "4868\n",
      "4868\n",
      "4868\n",
      "4869\n",
      "4869\n",
      "4870\n",
      "4870\n",
      "4871\n",
      "4871\n",
      "4872\n",
      "4873\n",
      "4874\n",
      "4874\n",
      "4875\n",
      "4876\n",
      "4877\n",
      "4877\n",
      "4878\n",
      "4879\n",
      "4880\n",
      "4881\n",
      "4882\n",
      "4883\n",
      "4884\n",
      "4884\n",
      "4884\n",
      "4885\n",
      "4885\n",
      "4886\n",
      "4887\n",
      "4887\n",
      "4888\n",
      "4888\n",
      "4889\n",
      "4890\n",
      "4891\n",
      "4891\n",
      "4892\n",
      "4893\n",
      "4894\n",
      "4895\n",
      "4896\n",
      "4896\n",
      "4896\n",
      "4896\n",
      "4896\n",
      "4897\n",
      "4897\n",
      "4897\n",
      "4897\n",
      "4897\n",
      "4898\n",
      "4899\n",
      "4899\n",
      "4900\n",
      "4901\n",
      "4902\n",
      "4902\n",
      "4903\n",
      "4904\n",
      "4905\n",
      "4905\n",
      "4906\n",
      "4907\n",
      "4908\n",
      "4909\n",
      "4910\n",
      "4910\n",
      "4911\n",
      "4912\n",
      "4913\n",
      "4914\n",
      "4915\n",
      "4916\n",
      "4917\n",
      "4918\n",
      "4919\n",
      "4920\n",
      "4920\n",
      "4921\n",
      "4922\n",
      "4923\n",
      "4924\n",
      "4925\n",
      "4925\n",
      "4926\n",
      "4926\n",
      "4927\n",
      "4927\n",
      "4928\n",
      "4929\n",
      "4930\n",
      "4931\n",
      "4931\n",
      "4932\n",
      "4933\n",
      "4934\n",
      "4935\n",
      "4936\n",
      "4937\n",
      "4938\n",
      "4939\n",
      "4940\n",
      "4941\n",
      "4941\n",
      "4942\n",
      "4943\n",
      "4944\n",
      "4945\n",
      "4946\n",
      "4947\n",
      "4948\n",
      "4949\n",
      "4950\n",
      "4951\n",
      "4952\n",
      "4953\n",
      "4954\n",
      "4955\n",
      "4956\n",
      "4956\n",
      "4957\n",
      "4957\n",
      "4958\n",
      "4959\n",
      "4959\n",
      "4959\n",
      "4960\n",
      "4961\n",
      "4962\n",
      "4963\n",
      "4964\n",
      "4965\n",
      "4965\n",
      "4966\n",
      "4967\n",
      "4967\n",
      "4967\n",
      "4967\n",
      "4968\n",
      "4969\n",
      "4969\n",
      "4969\n",
      "4969\n",
      "4969\n",
      "4969\n",
      "4969\n",
      "4970\n",
      "4971\n",
      "4972\n",
      "4972\n",
      "4973\n",
      "4974\n",
      "4974\n",
      "4975\n",
      "4976\n",
      "4977\n",
      "4978\n",
      "4979\n",
      "4979\n",
      "4979\n",
      "4980\n",
      "4981\n",
      "4981\n",
      "4982\n",
      "4982\n",
      "4982\n",
      "4982\n",
      "4982\n",
      "4983\n",
      "4984\n",
      "4985\n",
      "4986\n",
      "4987\n",
      "4988\n",
      "4989\n",
      "4989\n",
      "4989\n",
      "4989\n",
      "4990\n",
      "4991\n",
      "4991\n",
      "4992\n",
      "4993\n",
      "4994\n",
      "4994\n",
      "4995\n",
      "4996\n",
      "4996\n",
      "4996\n",
      "4996\n",
      "4996\n",
      "4997\n",
      "4998\n",
      "4999\n",
      "4999\n",
      "4999\n"
     ]
    }
   ],
   "source": [
    "crawl(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltisoJE9tUT2"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "jsonString = json.dumps(papers)\n",
    "jsonFile = open(\"data.json\", \"w\")\n",
    "jsonFile.write(jsonString)\n",
    "jsonFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-__CVsYiWZf"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "برای سیو کردن آرایه از آبجکت ها تحت فرمت json از کتابخانه json استفاده کردیم.\n",
    "\n",
    "یک فایل json ساخته می شود که حاوی اطلاعات مقالات کراول شده است.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-jbSfmnvf3nO",
    "outputId": "8e52a8a4-7a28-483a-f404-83e85986ea3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'abstract': 'In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.',\n",
       "  'authors': ['Han Zhang 1',\n",
       "   ' Ian Goodfellow 1',\n",
       "   ' Dimitris Metaxas 2',\n",
       "   ' Augustus Odena 1'],\n",
       "  'date': '2018',\n",
       "  'identifier': '2950893734',\n",
       "  'references': ['2964121744',\n",
       "   '2963403868',\n",
       "   '2117539524',\n",
       "   '2099471712',\n",
       "   '2964308564',\n",
       "   '2963073614',\n",
       "   '2962793481',\n",
       "   '2963684088',\n",
       "   '2963373786',\n",
       "   '2963470893'],\n",
       "  'title': 'Self-Attention Generative Adversarial Networks'},\n",
       " {'abstract': 'Abstract: We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.',\n",
       "  'authors': ['Diederik P. Kingma 1', ' Jimmy Lei Ba 2'],\n",
       "  'date': '2015',\n",
       "  'identifier': '2964121744',\n",
       "  'references': ['2963403868',\n",
       "   '2962739339',\n",
       "   '2963073614',\n",
       "   '2962793481',\n",
       "   '2331128040',\n",
       "   '2963470893',\n",
       "   '2964015378',\n",
       "   '1514535095',\n",
       "   '2508457857'],\n",
       "  'title': 'Adam: A Method for Stochastic Optimization'},\n",
       " {'abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms. We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.',\n",
       "  'authors': ['Ashish Vaswani 1',\n",
       "   ' Noam Shazeer 1',\n",
       "   ' Niki Parmar 2',\n",
       "   ' Jakob Uszkoreit 1',\n",
       "   ' Llion Jones 1',\n",
       "   ' Aidan N. Gomez 1',\n",
       "   ' Lukasz Kaiser 1',\n",
       "   ' Illia Polosukhin 1'],\n",
       "  'date': '2017',\n",
       "  'identifier': '2963403868',\n",
       "  'references': ['2963341956',\n",
       "   '2965373594',\n",
       "   '2970597249',\n",
       "   '2963091558',\n",
       "   '2923014074',\n",
       "   '2996428491',\n",
       "   '2911489562',\n",
       "   '2964110616'],\n",
       "  'title': 'Attention is All You Need'},\n",
       " {'abstract': 'Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.',\n",
       "  'authors': ['Christian Ledig 1',\n",
       "   ' Lucas Theis 1',\n",
       "   ' Ferenc Huszar 2',\n",
       "   ' Jose Caballero 2',\n",
       "   ' Andrew Cunningham ',\n",
       "   ' Alejandro Acosta 2',\n",
       "   ' Andrew Aitken 2',\n",
       "   ' Alykhan Tejani 2',\n",
       "   ' Johannes Totz 2',\n",
       "   ' Zehan Wang 2',\n",
       "   ' Wenzhe Shi 2'],\n",
       "  'date': '2017',\n",
       "  'identifier': '2963470893',\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '2962835968',\n",
       "   '2964121744',\n",
       "   '2097117768',\n",
       "   '1836465849',\n",
       "   '2117539524',\n",
       "   '2099471712',\n",
       "   '1677182931',\n",
       "   '2133665775'],\n",
       "  'title': 'Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network'},\n",
       " {'abstract': 'We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.',\n",
       "  'authors': ['Christian Szegedy 1',\n",
       "   ' Wei Liu 2',\n",
       "   ' Yangqing Jia 1',\n",
       "   ' Pierre Sermanet 1',\n",
       "   ' Scott Reed 3',\n",
       "   ' Dragomir Anguelov 1',\n",
       "   ' Dumitru Erhan 1',\n",
       "   ' Vincent Vanhoucke 1',\n",
       "   ' Andrew Rabinovich 4'],\n",
       "  'date': '2015',\n",
       "  'identifier': '2097117768',\n",
       "  'references': ['2618530766',\n",
       "   '2102605133',\n",
       "   '1849277567',\n",
       "   '2963542991',\n",
       "   '2310919327',\n",
       "   '1904365287',\n",
       "   '2963911037',\n",
       "   '2168231600',\n",
       "   '2068730032',\n",
       "   '104184427'],\n",
       "  'title': 'Going deeper with convolutions'},\n",
       " {'abstract': 'Human ability to understand language is general, flexible, and robust. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.',\n",
       "  'authors': ['Alex Wang 1',\n",
       "   ' Amanpreet Singh 1',\n",
       "   ' Julian Michael 2',\n",
       "   ' Felix Hill 3',\n",
       "   ' Omer Levy 4',\n",
       "   ' Samuel R. Bowman 1'],\n",
       "  'date': '2018',\n",
       "  'identifier': '2923014074',\n",
       "  'references': ['2962739339',\n",
       "   '2251939518',\n",
       "   '2963748441',\n",
       "   '1486649854',\n",
       "   '2963918774',\n",
       "   '2963846996',\n",
       "   '2525127255',\n",
       "   '2962736243',\n",
       "   '3104033643',\n",
       "   '2130158090'],\n",
       "  'title': 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding'},\n",
       " {'abstract': 'The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.',\n",
       "  'authors': ['Olga Russakovsky 1',\n",
       "   ' Jia Deng 2',\n",
       "   ' Hao Su 1',\n",
       "   ' Jonathan Krause 1',\n",
       "   ' Sanjeev Satheesh 1',\n",
       "   ' Sean Ma 1',\n",
       "   ' Zhiheng Huang 1',\n",
       "   ' Andrej Karpathy 1',\n",
       "   ' Aditya Khosla 3',\n",
       "   ' Michael Bernstein 1',\n",
       "   ' Alexander C. Berg 4',\n",
       "   ' Li Fei-Fei 1'],\n",
       "  'date': '2015',\n",
       "  'identifier': '2117539524',\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2151103935',\n",
       "   '2097117768',\n",
       "   '2102605133',\n",
       "   '1614298861',\n",
       "   '2108598243',\n",
       "   '2155893237',\n",
       "   '2168356304',\n",
       "   '1849277567'],\n",
       "  'title': 'ImageNet Large Scale Visual Recognition Challenge'},\n",
       " {'abstract': 'Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to push F(G(X)) ≈ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.',\n",
       "  'authors': ['Jun-Yan Zhu 1',\n",
       "   ' Taesung Park 2',\n",
       "   ' Phillip Isola 2',\n",
       "   ' Alexei A. Efros 2'],\n",
       "  'date': '2017',\n",
       "  'identifier': '2962793481',\n",
       "  'references': ['2194775991',\n",
       "   '2962835968',\n",
       "   '2117539524',\n",
       "   '1903029394',\n",
       "   '2099471712',\n",
       "   '1959608418',\n",
       "   '2100495367',\n",
       "   '2963684088',\n",
       "   '2340897893',\n",
       "   '2963373786'],\n",
       "  'title': 'Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks'},\n",
       " {'abstract': 'Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO.',\n",
       "  'authors': ['Kelvin Xu 1',\n",
       "   ' Jimmy Ba 2',\n",
       "   ' Ryan Kiros 2',\n",
       "   ' Kyunghyun Cho 1',\n",
       "   ' Aaron Courville 1',\n",
       "   ' Ruslan Salakhudinov 2',\n",
       "   ' 3',\n",
       "   ' Rich Zemel 2',\n",
       "   ' 3',\n",
       "   ' Yoshua Bengio 1',\n",
       "   ' 3'],\n",
       "  'date': '2015',\n",
       "  'identifier': '1514535095',\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2964121744',\n",
       "   '2097117768',\n",
       "   '2117539524',\n",
       "   '2964308564',\n",
       "   '2095705004',\n",
       "   '2130942839',\n",
       "   '2157331557',\n",
       "   '1861492603'],\n",
       "  'title': 'Show, Attend and Tell: Neural Image Caption Generation with Visual Attention'},\n",
       " {'abstract': 'Abstract: In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.',\n",
       "  'authors': ['Karen Simonyan ', ' Andrew Zisserman'],\n",
       "  'date': '2015',\n",
       "  'identifier': '2962835968',\n",
       "  'references': ['2194775991',\n",
       "   '639708223',\n",
       "   '1901129140',\n",
       "   '1903029394',\n",
       "   '1536680647',\n",
       "   '3106250896'],\n",
       "  'title': 'Very Deep Convolutional Networks for Large-Scale Image Recognition'},\n",
       " {'abstract': 'Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.',\n",
       "  'authors': ['Yinhan Liu ',\n",
       "   ' Myle Ott ',\n",
       "   ' Naman Goyal ',\n",
       "   ' Jingfei Du ',\n",
       "   ' Mandar Joshi ',\n",
       "   ' Danqi Chen ',\n",
       "   ' Omer Levy ',\n",
       "   ' Mike Lewis ',\n",
       "   ' Luke Zettlemoyer ',\n",
       "   ' Veselin Stoyanov'],\n",
       "  'date': '2019',\n",
       "  'identifier': '2965373594',\n",
       "  'references': ['2964121744',\n",
       "   '2963403868',\n",
       "   '2963341956',\n",
       "   '2251939518',\n",
       "   '2899771611',\n",
       "   '2963748441',\n",
       "   '2962784628',\n",
       "   '1840435438',\n",
       "   '2970597249',\n",
       "   '2963026768'],\n",
       "  'title': 'RoBERTa: A Robustly Optimized BERT Pretraining Approach'},\n",
       " {'abstract': 'State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with ’attention’ mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3] , our detection system has a frame rate of 5 fps ( including all steps ) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.',\n",
       "  'authors': ['Shaoqing Ren 1',\n",
       "   ' Kaiming He 2',\n",
       "   ' Ross Girshick 3',\n",
       "   ' Jian Sun 2'],\n",
       "  'date': '2017',\n",
       "  'identifier': '639708223',\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '2102605133',\n",
       "   '2117539524',\n",
       "   '1903029394',\n",
       "   '2155893237',\n",
       "   '1536680647',\n",
       "   '2168356304'],\n",
       "  'title': 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks'},\n",
       " {'abstract': 'We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.',\n",
       "  'authors': ['Tomas Mikolov 1',\n",
       "   ' Kai Chen 2',\n",
       "   ' Greg S. Corrado 2',\n",
       "   ' Jeffrey Dean 2'],\n",
       "  'date': '2013',\n",
       "  'identifier': '1614298861',\n",
       "  'references': ['2153579005',\n",
       "   '2250539671',\n",
       "   '2271840356',\n",
       "   '1895577753',\n",
       "   '3104097132',\n",
       "   '1888005072',\n",
       "   '1486649854',\n",
       "   '2964321699',\n",
       "   '2100664567',\n",
       "   '2123024445'],\n",
       "  'title': 'Efficient Estimation of Word Representations in Vector Space'},\n",
       " {'abstract': 'Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.',\n",
       "  'authors': ['Matthew D. Zeiler ', ' Rob Fergus'],\n",
       "  'date': '2014',\n",
       "  'identifier': '1849277567',\n",
       "  'references': ['2618530766',\n",
       "   '2102605133',\n",
       "   '2108598243',\n",
       "   '2136922672',\n",
       "   '1904365287',\n",
       "   '2155541015',\n",
       "   '2546302380',\n",
       "   '2025768430',\n",
       "   '2110798204',\n",
       "   '2097018403'],\n",
       "  'title': 'Visualizing and Understanding Convolutional Networks'},\n",
       " {'abstract': 'We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.',\n",
       "  'authors': ['Alex Krizhevsky 1',\n",
       "   ' Ilya Sutskever 1',\n",
       "   ' Geoffrey E. Hinton 2'],\n",
       "  'date': '2017',\n",
       "  'identifier': '2618530766',\n",
       "  'references': ['2194775991',\n",
       "   '2097117768',\n",
       "   '2108598243',\n",
       "   '2911964244',\n",
       "   '3118608800',\n",
       "   '1904365287',\n",
       "   '1665214252',\n",
       "   '2546302380',\n",
       "   '2110764733',\n",
       "   '2130325614'],\n",
       "  'title': 'ImageNet classification with deep convolutional neural networks'},\n",
       " {'abstract': 'Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.',\n",
       "  'authors': ['Ross Girshick ',\n",
       "   ' Jeff Donahue ',\n",
       "   ' Trevor Darrell ',\n",
       "   ' Jitendra Malik'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2102605133',\n",
       "  'references': ['2618530766',\n",
       "   '2151103935',\n",
       "   '2161969291',\n",
       "   '2108598243',\n",
       "   '2168356304',\n",
       "   '3118608800',\n",
       "   '2310919327',\n",
       "   '2031489346',\n",
       "   '2088049833',\n",
       "   '2155541015'],\n",
       "  'title': 'Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation'},\n",
       " {'abstract': 'Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.',\n",
       "  'authors': ['Alexis Conneau 1',\n",
       "   ' Douwe Kiela 2',\n",
       "   ' Holger Schwenk 3',\n",
       "   ' Loïc Barrault 3',\n",
       "   ' Antoine Bordes 1'],\n",
       "  'date': '2017',\n",
       "  'identifier': '2963918774',\n",
       "  'references': ['2194775991',\n",
       "   '2964121744',\n",
       "   '2153579005',\n",
       "   '2250539671',\n",
       "   '2130942839',\n",
       "   '2108598243',\n",
       "   '2158899491',\n",
       "   '2064675550',\n",
       "   '2131744502',\n",
       "   '2145287260'],\n",
       "  'title': 'Supervised Learning of Universal Sentence Representations from Natural Language Inference Data'},\n",
       " {'abstract': 'Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.',\n",
       "  'authors': ['Yoshua Bengio ',\n",
       "   ' Pascal Lamblin ',\n",
       "   ' Dan Popovici ',\n",
       "   ' Hugo Larochelle'],\n",
       "  'date': '2006',\n",
       "  'identifier': '2110798204',\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2116064496',\n",
       "   '2613634265',\n",
       "   '2124914669',\n",
       "   '1993845689',\n",
       "   '2109779438',\n",
       "   '2103626435',\n",
       "   '2125569215',\n",
       "   '2130313186'],\n",
       "  'title': 'Greedy Layer-Wise Training of Deep Networks'},\n",
       " {'abstract': 'We show how to use \"complementary priors\" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.',\n",
       "  'authors': ['Geoffrey E. Hinton 1', ' Simon Osindero 1', ' Yee-Whye Teh 2'],\n",
       "  'date': '2006',\n",
       "  'identifier': '2136922672',\n",
       "  'references': ['2310919327',\n",
       "   '2116064496',\n",
       "   '2057175746',\n",
       "   '2159080219',\n",
       "   '2156163116',\n",
       "   '2131686571',\n",
       "   '2567948266',\n",
       "   '2158778629',\n",
       "   '2159737176',\n",
       "   '2124914669'],\n",
       "  'title': 'A fast learning algorithm for deep belief nets'},\n",
       " {'abstract': 'Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.',\n",
       "  'authors': ['Zhou Wang 1',\n",
       "   ' A.C. Bovik 2',\n",
       "   ' H.R. Sheikh 2',\n",
       "   ' E.P. Simoncelli 3'],\n",
       "  'date': '2004',\n",
       "  'identifier': '2133665775',\n",
       "  'references': ['2159269332',\n",
       "   '2142276208',\n",
       "   '2053691921',\n",
       "   '2118217749',\n",
       "   '2153777140',\n",
       "   '2912116903',\n",
       "   '2107790757',\n",
       "   '2158564760',\n",
       "   '2124731682',\n",
       "   '2115838129'],\n",
       "  'title': 'Image quality assessment: from error visibility to structural similarity'},\n",
       " {'abstract': 'High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such \"autoencoder\" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.',\n",
       "  'authors': ['G. E. Hinton ', ' R. R. Salakhutdinov'],\n",
       "  'date': '2006',\n",
       "  'identifier': '2100495367',\n",
       "  'references': ['2136922672',\n",
       "   '2053186076',\n",
       "   '2001141328',\n",
       "   '2293063825',\n",
       "   '2121122425',\n",
       "   '2032647857',\n",
       "   '2021774695'],\n",
       "  'title': 'Reducing the Dimensionality of Data with Neural Networks'},\n",
       " {'abstract': 'We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.',\n",
       "  'authors': ['Ronan Collobert ',\n",
       "   ' Jason Weston 1',\n",
       "   ' Léon Bottou ',\n",
       "   ' Michael Karlen ',\n",
       "   ' Koray Kavukcuoglu 2',\n",
       "   ' Pavel Kuksa 3'],\n",
       "  'date': '2011',\n",
       "  'identifier': '2158899491',\n",
       "  'references': ['2136922672',\n",
       "   '2310919327',\n",
       "   '2147880316',\n",
       "   '2125838338',\n",
       "   '2110798204',\n",
       "   '2158139315',\n",
       "   '2159080219',\n",
       "   '2098162425',\n",
       "   '2150102617',\n",
       "   '2296073425'],\n",
       "  'title': 'Natural Language Processing (Almost) from Scratch'},\n",
       " {'abstract': \"This paper presents the Third PASCAL Recognising Textual Entailment Challenge (RTE-3), providing an overview of the dataset creating methodology and the submitted systems. In creating this year's dataset, a number of longer texts were introduced to make the challenge more oriented to realistic scenarios. Additionally, a pool of resources was offered so that the participants could share common tools. A pilot task was also set up, aimed at differentiating unknown entailments from identified contradictions and providing justifications for overall system decisions. 26 participants submitted 44 runs, using different approaches and generally presenting new entailment models and achieving higher scores than in the previous challenges.\",\n",
       "  'authors': ['Danilo Giampiccolo 1',\n",
       "   ' Bernardo Magnini 2',\n",
       "   ' Ido Dagan 3',\n",
       "   ' Bill Dolan 4'],\n",
       "  'date': '2007',\n",
       "  'identifier': '2130158090',\n",
       "  'references': ['2912565176',\n",
       "   '2525127255',\n",
       "   '2115792525',\n",
       "   '2102065370',\n",
       "   '2396767181',\n",
       "   '2002664886',\n",
       "   '1990524510',\n",
       "   '137514618',\n",
       "   '2182572585',\n",
       "   '2134061542'],\n",
       "  'title': 'The Third PASCAL Recognizing Textual Entailment Challenge'},\n",
       " {'abstract': 'The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.',\n",
       "  'authors': ['Jia Deng ',\n",
       "   ' Wei Dong ',\n",
       "   ' Richard Socher ',\n",
       "   ' Li-Jia Li ',\n",
       "   ' Kai Li ',\n",
       "   ' Li Fei-Fei'],\n",
       "  'date': '2009',\n",
       "  'identifier': '2108598243',\n",
       "  'references': ['2151103935',\n",
       "   '2038721957',\n",
       "   '2128017662',\n",
       "   '2110764733',\n",
       "   '1782590233',\n",
       "   '1576445103',\n",
       "   '2145607950',\n",
       "   '2141282920',\n",
       "   '2115733720',\n",
       "   '1528789833'],\n",
       "  'title': 'ImageNet: A large-scale hierarchical image database'},\n",
       " {'abstract': 'Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.',\n",
       "  'authors': ['Nitish Srivastava ',\n",
       "   ' Geoffrey Hinton ',\n",
       "   ' Alex Krizhevsky ',\n",
       "   ' Ilya Sutskever ',\n",
       "   ' Ruslan Salakhutdinov'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2095705004',\n",
       "  'references': ['2618530766',\n",
       "   '2136922672',\n",
       "   '3118608800',\n",
       "   '2100495367',\n",
       "   '2135046866',\n",
       "   '2546302380',\n",
       "   '2294059674',\n",
       "   '2145094598',\n",
       "   '2025768430',\n",
       "   '2131241448'],\n",
       "  'title': 'Dropout: a simple way to prevent neural networks from overfitting'},\n",
       " {'abstract': 'We present a novel approach to measuring similarity between shapes and exploit it for object recognition. In our framework, the measurement of similarity is preceded by: (1) solving for correspondences between points on the two shapes; (2) using the correspondences to estimate an aligning transform. In order to solve the correspondence problem, we attach a descriptor, the shape context, to each point. The shape context at a reference point captures the distribution of the remaining points relative to it, thus offering a globally discriminative characterization. Corresponding points on two similar shapes will have similar shape contexts, enabling us to solve for correspondences as an optimal assignment problem. Given the point correspondences, we estimate the transformation that best aligns the two shapes; regularized thin-plate splines provide a flexible class of transformation maps for this purpose. The dissimilarity between the two shapes is computed as a sum of matching errors between corresponding points, together with a term measuring the magnitude of the aligning transform. We treat recognition in a nearest-neighbor classification framework as the problem of finding the stored prototype shape that is maximally similar to that in the image. Results are presented for silhouettes, trademarks, handwritten digits, and the COIL data set.',\n",
       "  'authors': ['S. Belongie 1', ' J. Malik 2', ' J. Puzicha 3'],\n",
       "  'date': '2002',\n",
       "  'identifier': '2057175746',\n",
       "  'references': ['2310919327',\n",
       "   '2124386111',\n",
       "   '2119821739',\n",
       "   '2138451337',\n",
       "   '2117812871',\n",
       "   '2038952578',\n",
       "   '2124087378',\n",
       "   '2123977795',\n",
       "   '2101522199',\n",
       "   '2146766088'],\n",
       "  'title': 'Shape matching and object recognition using shape contexts'},\n",
       " {'abstract': 'Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.',\n",
       "  'authors': ['Jonathan Long ', ' Evan Shelhamer ', ' Trevor Darrell'],\n",
       "  'date': '2015',\n",
       "  'identifier': '1903029394',\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '2102605133',\n",
       "   '2155893237',\n",
       "   '1663973292',\n",
       "   '1849277567',\n",
       "   '2963542991',\n",
       "   '2109255472',\n",
       "   '2155541015'],\n",
       "  'title': 'Fully convolutional networks for semantic segmentation'},\n",
       " {'abstract': 'Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.',\n",
       "  'authors': ['Kaiming He ', ' Xiangyu Zhang ', ' Shaoqing Ren ', ' Jian Sun'],\n",
       "  'date': '2016',\n",
       "  'identifier': '2194775991',\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '639708223',\n",
       "   '1836465849',\n",
       "   '2102605133',\n",
       "   '2117539524',\n",
       "   '1903029394',\n",
       "   '2155893237',\n",
       "   '1536680647'],\n",
       "  'title': 'Deep Residual Learning for Image Recognition'},\n",
       " {'abstract': 'Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.',\n",
       "  'authors': ['Pascal Vincent ',\n",
       "   ' Hugo Larochelle ',\n",
       "   ' Yoshua Bengio ',\n",
       "   ' Pierre-Antoine Manzagol'],\n",
       "  'date': '2008',\n",
       "  'identifier': '2025768430',\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2072128103',\n",
       "   '2110798204',\n",
       "   '2153663612',\n",
       "   '1652505363',\n",
       "   '1498436455',\n",
       "   '1994197834',\n",
       "   '2293063825',\n",
       "   '2172174689'],\n",
       "  'title': 'Extracting and composing robust features with denoising autoencoders'},\n",
       " {'abstract': 'The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data. High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.',\n",
       "  'authors': ['Corinna Cortes ', ' Vladimir Vapnik'],\n",
       "  'date': '1995',\n",
       "  'identifier': '2119821739',\n",
       "  'references': ['2154642048',\n",
       "   '1498436455',\n",
       "   '2087347434',\n",
       "   '1530699444',\n",
       "   '2154579312',\n",
       "   '2168228682',\n",
       "   '2504871398',\n",
       "   '1568787085',\n",
       "   '5594912',\n",
       "   '2322002063'],\n",
       "  'title': 'Support-Vector Networks'},\n",
       " {'abstract': \"SUMMARY We propose a new method for estimation in linear models. The 'lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.\",\n",
       "  'authors': ['Robert Tibshirani'],\n",
       "  'date': '1996',\n",
       "  'identifier': '2135046866',\n",
       "  'references': ['2331432542',\n",
       "   '3085162807',\n",
       "   '2158940042',\n",
       "   '1594031697',\n",
       "   '2797583072',\n",
       "   '2106706098',\n",
       "   '2102201073',\n",
       "   '2117897510',\n",
       "   '2075665712',\n",
       "   '2954064014'],\n",
       "  'title': 'Regression Shrinkage and Selection via the Lasso'},\n",
       " {'abstract': 'We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.',\n",
       "  'authors': ['David E. Rumelhart 1',\n",
       "   ' Geoffrey E. Hinton 2',\n",
       "   ' Ronald J. Williams 1'],\n",
       "  'date': '1988',\n",
       "  'identifier': '1498436455',\n",
       "  'references': ['1652505363', '2322002063'],\n",
       "  'title': 'Learning representations by back-propagating errors'},\n",
       " {'abstract': 'It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual \"expert\" models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called \"contrastive divergence\" whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data.',\n",
       "  'authors': ['Geoffrey E. Hinton'],\n",
       "  'date': '2002',\n",
       "  'identifier': '2116064496',\n",
       "  'references': ['1652505363',\n",
       "   '1997063559',\n",
       "   '2096175520',\n",
       "   '1746680969',\n",
       "   '1993845689',\n",
       "   '2165225968',\n",
       "   '2083380015',\n",
       "   '1547224907',\n",
       "   '2114153178',\n",
       "   '2101706260'],\n",
       "  'title': 'Training products of experts by minimizing contrastive divergence'},\n",
       " {'abstract': 'Directed graphical models with one layer of observed random variables and one or more layers of hidden random variables have been the dominant modelling paradigm in many research fields. Although this approach has met with considerable success, the causal semantics of these models can make it difficult to infer the posterior distribution over the hidden variables. In this paper we propose an alternative two-layer model based on exponential family distributions and the semantics of undirected models. Inference in these \"exponential family harmoniums\" is fast while learning is performed by minimizing contrastive divergence. A member of this family is then studied as an alternative probabilistic model for latent semantic indexing. In experiments it is shown that they perform well on document retrieval tasks and provide an elegant solution to searching with keywords.',\n",
       "  'authors': ['Max Welling 1', ' Michal Rosen-zvi 1', ' Geoffrey E. Hinton 2'],\n",
       "  'date': '2004',\n",
       "  'identifier': '2124914669',\n",
       "  'references': ['1880262756',\n",
       "   '2116064496',\n",
       "   '2147152072',\n",
       "   '1612003148',\n",
       "   '2140124448',\n",
       "   '1934021597',\n",
       "   '2138448681',\n",
       "   '145818128',\n",
       "   '2109720450',\n",
       "   '1813659000'],\n",
       "  'title': 'Exponential Family Harmoniums with an Application to Information Retrieval'},\n",
       " {'abstract': 'Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English!German and English!Russian by up to 1.1 and 1.3 BLEU, respectively.',\n",
       "  'authors': ['Rico Sennrich ', ' Barry Haddow ', ' Alexandra Birch'],\n",
       "  'date': '2016',\n",
       "  'identifier': '2962784628',\n",
       "  'references': ['2964308564',\n",
       "   '2130942839',\n",
       "   '2157331557',\n",
       "   '1902237438',\n",
       "   '6908809',\n",
       "   '1753482797',\n",
       "   '2124807415',\n",
       "   '2251012068',\n",
       "   '1815076433',\n",
       "   '2100664567'],\n",
       "  'title': 'Neural Machine Translation of Rare Words with Subword Units'},\n",
       " {'abstract': 'Probability Distributions.- Linear Models for Regression.- Linear Models for Classification.- Neural Networks.- Kernel Methods.- Sparse Kernel Machines.- Graphical Models.- Mixture Models and EM.- Approximate Inference.- Sampling Methods.- Continuous Latent Variables.- Sequential Data.- Combining Models.',\n",
       "  'authors': ['Christopher M. Bishop'],\n",
       "  'date': '2006',\n",
       "  'identifier': '1663973292',\n",
       "  'references': ['2117812871', '1496357020'],\n",
       "  'title': 'Pattern Recognition and Machine Learning'},\n",
       " {'abstract': 'With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment setting, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.',\n",
       "  'authors': ['Zhilin Yang 1',\n",
       "   ' Zihang Dai 1',\n",
       "   ' Yiming Yang 1',\n",
       "   ' Jaime G. Carbonell 1',\n",
       "   ' Ruslan Salakhutdinov 1',\n",
       "   ' Quoc V. Le 2'],\n",
       "  'date': '2019',\n",
       "  'identifier': '2970597249',\n",
       "  'references': ['2996035354',\n",
       "   '2990704537',\n",
       "   '3100307207',\n",
       "   '3105966348',\n",
       "   '3034238904',\n",
       "   '3099342932',\n",
       "   '2995015695'],\n",
       "  'title': 'XLNet: Generalized Autoregressive Pretraining for Language Understanding'},\n",
       " {'abstract': 'We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For \\\\(300 \\\\times 300\\\\) input, SSD achieves 74.3 % mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for \\\\(512 \\\\times 512\\\\) input, SSD achieves 76.9 % mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.',\n",
       "  'authors': ['Wei Liu 1',\n",
       "   ' Dragomir Anguelov 2',\n",
       "   ' Dumitru Erhan 3',\n",
       "   ' Christian Szegedy 3',\n",
       "   ' Scott E. Reed 4',\n",
       "   ' Cheng-Yang Fu 1',\n",
       "   ' Alexander C. Berg 1'],\n",
       "  'date': '2016',\n",
       "  'identifier': '3106250896',\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '639708223',\n",
       "   '1836465849',\n",
       "   '2102605133',\n",
       "   '2117539524',\n",
       "   '1903029394',\n",
       "   '2155893237'],\n",
       "  'title': 'SSD: Single Shot MultiBox Detector'},\n",
       " {'abstract': \"This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the ``LINE,'' which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online\\\\footnote{\\\\url{https://github.com/tangjianpku/LINE}}.\",\n",
       "  'authors': ['Jian Tang 1',\n",
       "   ' Meng Qu 2',\n",
       "   ' Mingzhe Wang 2',\n",
       "   ' Ming Zhang 2',\n",
       "   ' Jun Yan 1',\n",
       "   ' Qiaozhu Mei 3'],\n",
       "  'date': '2015',\n",
       "  'identifier': '1888005072',\n",
       "  'references': ['2153579005',\n",
       "   '1614298861',\n",
       "   '2187089797',\n",
       "   '1532325895',\n",
       "   '2131744502',\n",
       "   '2053186076',\n",
       "   '2001141328',\n",
       "   '3104097132',\n",
       "   '1854214752',\n",
       "   '2156718197'],\n",
       "  'title': 'LINE: Large-scale Information Network Embedding'},\n",
       " {'abstract': 'With the advent of the Internet, billions of images are now freely available online and constitute a dense sampling of the visual world. Using a variety of non-parametric methods, we explore this world with the aid of a large dataset of 79,302,017 images collected from the Internet. Motivated by psychophysical results showing the remarkable tolerance of the human visual system to degradations in image resolution, the images in the dataset are stored as 32 x 32 color images. Each image is loosely labeled with one of the 75,062 non-abstract nouns in English, as listed in the Wordnet lexical database. Hence the image database gives a comprehensive coverage of all object categories and scenes. The semantic information from Wordnet can be used in conjunction with nearest-neighbor methods to perform object classification over a range of semantic levels minimizing the effects of labeling noise. For certain classes that are particularly prevalent in the dataset, such as people, we are able to demonstrate a recognition performance comparable to class-specific Viola-Jones style detectors.',\n",
       "  'authors': ['A. Torralba 1', ' R. Fergus 2', ' W.T. Freeman 1'],\n",
       "  'date': '2008',\n",
       "  'identifier': '2145607950',\n",
       "  'references': ['2164598857',\n",
       "   '2162915993',\n",
       "   '2124386111',\n",
       "   '2038721957',\n",
       "   '2128017662',\n",
       "   '2110764733',\n",
       "   '1576445103',\n",
       "   '2107034620',\n",
       "   '1566135517',\n",
       "   '2111993661'],\n",
       "  'title': '80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition'},\n",
       " {'abstract': \"Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.\",\n",
       "  'authors': ['Sergey Ioffe ', ' Christian Szegedy'],\n",
       "  'date': '2015',\n",
       "  'identifier': '1836465849',\n",
       "  'references': ['2097117768',\n",
       "   '2117539524',\n",
       "   '2095705004',\n",
       "   '1677182931',\n",
       "   '2146502635',\n",
       "   '2310919327',\n",
       "   '1665214252',\n",
       "   '2168231600',\n",
       "   '1533861849',\n",
       "   '104184427'],\n",
       "  'title': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift'},\n",
       " {'abstract': 'Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.',\n",
       "  'authors': ['Jeffrey Dean ',\n",
       "   ' Greg Corrado ',\n",
       "   ' Rajat Monga ',\n",
       "   ' Kai Chen ',\n",
       "   ' Matthieu Devin ',\n",
       "   ' Mark Mao ',\n",
       "   \" Marc'aurelio Ranzato \",\n",
       "   ' Andrew Senior ',\n",
       "   ' Paul Tucker ',\n",
       "   ' Ke Yang ',\n",
       "   ' Quoc V. Le ',\n",
       "   ' Andrew Y. Ng'],\n",
       "  'date': '2012',\n",
       "  'identifier': '2168231600',\n",
       "  'references': ['2173213060',\n",
       "   '2108598243',\n",
       "   '2146502635',\n",
       "   '3118608800',\n",
       "   '2117130368',\n",
       "   '2147768505',\n",
       "   '2132339004',\n",
       "   '2141125852',\n",
       "   '2184045248',\n",
       "   '2118858186'],\n",
       "  'title': 'Large Scale Distributed Deep Networks'},\n",
       " {'abstract': 'In DUC 2005, the pyramid method for content evaluation was used for the first time in a crosssite evaluation. We discuss the method used in creating pyramid models and performing peer annotation. Analysis of score averages for the peers indicates that the best systems score half as well as humans, and that systems can be grouped into better and worse performers. There were few significant differences among systems. High score correlations between sets from different annotators, and good interannotator agreement, indicate that participants can perform annotation reliably. We found that a modified pyramid score gave good results and would simplify peer annotation in the future.',\n",
       "  'authors': ['Kathleen McKeown ',\n",
       "   ' Rebecca Passonneau ',\n",
       "   ' Ani Nenkova ',\n",
       "   ' Sergey Sigelman'],\n",
       "  'date': '2005',\n",
       "  'identifier': '2134061542',\n",
       "  'references': ['3021916629',\n",
       "   '2102065370',\n",
       "   '2036002592',\n",
       "   '2002664886',\n",
       "   '1940278502',\n",
       "   '1977747299',\n",
       "   '1985463960',\n",
       "   '1501617060',\n",
       "   '2907147533',\n",
       "   '2128530885'],\n",
       "  'title': 'Applying the Pyramid Method in DUC 2005'},\n",
       " {'abstract': 'Abstract: How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.',\n",
       "  'authors': ['Diederik P Kingma ', ' Max Welling'],\n",
       "  'date': '2014',\n",
       "  'identifier': '1959608418',\n",
       "  'references': ['2146502635',\n",
       "   '2163922914',\n",
       "   '2145094598',\n",
       "   '2166851633',\n",
       "   '2097268041',\n",
       "   '2963173382',\n",
       "   '2951493172',\n",
       "   '2171490498',\n",
       "   '2119196781',\n",
       "   '3104819538'],\n",
       "  'title': 'Auto-Encoding Variational Bayes'},\n",
       " {'abstract': 'SUMMARY With ideal spatial adaptation, an oracle furnishes information about how best to adapt a spatially variable estimator, whether piecewise constant, piecewise polynomial, variable knot spline, or variable bandwidth kernel, to the unknown function. Estimation with the aid of an oracle offers dramatic advantages over traditional linear estimation by nonadaptive kernels; however, it is a priori unclear whether such performance can be obtained by a procedure relying on the data alone. We describe a new principle for spatially-adaptive estimation: selective wavelet reconstruction. We show that variable-knot spline fits and piecewise-polynomial fits, when equipped with an oracle to select the knots, are not dramatically more powerful than selective wavelet reconstruction with an oracle. We develop a practical spatially adaptive method, RiskShrink, which works by shrinkage of empirical wavelet coefficients. RiskShrink mimics the performance of an oracle for selective wavelet reconstruction as well as it is possible to do so. A new inequality in multivariate normal decision theory which we call the oracle inequality shows that attained performance differs from ideal performance by at most a factor of approximately 2 log n, where n is the sample size. Moreover no estimator can give a better guarantee than this. Within the class of spatially adaptive procedures, RiskShrink is essentially optimal. Relying only on the data, it comes within a factor log 2 n of the performance of piecewise polynomial and variableknot spline methods equipped with an oracle. In contrast, it is unknown how or if piecewise polynomial methods could be made to function this well when denied access to an oracle and forced to rely on data alone.',\n",
       "  'authors': ['David L Donoho ', ' Iain M Johnstone'],\n",
       "  'date': '1994',\n",
       "  'identifier': '2158940042',\n",
       "  'references': ['2062024414',\n",
       "   '2098914003',\n",
       "   '1489213177',\n",
       "   '1969423031',\n",
       "   '139959648',\n",
       "   '654435104',\n",
       "   '2797502950',\n",
       "   '2024599390',\n",
       "   '2140832824',\n",
       "   '2036144352'],\n",
       "  'title': 'Ideal spatial adaptation by wavelet shrinkage'},\n",
       " {'abstract': \"We describe a class of causal, discrete latent variable models called Multiple Multiplicative Factor models (MMFs). A data vector is represented in the latent space as a vector of factors that have discrete, non-negative expression levels. Each factor proposes a distribution over the data vector. The distinguishing feature of MMFs is that they combine the factors' proposed distributions multiplicatively, taking into account factor expression levels. The product formulation of MMFs allow factors to specialize to a subset of the items, while the causal generative semantics mean MMFs can readily accommodate missing data. This makes MMFs distinct from both directed models with mixture semantics and undirected product models. In this paper we present empirical results from the collaborative filtering domain showing that a binary/multinomial MMF model matches the performance of the best existing models while learning an interesting latent space description of the users.\",\n",
       "  'authors': ['Benjamin Marlin ', ' Richard S. Zemel'],\n",
       "  'date': '2004',\n",
       "  'identifier': '2109720450',\n",
       "  'references': ['2116064496',\n",
       "   '2110325612',\n",
       "   '2155106456',\n",
       "   '2567948266',\n",
       "   '2070786785',\n",
       "   '2151052953',\n",
       "   '2118079529',\n",
       "   '1669104078',\n",
       "   '1806731464',\n",
       "   '2148124601'],\n",
       "  'title': 'The multiple multiplicative factor model for collaborative filtering'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Yann Lecun 1',\n",
       "   ' Leon Bottou 2',\n",
       "   ' 3',\n",
       "   ' Yoshua Bengio 3',\n",
       "   ' 4',\n",
       "   ' 5',\n",
       "   ' Patrick Haffner 3',\n",
       "   ' 6'],\n",
       "  'date': '2001',\n",
       "  'identifier': '2310919327',\n",
       "  'references': ['2147880316',\n",
       "   '2156163116',\n",
       "   '2963399829',\n",
       "   '2964311892',\n",
       "   '2157364932',\n",
       "   '1510526001',\n",
       "   '1944615693',\n",
       "   '2158778629'],\n",
       "  'title': 'Gradient-based learning applied to document recognition'},\n",
       " {'abstract': \"An Overview: From Fourier Analysis to Wavelet Analysis. The Integral Wavelet Transform and Time-Frequency Analysis. Inversion Formulas and Duals. Classification of Wavelets. Multiresolution Analysis, Splines, and Wavelets. Wavelet Decompositions and Reconstructions. Fourier Analysis: Fourier and Inverse Fourier Transforms. Continuous-Time Convolution and the Delta Function. Fourier Transform of Square-Integrable Functions. Fourier Series. Basic Convergence Theory and Poisson's Summation Formula. Wavelet Transforms and Time-Frequency Analysis: The Gabor Transform. Short-Time Fourier Transforms and the Uncertainty Principle. The Integral Wavelet Transform. Dyadic Wavelets and Inversions. Frames. Wavelet Series. Cardinal Spline Analysis: Cardinal Spline Spaces. B-Splines and Their Basic Properties. The Two-Scale Relation and an Interpolatory Graphical Display Algorithm. B-Net Representations and Computation of Cardinal Splines. Construction of Spline Approximation Formulas. Construction of Spline Interpolation Formulas. Scaling Functions and Wavelets: Multiresolution Analysis. Scaling Functions with Finite Two-Scale Relations. Direct-Sum Decompositions of L2(R). Wavelets and Their Duals. Linear-Phase Filtering. Compactly Supported Wavelets. Cardinal Spline-Wavelets: Interpolaratory Spline-Wavelets. Compactly Supported Spline-Wavelets. Computation of Cardinal Spline-Wavelets. Euler-Frobenius Polynomials. Error Analysis in Spline-Wavelet Decomposition. Total Positivity, Complete Oscillation, Zero-Crossings. Orthogonal Wavelets and Wavelet Packets: Examples of Orthogonal Wavelets. Identification of Orthogonal Two-Scale Symbols. Construction of Compactly Supported Orthogonal Wavelets. Orthogonal Wavelet Packets. Orthogonal Decomposition of Wavelet Series. Notes. References. Subject Index. Appendix.\",\n",
       "  'authors': ['Charles K. Chui'],\n",
       "  'date': '1992',\n",
       "  'identifier': '1489213177',\n",
       "  'references': ['2158940042',\n",
       "   '2015370045',\n",
       "   '2116308679',\n",
       "   '2035687084',\n",
       "   '2294985758',\n",
       "   '2020919250',\n",
       "   '2136104104',\n",
       "   '2139055047',\n",
       "   '2152254169',\n",
       "   '2119234283'],\n",
       "  'title': 'An introduction to wavelets'},\n",
       " {'abstract': 'We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service.',\n",
       "  'authors': ['Yann LeCun 1',\n",
       "   ' Bernhard E. Boser 2',\n",
       "   ' John S. Denker 2',\n",
       "   ' 3',\n",
       "   ' Donnie Henderson 1',\n",
       "   ' R. E. Howard 2',\n",
       "   ' Wayne E. Hubbard 2',\n",
       "   ' Lawrence D. Jackel 1'],\n",
       "  'date': '1989',\n",
       "  'identifier': '2154579312',\n",
       "  'references': ['2154642048',\n",
       "   '2147800946',\n",
       "   '2114766824',\n",
       "   '169539560',\n",
       "   '2157475639',\n",
       "   '1965770722',\n",
       "   '56903235',\n",
       "   '2091987367',\n",
       "   '2058841211',\n",
       "   '2285102547'],\n",
       "  'title': 'Handwritten Digit Recognition with a Back-Propagation Network'},\n",
       " {'abstract': 'In this paper we present a generative latent variable model for rating-based collaborative filtering called the User Rating Profile model (URP). The generative process which underlies URP is designed to produce complete user rating profiles, an assignment of one rating to each item for each user. Our model represents each user as a mixture of user attitudes, and the mixing proportions are distributed according to a Dirichlet random variable. The rating for each item is generated by selecting a user attitude for the item, and then selecting a rating according to the preference pattern associated with that attitude. URP is related to several models including a multinomial mixture model, the aspect model [7], and LDA [1], but has clear advantages over each.',\n",
       "  'authors': ['Benjamin M. Marlin'],\n",
       "  'date': '2003',\n",
       "  'identifier': '2151052953',\n",
       "  'references': ['1880262756',\n",
       "   '2155106456',\n",
       "   '2117354486',\n",
       "   '190008395',\n",
       "   '1510348757',\n",
       "   '1488394244',\n",
       "   '1806731464',\n",
       "   '2063397738',\n",
       "   '2963440404'],\n",
       "  'title': 'Modeling User Rating Profiles For Collaborative Filtering'},\n",
       " {'abstract': 'We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.',\n",
       "  'authors': ['Tim Salimans 1',\n",
       "   ' Ian Goodfellow 2',\n",
       "   ' Wojciech Zaremba 3',\n",
       "   ' Vicki Cheung ',\n",
       "   ' Alec Radford 1',\n",
       "   ' Xi Chen 4'],\n",
       "  'date': '2016',\n",
       "  'identifier': '2963373786',\n",
       "  'references': ['1836465849',\n",
       "   '2183341477',\n",
       "   '2963684088',\n",
       "   '2964153729',\n",
       "   '2271840356',\n",
       "   '648143168',\n",
       "   '830076066',\n",
       "   '2963685250',\n",
       "   '2949416428',\n",
       "   '1487641199'],\n",
       "  'title': 'Improved techniques for training GANs'},\n",
       " {'abstract': 'Embedded zerotree wavelet (EZW) coding, introduced by Shapiro (see IEEE Trans. Signal Processing, vol.41, no.12, p.3445, 1993), is a very effective and computationally simple technique for image compression. We offer an alternative explanation of the principles of its operation, so that the reasons for its excellent performance can be better understood. These principles are partial ordering by magnitude with a set partitioning sorting algorithm, ordered bit plane transmission, and exploitation of self-similarity across different scales of an image wavelet transform. Moreover, we present a new and different implementation based on set partitioning in hierarchical trees (SPIHT), which provides even better performance than our previously reported extension of EZW that surpassed the performance of the original EZW. The image coding results, calculated from actual file sizes and images reconstructed by the decoding algorithm, are either comparable to or surpass previous results obtained through much more sophisticated and computationally complex methods. In addition, the new coding and decoding procedures are extremely fast, and they can be made even faster, with only small loss in performance, by omitting entropy coding of the bit stream by the arithmetic code.',\n",
       "  'authors': ['A. Said 1', ' W.A. Pearlman 2'],\n",
       "  'date': '1996',\n",
       "  'identifier': '2142276208',\n",
       "  'references': ['2053691921',\n",
       "   '2148593155',\n",
       "   '2103504761',\n",
       "   '2129652681',\n",
       "   '1931641413',\n",
       "   '2166087152',\n",
       "   '2058719583',\n",
       "   '1592970628',\n",
       "   '2117465325',\n",
       "   '2147399030'],\n",
       "  'title': 'A new, fast, and efficient image codec based on set partitioning in hierarchical trees'},\n",
       " {'abstract': \"Studies of the contextual and linguistic factors that constrain discourse phenomena such as reference are coming to depend increasingly on annotated language corpora. In preparing the corpora, it is important to evaluate the reliability of the annotation, but methods for doing so have not been readily available. In this report, I present a method for computing reliability of coreference annotation. First I review a method for applying the information retrieval metrics of recall and precision to coreference annotation proposed by Marc Vilain and his collaborators. I show how this method makes it possible to construct contingency tables for computing Cohen's Kappa, a familiar reliability metric. By comparing recall and precision to reliability on the same data sets, I also show that recall and precision can be misleadingly high. Because Kappa factors out chance agreement among coders, it is a preferable measure for developing annotated corpora where no pre-existing target annotation exists.\",\n",
       "  'authors': ['Rebecca J. Passonneau'],\n",
       "  'date': '1997',\n",
       "  'identifier': '2128530885',\n",
       "  'references': ['3021916629',\n",
       "   '1483126227',\n",
       "   '1965693266',\n",
       "   '1669912781',\n",
       "   '1967545963',\n",
       "   '2915157635'],\n",
       "  'title': 'Applying Reliability Metrics to Co-Reference Annotation'},\n",
       " {'abstract': 'The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a \"black art\" requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm\\'s generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.',\n",
       "  'authors': ['Jasper Snoek 1', ' Hugo Larochelle 2', ' Ryan P Adams 3'],\n",
       "  'date': '2012',\n",
       "  'identifier': '2131241448',\n",
       "  'references': ['3118608800',\n",
       "   '1746819321',\n",
       "   '2141125852',\n",
       "   '2097998348',\n",
       "   '2106411961',\n",
       "   '2951665052',\n",
       "   '60686164',\n",
       "   '2165599843',\n",
       "   '2099201756',\n",
       "   '1973333099'],\n",
       "  'title': 'Practical Bayesian Optimization of Machine Learning Algorithms'},\n",
       " {'abstract': 'Collaborative filters help people make choices based on the opinions of other people. GroupLens is a system for collaborative filtering of netnews, to help people find articles they will like in the huge stream of available articles. News reader clients display predicted scores and make it easy for users to rate articles after they read them. Rating servers, called Better Bit Bureaus, gather and disseminate the ratings. The rating servers predict scores based on the heuristic that people who agreed in the past will probably agree again. Users can protect their privacy by entering ratings under pseudonyms, without reducing the effectiveness of the score prediction. The entire architecture is open: alternative software for news clients and Better Bit Bureaus can be developed independently and can interoperate with the components we have developed.',\n",
       "  'authors': ['Paul Resnick 1',\n",
       "   ' Neophytos Iacovou 2',\n",
       "   ' Mitesh Suchak 1',\n",
       "   ' Peter Bergstrom 2',\n",
       "   ' John Riedl 2'],\n",
       "  'date': '1994',\n",
       "  'identifier': '2155106456',\n",
       "  'references': ['2147152072',\n",
       "   '1966553486',\n",
       "   '1978394996',\n",
       "   '2141824507',\n",
       "   '2000672666',\n",
       "   '2106365165',\n",
       "   '2058428373',\n",
       "   '2037717074',\n",
       "   '2056029990',\n",
       "   '2142094977'],\n",
       "  'title': 'GroupLens: an open architecture for collaborative filtering of netnews'},\n",
       " {'abstract': 'We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.',\n",
       "  'authors': ['Matthew E. Peters 1',\n",
       "   ' Mark Neumann 1',\n",
       "   ' Mohit Iyyer 2',\n",
       "   ' Matt Gardner 3',\n",
       "   ' Christopher Clark 1',\n",
       "   ' Kenton Lee 4',\n",
       "   ' Luke Zettlemoyer 5'],\n",
       "  'date': '2018',\n",
       "  'identifier': '2962739339',\n",
       "  'references': ['2964121744',\n",
       "   '2153579005',\n",
       "   '2095705004',\n",
       "   '2250539671',\n",
       "   '2158899491',\n",
       "   '2064675550',\n",
       "   '2147880316',\n",
       "   '2251939518',\n",
       "   '2493916176',\n",
       "   '2963748441'],\n",
       "  'title': 'Deep contextualized word representations'},\n",
       " {'abstract': 'The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL. Although simple, it performs slightly better than a much more elaborate system with which it has been compared. It effectively works by treating complex suffixes as compounds made up of simple suffixes, and removing the simple suffixes in a number of steps. In each step the removal of the suffix is made to depend upon the form of the remaining stem, which usually involves a measure of its syllable length.',\n",
       "  'authors': ['M. F. Porter'],\n",
       "  'date': '1997',\n",
       "  'identifier': '2098162425',\n",
       "  'references': ['26591655', '1495821370', '1988344511', '2005095359'],\n",
       "  'title': 'An algorithm for suffix stripping'},\n",
       " {'abstract': 'Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.',\n",
       "  'authors': ['Dan Cireşan ', ' Ueli Meier ', ' Juergen Schmidhuber'],\n",
       "  'date': '2012',\n",
       "  'identifier': '2141125852',\n",
       "  'references': ['3118608800',\n",
       "   '2310919327',\n",
       "   '2110798204',\n",
       "   '2154642048',\n",
       "   '2134557905',\n",
       "   '2156163116',\n",
       "   '2138857742',\n",
       "   '2148461049',\n",
       "   '2144982973',\n",
       "   '1991848143'],\n",
       "  'title': 'Multi-column deep neural networks for image classification'},\n",
       " {'abstract': 'Neural networks are a powerful technology forclassification of visual inputs arising from documents.However, there is a confusing plethora of different neuralnetwork methods that are used in the literature and inindustry. This paper describes a set of concrete bestpractices that document analysis researchers can use toget good results with neural networks. The mostimportant practice is getting a training set as large aspossible: we expand the training set by adding a newform of distorted data. The next most important practiceis that convolutional neural networks are better suited forvisual document tasks than fully connected networks. Wepropose that a simple \"do-it-yourself\" implementation ofconvolution with a flexible architecture is suitable formany visual document problems. This simpleconvolutional neural network does not require complexmethods, such as momentum, weight decay, structure-dependentlearning rates, averaging layers, tangent prop,or even finely-tuning the architecture. The end result is avery simple yet general architecture which can yieldstate-of-the-art performance for document analysis. Weillustrate our claims on the MNIST set of English digitimages.',\n",
       "  'authors': ['P.Y. Simard ', ' D. Steinkraus ', ' J.C. Platt'],\n",
       "  'date': '2003',\n",
       "  'identifier': '2156163116',\n",
       "  'references': ['2310919327',\n",
       "   '1554663460',\n",
       "   '2159737176',\n",
       "   '2027197837',\n",
       "   '2068017609',\n",
       "   '2147345686',\n",
       "   '51975515',\n",
       "   '2166469100'],\n",
       "  'title': 'Best practices for convolutional neural networks applied to visual document analysis'},\n",
       " {'abstract': 'Realism and Instrumentalism: Classical Statistics and VC Theory (1960-1980).- Falsifiability and Parsimony: VC Dimension and the Number of Entities (1980-2000).- Noninductive Methods of Inference: Direct Inference Instead of Generalization (2000-...).- The Big Picture.',\n",
       "  'authors': ['Vladimir Naumovich Vapnik'],\n",
       "  'date': '2006',\n",
       "  'identifier': '1530699444',\n",
       "  'references': ['2137775453',\n",
       "   '2119821739',\n",
       "   '2139212933',\n",
       "   '1988790447',\n",
       "   '2119479037',\n",
       "   '1964357740',\n",
       "   '2132549764',\n",
       "   '2087347434',\n",
       "   '607505555',\n",
       "   '1975846642'],\n",
       "  'title': 'Estimation of Dependences Based on Empirical Data'},\n",
       " {'abstract': 'A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.',\n",
       "  'authors': ['Bernhard E. Boser 1',\n",
       "   ' Isabelle M. Guyon 2',\n",
       "   ' Vladimir N. Vapnik 2'],\n",
       "  'date': '1992',\n",
       "  'identifier': '2087347434',\n",
       "  'references': ['3017143921',\n",
       "   '2171277043',\n",
       "   '1530699444',\n",
       "   '2165758113',\n",
       "   '2154579312',\n",
       "   '2076118331',\n",
       "   '3023695801',\n",
       "   '2086472796',\n",
       "   '1965770722',\n",
       "   '2111494971'],\n",
       "  'title': 'A training algorithm for optimal margin classifiers'},\n",
       " {'abstract': 'Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs-30,000 auditory nerve fibers or 10(6) optic nerve fibers-a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.',\n",
       "  'authors': ['J. B. Tenenbaum 1', ' V. de Silva 1', ' J. C. Langford 2'],\n",
       "  'date': '2000',\n",
       "  'identifier': '2001141328',\n",
       "  'references': ['2138451337',\n",
       "   '2099741732',\n",
       "   '3110653090',\n",
       "   '2587818897',\n",
       "   '2123977795',\n",
       "   '2107636931',\n",
       "   '2122538988',\n",
       "   '2047870719',\n",
       "   '2070320140',\n",
       "   '2032647857'],\n",
       "  'title': 'A Global Geometric Framework for Nonlinear Dimensionality Reduction'},\n",
       " {'abstract': 'We present a distribution model for binary vectors, called the influence combination model and show how this model can be used as the basis for unsupervised learning algorithms for feature selection. The model can be represented by a particular type of Boltzmann machine with a bipartite graph structure that we call the combination machine. This machine is closely related to the Harmonium model defined by Smolensky. In the first part of the paper we analyze properties of this distribution representation scheme. We show that arbitrary distributions of binary vectors can be approximated by the combination model. We show how the weight vectors in the model can be interpreted as high order correlation patterns among the input bits, and how the combination machine can be used as a mechanism for detecting these patterns. We compare the combination model with the mixture model and with principle component analysis. In the second part of the paper we present two algorithms for learning the combination model from examples. The first learning algorithm is the standard gradient ascent heuristic for computing maximum likelihood estimates for the parameters of the model. Here we give a closed form for this gradient that is significantly easier to compute than the corresponding gradient for the general Boltzmann machine. The second learning algorithm is a greedy method that creates the hidden units and computes their weights one at a time. This method is a variant of projection pursuit density estimation. In the third part of the paper we give experimental results for these learning methods on synthetic data and on natural data of handwritten digit images.',\n",
       "  'authors': ['Yoav Freund ', ' David Haussler'],\n",
       "  'date': '1991',\n",
       "  'identifier': '2165225968',\n",
       "  'references': ['1652505363',\n",
       "   '2159080219',\n",
       "   '1997063559',\n",
       "   '2049633694',\n",
       "   '2293063825',\n",
       "   '1507849272',\n",
       "   '1964724001',\n",
       "   '2121407732',\n",
       "   '2725061391',\n",
       "   '2315016682'],\n",
       "  'title': 'Unsupervised learning of distributions on binary vectors using two layer networks'},\n",
       " {'abstract': 'Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrase-based statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to outperform the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use the ensemble of a few models with very large target vocabularies, we achieve the state-of-the-art translation performance (measured by BLEU) on the English!German translation and almost as high performance as state-of-the-art English!French translation system.',\n",
       "  'authors': ['Sébastien Jean ',\n",
       "   ' Kyunghyun Cho ',\n",
       "   ' Roland Memisevic ',\n",
       "   ' Yoshua Bengio'],\n",
       "  'date': '2015',\n",
       "  'identifier': '2100664567',\n",
       "  'references': ['2964308564',\n",
       "   '1614298861',\n",
       "   '2130942839',\n",
       "   '2157331557',\n",
       "   '2101105183',\n",
       "   '1753482797',\n",
       "   '2964199361',\n",
       "   '2153653739',\n",
       "   '1606347560',\n",
       "   '2118434577'],\n",
       "  'title': 'On Using Very Large Target Vocabulary for Neural Machine Translation'},\n",
       " {'abstract': 'This chapter contains sections titled: Relaxation Searches, Easy and Hard Learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, An Example of the Effects of Damage, Conclusion, Acknowledgments, Appendix: Derivation of the Learning Algorithm, References',\n",
       "  'authors': ['G. E. Hinton ', ' T. J. Sejnowski'],\n",
       "  'date': '1986',\n",
       "  'identifier': '1547224907',\n",
       "  'references': ['2310919327',\n",
       "   '2076063813',\n",
       "   '2072128103',\n",
       "   '2116064496',\n",
       "   '2137813581',\n",
       "   '2133671888',\n",
       "   '1562911371',\n",
       "   '1516111018',\n",
       "   '2321533354'],\n",
       "  'title': 'Learning and relearning in Boltzmann machines'},\n",
       " {'abstract': 'The state of the art in data compression is arithmetic coding, not the better-known Huffman method. Arithmetic coding gives greater compression, is faster for adaptive models, and clearly separates the model from the channel encoding.',\n",
       "  'authors': ['Ian H. Witten ', ' Radford M. Neal ', ' John G. Cleary'],\n",
       "  'date': '1987',\n",
       "  'identifier': '2129652681',\n",
       "  'references': ['2122962290',\n",
       "   '1990653637',\n",
       "   '1995875735',\n",
       "   '2161628678',\n",
       "   '1975965284',\n",
       "   '2119047110',\n",
       "   '2107927941',\n",
       "   '2165564574',\n",
       "   '2079729471',\n",
       "   '2038649859'],\n",
       "  'title': 'Arithmetic coding for data compression'},\n",
       " {'abstract': 'We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be repurposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.',\n",
       "  'authors': ['Jeff Donahue ',\n",
       "   ' Yangqing Jia ',\n",
       "   ' Oriol Vinyals ',\n",
       "   ' Judy Hoffman ',\n",
       "   ' Ning Zhang ',\n",
       "   ' Eric Tzeng ',\n",
       "   ' Trevor Darrell'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2155541015',\n",
       "  'references': ['2618530766',\n",
       "   '2161969291',\n",
       "   '2108598243',\n",
       "   '2168356304',\n",
       "   '2100495367',\n",
       "   '2310919327',\n",
       "   '1677409904',\n",
       "   '1904365287',\n",
       "   '2187089797',\n",
       "   '2546302380'],\n",
       "  'title': 'DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition'},\n",
       " {'abstract': 'Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.',\n",
       "  'authors': ['Oriol Vinyals ',\n",
       "   ' Alexander Toshev ',\n",
       "   ' Samy Bengio ',\n",
       "   ' Dumitru Erhan'],\n",
       "  'date': '2015',\n",
       "  'identifier': '1895577753',\n",
       "  'references': ['2097117768',\n",
       "   '1836465849',\n",
       "   '2117539524',\n",
       "   '2964308564',\n",
       "   '1614298861',\n",
       "   '2130942839',\n",
       "   '2157331557',\n",
       "   '2963542991',\n",
       "   '2155541015',\n",
       "   '2064675550'],\n",
       "  'title': 'Show and tell: A neural image caption generator'},\n",
       " {'abstract': 'We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.',\n",
       "  'authors': ['N. Dalal ', ' B. Triggs'],\n",
       "  'date': '2005',\n",
       "  'identifier': '2161969291',\n",
       "  'references': ['2151103935',\n",
       "   '2177274842',\n",
       "   '3021469268',\n",
       "   '2145072179',\n",
       "   '2172188317',\n",
       "   '2152473410',\n",
       "   '1576520375',\n",
       "   '1608462934',\n",
       "   '1992825118',\n",
       "   '2295106276'],\n",
       "  'title': 'Histograms of oriented gradients for human detection'},\n",
       " {'abstract': 'We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.',\n",
       "  'authors': ['David M. Blei 1', ' Andrew Y. Ng 2', ' Michael I. Jordan 1'],\n",
       "  'date': '2003',\n",
       "  'identifier': '1880262756',\n",
       "  'references': ['2045656233',\n",
       "   '2147152072',\n",
       "   '2107743791',\n",
       "   '2097089247',\n",
       "   '1956559956',\n",
       "   '1516111018',\n",
       "   '1508165687',\n",
       "   '1746680969',\n",
       "   '2020842694',\n",
       "   '2063392856'],\n",
       "  'title': 'Latent dirichlet allocation'},\n",
       " {'abstract': 'Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.',\n",
       "  'authors': ['Yangqing Jia 1',\n",
       "   ' Evan Shelhamer 2',\n",
       "   ' Jeff Donahue 2',\n",
       "   ' Sergey Karayev 2',\n",
       "   ' Jonathan Long 2',\n",
       "   ' Ross Girshick 2',\n",
       "   ' Sergio Guadarrama 2',\n",
       "   ' Trevor Darrell 2'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2155893237',\n",
       "  'references': ['2618530766',\n",
       "   '2102605133',\n",
       "   '2963542991',\n",
       "   '2088049833',\n",
       "   '2155541015',\n",
       "   '753012316',\n",
       "   '1825604117',\n",
       "   '2147414309',\n",
       "   '1872489089',\n",
       "   '2962883796'],\n",
       "  'title': 'Caffe: Convolutional Architecture for Fast Feature Embedding'},\n",
       " {'abstract': 'The fundamental principles, basic mechanisms, and formal analyses involved in the development of parallel distributed processing (PDP) systems are presented in individual chapters contributed by leading experts. Topics examined include distributed representations, PDP models and general issues in cognitive science, feature discovery by competitive learning, the foundations of harmony theory, learning and relearning in Boltzmann machines, and learning internal representations by error propagation. Consideration is given to linear algebra in PDP, the logic of additive functions, resource requirements of standard and programmable nets, and the P3 parallel-network simulating system.',\n",
       "  'authors': ['David E. Rumelhart 1', ' James L. McClelland 2'],\n",
       "  'date': '1986',\n",
       "  'identifier': '1652505363',\n",
       "  'references': ['1614298861',\n",
       "   '2145339207',\n",
       "   '2076063813',\n",
       "   '2072128103',\n",
       "   '2116064496',\n",
       "   '2145094598',\n",
       "   '2025768430',\n",
       "   '2107941094',\n",
       "   '2154642048',\n",
       "   '2137983211'],\n",
       "  'title': 'Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations'},\n",
       " {'abstract': 'Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.',\n",
       "  'authors': ['Jeremy Howard ', ' Sebastian Ruder'],\n",
       "  'date': '2018',\n",
       "  'identifier': '2963026768',\n",
       "  'references': ['2194775991',\n",
       "   '1836465849',\n",
       "   '2153579005',\n",
       "   '1903029394',\n",
       "   '2963446712',\n",
       "   '2962739339',\n",
       "   '2155541015',\n",
       "   '3112605745',\n",
       "   '2062118960',\n",
       "   '2963012544'],\n",
       "  'title': 'Universal Language Model Fine-tuning for Text Classification'},\n",
       " {'abstract': 'We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice.',\n",
       "  'authors': ['Ryan Kiros 1',\n",
       "   ' Yukun Zhu 1',\n",
       "   ' Ruslan Salakhutdinov 2',\n",
       "   ' Richard S. Zemel 2',\n",
       "   ' Antonio Torralba 3',\n",
       "   ' Raquel Urtasun 1',\n",
       "   ' Sanja Fidler 1'],\n",
       "  'date': '2015',\n",
       "  'identifier': '1486649854',\n",
       "  'references': ['2962835968',\n",
       "   '2964121744',\n",
       "   '2964308564',\n",
       "   '1614298861',\n",
       "   '2130942839',\n",
       "   '2157331557',\n",
       "   '1861492603',\n",
       "   '1832693441',\n",
       "   '2064675550',\n",
       "   '2187089797'],\n",
       "  'title': 'Skip-thought vectors'},\n",
       " {'abstract': 'Recently SVMs using spatial pyramid matching (SPM) kernel have been highly successful in image classification. Despite its popularity, these nonlinear SVMs have a complexity O(n2 ~ n3) in training and O(n) in testing, where n is the training size, implying that it is nontrivial to scaleup the algorithms to handle more than thousands of training images. In this paper we develop an extension of the SPM method, by generalizing vector quantization to sparse coding followed by multi-scale spatial max pooling, and propose a linear SPM kernel based on SIFT sparse codes. This new approach remarkably reduces the complexity of SVMs to O(n) in training and a constant in testing. In a number of image categorization experiments, we find that, in terms of classification accuracy, the suggested linear SPM based on sparse coding of SIFT descriptors always significantly outperforms the linear SPM kernel on histograms, and is even better than the nonlinear SPM kernels, leading to state-of-the-art performance on several benchmarks by using a single type of descriptors.',\n",
       "  'authors': ['Jianchao Yang 1',\n",
       "   ' Kai Yu 2',\n",
       "   ' Yihong Gong 2',\n",
       "   ' Thomas Huang 1'],\n",
       "  'date': '2009',\n",
       "  'identifier': '2097018403',\n",
       "  'references': ['2153635508',\n",
       "   '2162915993',\n",
       "   '1576445103',\n",
       "   '2153663612',\n",
       "   '2107034620',\n",
       "   '1566135517',\n",
       "   '2166049352',\n",
       "   '2113606819',\n",
       "   '2161516371',\n",
       "   '2166742463'],\n",
       "  'title': 'Linear spatial pyramid matching using sparse coding for image classification'},\n",
       " {'abstract': 'We propose a method for inferring human attributes (such as gender, hair style, clothes style, expression, action) from images of people under large variation of viewpoint, pose, appearance, articulation and occlusion. Convolutional Neural Nets (CNN) have been shown to perform very well on large scale object recognition problems. In the context of attribute classification, however, the signal is often subtle and it may cover only a small part of the image, while the image is dominated by the effects of pose and viewpoint. Discounting for pose variation would require training on very large labeled datasets which are not presently available. Part-based models, such as poselets [4] and DPM [12] have been shown to perform well for this problem but they are limited by shallow low-level features. We propose a new method which combines part-based models and deep learning by training pose-normalized CNNs. We show substantial improvement vs. state-of-the-art methods on challenging attribute classification tasks in unconstrained settings. Experiments confirm that our method outperforms both the best part-based methods on this problem and conventional CNNs trained on the full bounding box of the person.',\n",
       "  'authors': ['Ning Zhang 1',\n",
       "   ' Manohar Paluri 2',\n",
       "   \" Marc'Aurelio Ranzato 2\",\n",
       "   ' Trevor Darrell 1',\n",
       "   ' Lubomir Bourdev 2'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2147414309',\n",
       "  'references': ['2618530766',\n",
       "   '2168356304',\n",
       "   '2310919327',\n",
       "   '2155541015',\n",
       "   '2162915993',\n",
       "   '2546302380',\n",
       "   '1498436455',\n",
       "   '2536626143',\n",
       "   '2134270519',\n",
       "   '2098411764'],\n",
       "  'title': 'PANDA: Pose Aligned Networks for Deep Attribute Modeling'},\n",
       " {'abstract': \"The recently developed technique of arithmetic coding, in conjunction with a Markov model of the source, is a powerful method of data compression in situations where a linear treatment is inappropriate. Adaptive coding allows the model to be constructed dynamically by both encoder and decoder during the course of the transmission, and has been shown to incur a smaller coding overhead than explicit transmission of the model's statistics. But there is a basic conflict between the desire to use high-order Markov models and the need to have them formed quickly as the initial part of the message is sent. This paper describes how the conflict can be resolved with partial string matching, and reports experimental results which show that mixed-case English text can be coded in as little as 2.2 bits/ character with no prior knowledge of the source.\",\n",
       "  'authors': ['J. Cleary ', ' I. Witten'],\n",
       "  'date': '1984',\n",
       "  'identifier': '2161628678',\n",
       "  'references': ['2122962290',\n",
       "   '2119047110',\n",
       "   '2165564574',\n",
       "   '2128066268',\n",
       "   '1519253855',\n",
       "   '2034323860',\n",
       "   '2006384477',\n",
       "   '2120465215',\n",
       "   '1974717499',\n",
       "   '1941015899'],\n",
       "  'title': 'Data Compression Using Adaptive Coding and Partial String Matching'},\n",
       " {'abstract': 'Abstract Connectionist learning procedures are presented for “sigmoid” and “noisy-OR” varieties of probabilistic belief networks. These networks have previously been seen primarily as a means of representing knowledge derived from experts. Here it is shown that the “Gibbs sampling” simulation procedure for such networks can support maximum-likelihood learning from empirical data through local gradient ascent. This learning procedure resembles that used for “Boltzmann machines”, and like it, allows the use of “hidden” variables to model correlations between visible variables. Due to the directed nature of the connections in a belief network, however, the “negative phase” of Boltzmann machine learning is unnecessary. Experimental results show that, as a result, learning in a sigmoid belief network can be faster than in a Boltzmann machine. These networks have other advantages over Boltzmann machines in pattern classification and decision making applications, are naturally applicable to unsupervised learning problems, and provide a link between work on connectionist learning and work on the representation of expert knowledge.',\n",
       "  'authors': ['Radford M. Neal'],\n",
       "  'date': '1992',\n",
       "  'identifier': '2083380015',\n",
       "  'references': ['1652505363',\n",
       "   '2159080219',\n",
       "   '1498436455',\n",
       "   '2049633694',\n",
       "   '2083875149',\n",
       "   '1593793857',\n",
       "   '1507849272',\n",
       "   '2166698530',\n",
       "   '1992880122',\n",
       "   '1547224907'],\n",
       "  'title': 'Connectionist learning of belief networks'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Dimitri P. Bertsekas'],\n",
       "  'date': '1982',\n",
       "  'identifier': '1669104078',\n",
       "  'references': ['2164278908',\n",
       "   '2145962650',\n",
       "   '2202343345',\n",
       "   '2000982976',\n",
       "   '1997201895',\n",
       "   '2167732364',\n",
       "   '1506281249',\n",
       "   '2913535645',\n",
       "   '1736339626'],\n",
       "  'title': 'Constrained Optimization and Lagrange Multiplier Methods'},\n",
       " {'abstract': 'We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13% for SVM and 7% for convolutional nets. On a segmentation/recognition task with highly cluttered images, SVM proved impractical, while convolutional nets yielded 16/7% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second.',\n",
       "  'authors': ['Y. LeCun 1', ' Fu Jie Huang 1', ' L. Bottou 2'],\n",
       "  'date': '2004',\n",
       "  'identifier': '2134557905',\n",
       "  'references': ['2164598857',\n",
       "   '2310919327',\n",
       "   '2217896605',\n",
       "   '2124087378',\n",
       "   '2124351082',\n",
       "   '2123977795',\n",
       "   '2155511848',\n",
       "   '2160225842',\n",
       "   '2295106276',\n",
       "   '2141376824'],\n",
       "  'title': 'Learning methods for generic object recognition with invariance to pose and lighting'},\n",
       " {'abstract': 'Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations, longer training times, and unexpected model degradation. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large.',\n",
       "  'authors': ['Zhenzhong Lan 1',\n",
       "   ' Mingda Chen 2',\n",
       "   ' Sebastian Goodman 1',\n",
       "   ' Kevin Gimpel 3',\n",
       "   ' Piyush Sharma 1',\n",
       "   ' Radu Soricut 1'],\n",
       "  'date': '2020',\n",
       "  'identifier': '2996428491',\n",
       "  'references': ['2963403868',\n",
       "   '2963341956',\n",
       "   '2153579005',\n",
       "   '2250539671',\n",
       "   '2131744502',\n",
       "   '2251939518',\n",
       "   '2963748441',\n",
       "   '2964350391',\n",
       "   '2965373594',\n",
       "   '2970597249'],\n",
       "  'title': 'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations'},\n",
       " {'abstract': 'Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering.',\n",
       "  'authors': ['Shai Shalev-Shwartz 1', ' Shai Ben-David 2'],\n",
       "  'date': '2014',\n",
       "  'identifier': '607505555',\n",
       "  'references': ['2296319761',\n",
       "   '2156909104',\n",
       "   '2911964244',\n",
       "   '1663973292',\n",
       "   '2296616510',\n",
       "   '2136922672',\n",
       "   '2148603752',\n",
       "   '2129131372',\n",
       "   '1480376833',\n",
       "   '2147880316'],\n",
       "  'title': 'Understanding Machine Learning: From Theory to Algorithms'},\n",
       " {'abstract': 'Abstract: Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.',\n",
       "  'authors': ['Dzmitry Bahdanau 1', ' Kyunghyun Cho 2', ' Yoshua Bengio 2'],\n",
       "  'date': '2015',\n",
       "  'identifier': '2964308564',\n",
       "  'references': ['2157331557',\n",
       "   '2064675550',\n",
       "   '2294059674',\n",
       "   '2132339004',\n",
       "   '6908809',\n",
       "   '1753482797',\n",
       "   '1810943226',\n",
       "   '2964199361',\n",
       "   '2153653739',\n",
       "   '1815076433'],\n",
       "  'title': 'Neural Machine Translation by Jointly Learning to Align and Translate'},\n",
       " {'abstract': 'We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.',\n",
       "  'authors': ['Tim Salimans ', ' Diederik P. Kingma'],\n",
       "  'date': '2016',\n",
       "  'identifier': '2963685250',\n",
       "  'references': ['2194775991',\n",
       "   '1836465849',\n",
       "   '2145339207',\n",
       "   '1959608418',\n",
       "   '3118608800',\n",
       "   '2064675550',\n",
       "   '2963911037',\n",
       "   '1533861849',\n",
       "   '2294059674',\n",
       "   '104184427'],\n",
       "  'title': 'Weight normalization: a simple reparameterization to accelerate training of deep neural networks'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Carsten Peterson ', ' James R. Anderson'],\n",
       "  'date': '1987',\n",
       "  'identifier': '2725061391',\n",
       "  'references': ['2154642048', '2293063825', '1507849272'],\n",
       "  'title': 'A mean field theory learning algorithm for neural networks'},\n",
       " {'abstract': 'The limitation of using low electron doses in non-destructive cryo-electron tomography of biological specimens can be partially offset via averaging of aligned and structurally homogeneous subsets present in tomograms. This type of sub-volume averaging is especially challenging when multiple species are present. Here, we tackle the problem of conformational separation and alignment with a “collaborative” approach designed to reduce the effect of the “curse of dimensionality” encountered in standard pair-wise comparisons. Our new approach is based on using the nuclear norm as a collaborative similarity measure for alignment of sub-volumes, and by exploiting the presence of symmetry early in the processing. We provide a strict validation of this method by analyzing mixtures of intact simian immunodeficiency viruses SIV mac239 and SIV CP-MAC. Electron microscopic images of these two virus preparations are indistinguishable except for subtle differences in conformation of the envelope glycoproteins displayed on the surface of each virus particle. By using the nuclear norm-based, collaborative alignment method presented here, we demonstrate that the genetic identity of each virus particle present in the mixture can be assigned based solely on the structural information derived from single envelope glycoproteins displayed on the virus surface.',\n",
       "  'authors': ['Oleg Kuybeda 1',\n",
       "   ' Gabriel A. Frank 1',\n",
       "   ' Alberto Bartesaghi 1',\n",
       "   ' Mario Borgnia 1',\n",
       "   ' Sriram Subramaniam 1',\n",
       "   ' Guillermo Sapiro 2'],\n",
       "  'date': '2013',\n",
       "  'identifier': '1736339626',\n",
       "  'references': ['2148694408',\n",
       "   '2119667497',\n",
       "   '2100556411',\n",
       "   '2145962650',\n",
       "   '2124608575',\n",
       "   '2798766386',\n",
       "   '2103972604',\n",
       "   '2202343345',\n",
       "   '1976709621',\n",
       "   '2131628350'],\n",
       "  'title': 'A collaborative framework for 3D alignment and classification of heterogeneous subvolumes in cryo-electron tomography'},\n",
       " {'abstract': 'Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.',\n",
       "  'authors': ['Dumitru Erhan ',\n",
       "   ' Christian Szegedy ',\n",
       "   ' Alexander Toshev ',\n",
       "   ' Dragomir Anguelov'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2068730032',\n",
       "  'references': ['2618530766',\n",
       "   '2102605133',\n",
       "   '2168356304',\n",
       "   '2031489346',\n",
       "   '2088049833',\n",
       "   '2130306094',\n",
       "   '2129305389',\n",
       "   '2017691720',\n",
       "   '2128715914',\n",
       "   '2113201641'],\n",
       "  'title': 'Scalable Object Detection Using Deep Neural Networks'},\n",
       " {'abstract': 'We address the image denoising problem, where zero-mean white and homogeneous Gaussian additive noise is to be removed from a given image. The approach taken is based on sparse and redundant representations over trained dictionaries. Using the K-SVD algorithm, we obtain a dictionary that describes the image content effectively. Two training options are considered: using the corrupted image itself, or training on a corpus of high-quality image database. Since the K-SVD is limited in handling small image patches, we extend its deployment to arbitrary image sizes by defining a global image prior that forces sparsity over patches in every location in the image. We show how such Bayesian treatment leads to a simple and effective denoising algorithm. This leads to a state-of-the-art denoising performance, equivalent and sometimes surpassing recently published leading alternative denoising methods',\n",
       "  'authors': ['M. Elad ', ' M. Aharon'],\n",
       "  'date': '2006',\n",
       "  'identifier': '2153663612',\n",
       "  'references': ['2160547390',\n",
       "   '2078204800',\n",
       "   '2146842127',\n",
       "   '2158940042',\n",
       "   '2151693816',\n",
       "   '2097323375',\n",
       "   '2113945798',\n",
       "   '2132680427',\n",
       "   '2079724595',\n",
       "   '2069912449'],\n",
       "  'title': 'Image Denoising Via Sparse and Redundant Representations Over Learned Dictionaries'},\n",
       " {'abstract': 'A method is presented for encoding memoryless sources using trellis coded quantization (TCQ) with uniform thresholds. The trellis symbols are entropy-coded using arithmetic coding. The performance of the arithmetic coded uniform threshold TCQ, for encoding the family of generalized Gaussian densities, is compared with uniform threshold quantization (UTQ) and the Shannon lower bound (SLB). At high rates, the method performs within 0.5 dB of the rate-distortion bound for the family of generalized Gaussian densities. A simple modification of the uniform codebook is shown to result in improved performance at low bit rates. The arithmetic and trellis coding method is used for encoding image subbands. Coding results for monochrome images are presented and compared with other results in the literature. Working C code that implements arithmetic coded uniform threshold TCQ can be obtained using anonymous ftp.',\n",
       "  'authors': ['R.L. Joshi ', ' V.J. Crump ', ' T.R. Fischer'],\n",
       "  'date': '1995',\n",
       "  'identifier': '2117465325',\n",
       "  'references': ['2132984323',\n",
       "   '2053691921',\n",
       "   '2129652681',\n",
       "   '2165205968',\n",
       "   '2142384583',\n",
       "   '1992091448',\n",
       "   '2131578006',\n",
       "   '2133882791',\n",
       "   '2153639720',\n",
       "   '2171578374'],\n",
       "  'title': 'Image subband coding using arithmetic coded trellis coded quantization'},\n",
       " {'abstract': 'Abstract A new projection pursuit algorithm for exploring multivariate data is presented that has both statistical and computational advantages over previous methods. A number of practical issues concerning its application are addressed. A connection to multivariate density estimation is established, and its properties are investigated through simulation studies and application to real data. The goal of exploratory projection pursuit is to use the data to find low- (one-, two-, or three-) dimensional projections that provide the most revealing views of the full-dimensional data. With these views the human gift for pattern recognition can be applied to help discover effects that may not have been anticipated in advance. Since linear effects are directly captured by the covariance structure of the variable pairs (which are straightforward to estimate) the emphasis here is on the discovery of nonlinear effects such as clustering or other general nonlinear associations among the variables. Although arbitrary ...',\n",
       "  'authors': ['Jerome H. Friedman'],\n",
       "  'date': '1987',\n",
       "  'identifier': '1964724001',\n",
       "  'references': ['2954064014',\n",
       "   '2155199877',\n",
       "   '2800289289',\n",
       "   '2029469881',\n",
       "   '2082612735',\n",
       "   '1573763320',\n",
       "   '1968104963',\n",
       "   '2161831609',\n",
       "   '2052740976',\n",
       "   '1983993791'],\n",
       "  'title': 'Exploratory Projection Pursuit'},\n",
       " {'abstract': 'In this work we describe how to train a multi-layer generative model of natural images. We use a dataset of millions of tiny colour images, described in the next section. This has been attempted by several groups but without success. The models on which we focus are RBMs (Restricted Boltzmann Machines) and DBNs (Deep Belief Networks). These models learn interesting-looking filters, which we show are more useful to a classifier than the raw pixels. We train the classifier on a labeled subset that we have collected and call the CIFAR-10 dataset.',\n",
       "  'authors': ['Alex Krizhevsky'],\n",
       "  'date': '2009',\n",
       "  'identifier': '3118608800',\n",
       "  'references': ['2081580037', '2096192494', '2165225968'],\n",
       "  'title': 'Learning Multiple Layers of Features from Tiny Images'},\n",
       " {'abstract': 'This paper provides an algorithm for partitioning grayscale images into disjoint regions of coherent brightness and texture. Natural images contain both textured and untextured regions, so the cues of contour and texture differences are exploited simultaneously. Contours are treated in the intervening contour framework, while texture is analyzed using textons. Each of these cues has a domain of applicability, so to facilitate cue combination we introduce a gating operator based on the texturedness of the neighborhood at a pixel. Having obtained a local measure of how likely two nearby pixels are to belong to the same region, we use the spectral graph theoretic framework of normalized cuts to find partitions of the image into regions of coherent texture and brightness. Experimental results on a wide range of images are shown.',\n",
       "  'authors': ['Jitendra Malik ',\n",
       "   ' Serge Belongie ',\n",
       "   ' Thomas Leung ',\n",
       "   ' Jianbo Shi'],\n",
       "  'date': '2001',\n",
       "  'identifier': '2141376824',\n",
       "  'references': ['2121947440',\n",
       "   '2145023731',\n",
       "   '1578099820',\n",
       "   '1997063559',\n",
       "   '2121927366',\n",
       "   '1634005169',\n",
       "   '3017143921',\n",
       "   '2114487471',\n",
       "   '2160167256',\n",
       "   '1490632837'],\n",
       "  'title': 'Contour and Texture Analysis for Image Segmentation'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Jerome H. Friedman'],\n",
       "  'date': '1991',\n",
       "  'identifier': '2102201073',\n",
       "  'references': ['2798909945',\n",
       "   '3085162807',\n",
       "   '2146766088',\n",
       "   '2162870748',\n",
       "   '2017977879',\n",
       "   '133977063',\n",
       "   '2091886411',\n",
       "   '2040615655',\n",
       "   '3000332379',\n",
       "   '2082102453'],\n",
       "  'title': 'Multivariate Adaptive Regression Splines'},\n",
       " {'abstract': 'Collaborative filtering or recommender systems use a database about user preferences to predict additional topics or products a new user might like. In this paper we describe several algorithms designed for this task, including techniques based on correlation coefficients, vector-based similarity calculations, and statistical Bayesian methods. We compare the predictive accuracy of the various methods in a set of representative problem domains. We use two basic classes of evaluation metrics. The first characterizes accuracy over a set of individual predictions in terms of average absolute deviation. The second estimates the utility of a ranked list of suggested items. This metric uses an estimate of the probability that a user will see a recommendation in an ordered list. Experiments were run for datasets associated with 3 application areas, 4 experimental protocols, and the 2 evaluation metr rics for the various algorithms. Results indicate that for a wide range of conditions, Bayesian networks with decision trees at each node and correlation methods outperform Bayesian-clustering and vector-similarity methods. Between correlation and Bayesian networks, the preferred method depends on the nature of the dataset, nature of the application (ranked versus one-by-one presentation), and the availability of votes with which to make predictions. Other considerations include the size of database, speed of predictions, and learning time.',\n",
       "  'authors': ['John S. Breese ', ' David Heckerman ', ' Carl Kadie'],\n",
       "  'date': '1998',\n",
       "  'identifier': '2110325612',\n",
       "  'references': ['2155106456',\n",
       "   '2049633694',\n",
       "   '2341865734',\n",
       "   '1956559956',\n",
       "   '1524704912',\n",
       "   '2066590388',\n",
       "   '1567331820',\n",
       "   '1864566053',\n",
       "   '2031636842',\n",
       "   '1557657228'],\n",
       "  'title': 'Empirical analysis of predictive algorithms for collaborative filtering'},\n",
       " {'abstract': 'We introduce a very large family of binary features for two-dimensional shapes. The salient ones for separating particular shapes are determined by inductive learning during the construction of classification trees. There is a feature for every possible geometric arrangement of local topographic codes. The arrangements express coarse constraints on relative angles and distances among the code locations and are nearly invariant to substantial affine and nonlinear deformations. They are also partially ordered, which makes it possible to narrow the search for informative ones at each node of the tree. Different trees correspond to different aspects of shape. They are statistically and weakly dependent due to randomization and are aggregated in a simple way. Adapting the algorithm to a shape family is then fully automatic once training samples are provided. As an illustration, we classified handwritten digits from the NIST database; the error rate was 0.7 percent.',\n",
       "  'authors': ['Y. Amit 1', ' D. Geman 2', ' K. Wilder 2'],\n",
       "  'date': '1997',\n",
       "  'identifier': '2101522199',\n",
       "  'references': ['2912934387',\n",
       "   '3085162807',\n",
       "   '2087347434',\n",
       "   '1594031697',\n",
       "   '2120240539',\n",
       "   '1676820704',\n",
       "   '2154579312',\n",
       "   '2102734279',\n",
       "   '2168228682',\n",
       "   '2100659887'],\n",
       "  'title': 'Joint induction of shape features and tree classifiers'},\n",
       " {'abstract': 'WINNER OF THE 2005 DEGROOT PRIZE! This book is for people who want to learn probability and statistics quickly. It brings together many of the main ideas in modern statistics in one place. The book is suitable for students and researchers in statistics, computer science, data mining and machine learning. This book covers a much wider range of topics than a typical introductory text on mathematical statistics. It includes modern topics like nonparametric curve estimation, bootstrapping and classification, topics that are usually relegated to follow-up courses. The reader is assumed to know calculus and a little linear algebra. No previous knowledge of probability and statistics is required. The text can be used at the advanced undergraduate and graduate level.',\n",
       "  'authors': ['Larry Wasserman'],\n",
       "  'date': '2004',\n",
       "  'identifier': '1496357020',\n",
       "  'references': ['1663973292',\n",
       "   '2000042664',\n",
       "   '1521626219',\n",
       "   '2530395818',\n",
       "   '1951289974',\n",
       "   '255556494',\n",
       "   '1607663648',\n",
       "   '2552194003'],\n",
       "  'title': 'All of Statistics: A Concise Course in Statistical Inference'},\n",
       " {'abstract': 'We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.',\n",
       "  'authors': ['Matthew D. Hoffman 1',\n",
       "   ' David M. Blei 2',\n",
       "   ' Chong Wang 3',\n",
       "   ' John Paisley 4'],\n",
       "  'date': '2013',\n",
       "  'identifier': '2166851633',\n",
       "  'references': ['1880262756',\n",
       "   '2125838338',\n",
       "   '1503398984',\n",
       "   '1506806321',\n",
       "   '1511986666',\n",
       "   '1981457167',\n",
       "   '2001082470',\n",
       "   '2159080219',\n",
       "   '2158266063',\n",
       "   '2174706414'],\n",
       "  'title': 'Stochastic variational inference'},\n",
       " {'abstract': 'The summarization track at the Text Analysis Conference (TAC) is a direct continuation of the Document Understanding Conference (DUC) series of workshops, focused on providing common data and evaluation framework for research in automatic summarization. In the TAC 2008 summarization track, the main task was to produce two 100-word summaries from two related sets of 10 documents, where the second summary was an update summary. While all of the 71 submitted runs were automatically scored with the ROUGE and BE metrics, NIST assessors manually evaluated only 57 of the submitted runs for readability, content, and overall responsiveness.',\n",
       "  'authors': ['Hoa Trang Dang ', ' Karolina Owczarzak'],\n",
       "  'date': '2008',\n",
       "  'identifier': '2182572585',\n",
       "  'references': ['2154652894', '2134061542', '2140424253'],\n",
       "  'title': 'Overview of the TAC 2008 Update Summarization Task.'},\n",
       " {'abstract': 'Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [1] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P(y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.',\n",
       "  'authors': ['James S. Bergstra 1',\n",
       "   ' Rémi Bardenet 2',\n",
       "   ' Yoshua Bengio 3',\n",
       "   ' Balázs Kégl 2'],\n",
       "  'date': '2011',\n",
       "  'identifier': '2106411961',\n",
       "  'references': ['2136922672',\n",
       "   '2310919327',\n",
       "   '1746819321',\n",
       "   '1554663460',\n",
       "   '2145094598',\n",
       "   '2097998348',\n",
       "   '2123649031',\n",
       "   '2118858186',\n",
       "   '1994197834',\n",
       "   '2144161366'],\n",
       "  'title': 'Algorithms for Hyper-Parameter Optimization'},\n",
       " {'abstract': 'Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.',\n",
       "  'authors': ['Richard Socher 1',\n",
       "   ' Alex Perelygin ',\n",
       "   ' Jean Wu 1',\n",
       "   ' Jason Chuang 2',\n",
       "   ' Christopher D. Manning 1',\n",
       "   ' Andrew Ng 1',\n",
       "   ' Christopher Potts 1'],\n",
       "  'date': '2013',\n",
       "  'identifier': '2251939518',\n",
       "  'references': ['2146502635',\n",
       "   '2097726431',\n",
       "   '2117130368',\n",
       "   '2132339004',\n",
       "   '1423339008',\n",
       "   '71795751',\n",
       "   '1662133657',\n",
       "   '1889268436',\n",
       "   '2164019165',\n",
       "   '2097606805'],\n",
       "  'title': 'Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank'},\n",
       " {'abstract': 'In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.',\n",
       "  'authors': ['Jürgen Schmidhuber'],\n",
       "  'date': '2015',\n",
       "  'identifier': '2076063813',\n",
       "  'references': ['2618530766',\n",
       "   '2151103935',\n",
       "   '2097117768',\n",
       "   '2102605133',\n",
       "   '2130942839',\n",
       "   '2156909104',\n",
       "   '1663973292',\n",
       "   '2136922672',\n",
       "   '1849277567',\n",
       "   '2963542991'],\n",
       "  'title': 'Deep learning in neural networks'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Thomas N. Kipf ', ' Max Welling'],\n",
       "  'date': '2016',\n",
       "  'identifier': '2964015378',\n",
       "  'references': ['2962711740',\n",
       "   '2907492528',\n",
       "   '3100848837',\n",
       "   '2963224980',\n",
       "   '2962883549',\n",
       "   '2963184176',\n",
       "   '2796426482',\n",
       "   '3100278010',\n",
       "   '2786016794',\n",
       "   '2966149470'],\n",
       "  'title': 'Semi-Supervised Classification with Graph Convolutional Networks'},\n",
       " {'abstract': 'This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available. We introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However these assumptions are often violated in practice, and poor performance can result. We present two extensions to the algorithm that improve classification accuracy under these conditions: (1) a weighting factor to modulate the contribution of the unlabeled data, and (2) the use of multiple mixture components per class. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30%.',\n",
       "  'authors': ['Kamal Nigam 1',\n",
       "   ' Andrew Kachites McCallum 2',\n",
       "   ' Sebastian Thrun 1',\n",
       "   ' Tom Mitchell 1'],\n",
       "  'date': '2000',\n",
       "  'identifier': '2097089247',\n",
       "  'references': ['2099111195',\n",
       "   '2149684865',\n",
       "   '2049633694',\n",
       "   '2435251607',\n",
       "   '2048679005',\n",
       "   '2117853077',\n",
       "   '2114535528',\n",
       "   '1550206324',\n",
       "   '2140785063',\n",
       "   '2138745909'],\n",
       "  'title': 'Text Classification from Labeled and Unlabeled Documents using EM'},\n",
       " {'abstract': 'Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them \"curriculum learning\". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).',\n",
       "  'authors': ['Yoshua Bengio 1',\n",
       "   ' Jérôme Louradour 2',\n",
       "   ' Ronan Collobert 3',\n",
       "   ' Jason Weston 3'],\n",
       "  'date': '2009',\n",
       "  'identifier': '2296073425',\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2072128103',\n",
       "   '2117130368',\n",
       "   '2025768430',\n",
       "   '2110798204',\n",
       "   '2099866409',\n",
       "   '1994197834',\n",
       "   '2172174689',\n",
       "   '205159212'],\n",
       "  'title': 'Curriculum learning'},\n",
       " {'abstract': 'To judge whether the difference between two point estimates is statistically significant, data analysts often examine the overlap between the two associated confidence intervals. We compare this technique to the standard method of testing significance under the common assumptions of consistency, asymptotic normality, and asymptotic independence of the estimates. Rejection of the null hypothesis by the method of examining overlap implies rejection by the standard method, whereas failure to reject by the method of examining overlap does not imply failure to reject by the standard method. As a consequence, the method of examining overlap is more conservative (i.e., rejects the null hypothesis less often) than the standard method when the null hypothesis is true, and it mistakenly fails to reject the null hypothesis more frequently than does the standard method when the null hypothesis is false. Although the method of examining overlap is simple and especially convenient when lists or graphs of confidence int...',\n",
       "  'authors': ['Nathaniel Schenker ', ' Jane F Gentleman'],\n",
       "  'date': '2001',\n",
       "  'identifier': '2036002592',\n",
       "  'references': ['2150207805',\n",
       "   '2078519449',\n",
       "   '2142824866',\n",
       "   '1974610070',\n",
       "   '2117297458',\n",
       "   '1973031502',\n",
       "   '2132489270',\n",
       "   '2116302707',\n",
       "   '2104895648',\n",
       "   '2171768442'],\n",
       "  'title': 'On Judging the Significance of Differences by Examining the Overlap Between Confidence Intervals'},\n",
       " {'abstract': 'This is the revision of the classic text in the field, adding two new chapters and thoroughly updating all others. The original structure is retained, and the book continues to serve as a combined text/reference.',\n",
       "  'authors': ['Sidney Siegel'],\n",
       "  'date': '1956',\n",
       "  'identifier': '2002664886',\n",
       "  'references': ['2624431344',\n",
       "   '2158847908',\n",
       "   '2155243985',\n",
       "   '2112422413',\n",
       "   '1531237901',\n",
       "   '2061504941',\n",
       "   '2151170651',\n",
       "   '2148540129',\n",
       "   '2157289187'],\n",
       "  'title': 'Nonparametric statistics for the behavioral sciences'},\n",
       " {'abstract': 'Abstract A learning procedure, called back-propagation, for layered networks of deterministic, neuron-like units has been described previously. The ability of the procedure automatically to discover useful internal representations makes it a powerful tool for attacking difficult problems like speech recognition. This paper describes further research on the learning procedure and presents an example in which a network learns a set of filters that enable it to discriminate formant-like patterns in the presence of noise. The generality of the learning procedure is illustrated by a second example in which a similar network learns an edge detection task. The speed of learning is strongly dependent on the shape of the surface formed by the error measure in “weight space”. Examples are given of the error surface for a simple task and an acceleration method that speeds up descent in weight space is illustrated. The main drawback of the learning procedure is the way it scales as the size of the task and the network increases. Some preliminary results on scaling are reported and it is shown how the magnitude of the optimal weight changes depends on the fan-in of the units. Additional results show how the amount of interaction between the weights affects the learning speed. The paper is concluded with a discussion of the difficulties that are likely to be encounted in applying back-propagation to more realistic problems in speech recognition, and some promising approaches to overcoming these difficulties.',\n",
       "  'authors': ['David C. Plaut ', ' Geoffrey E. Hinton'],\n",
       "  'date': '1987',\n",
       "  'identifier': '2021774695',\n",
       "  'references': ['2154642048',\n",
       "   '1652505363',\n",
       "   '1498436455',\n",
       "   '1507849272',\n",
       "   '2155487652',\n",
       "   '1995169133',\n",
       "   '2591802459',\n",
       "   '2010581677'],\n",
       "  'title': 'Learning sets of filters using back-propagation'},\n",
       " {'abstract': \"We study the problem of object classification when training and test classes are disjoint, i.e. no training examples of the target classes are available. This setup has hardly been studied in computer vision research, but it is the rule rather than the exception, because the world contains tens of thousands of different object classes and for only a very few of them image, collections have been formed and annotated with suitable class labels. In this paper, we tackle the problem by introducing attribute-based classification. It performs object detection based on a human-specified high-level description of the target objects instead of training images. The description consists of arbitrary semantic attributes, like shape, color or even geographic information. Because such properties transcend the specific learning task at hand, they can be pre-learned, e.g. from image datasets unrelated to the current task. Afterwards, new classes can be detected based on their attribute representation, without the need for a new training phase. In order to evaluate our method and to facilitate research in this area, we have assembled a new large-scale dataset, “Animals with Attributes”, of over 30,000 animal images that match the 50 classes in Osherson's classic table of how strongly humans associate 85 semantic attributes with animal classes. Our experiments show that by using an attribute layer it is indeed possible to build a learning object detection system that does not require any training images of the target classes.\",\n",
       "  'authors': ['Christoph H Lampert ',\n",
       "   ' Hannes Nickisch ',\n",
       "   ' Stefan Harmeling'],\n",
       "  'date': '2009',\n",
       "  'identifier': '2134270519',\n",
       "  'references': ['2151103935',\n",
       "   '2161969291',\n",
       "   '2119605622',\n",
       "   '1988790447',\n",
       "   '1576445103',\n",
       "   '2154422044',\n",
       "   '2154642048',\n",
       "   '3085162807',\n",
       "   '2120419212',\n",
       "   '2098411764'],\n",
       "  'title': 'Learning to detect unseen object classes by between-class attribute transfer'},\n",
       " {'abstract': 'The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a mathod for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.',\n",
       "  'authors': ['Lawrence Page ',\n",
       "   ' Sergey Brin ',\n",
       "   ' Rajeev Motwani ',\n",
       "   ' Terry Winograd'],\n",
       "  'date': '1999',\n",
       "  'identifier': '1854214752',\n",
       "  'references': ['2148606196',\n",
       "   '3013264884',\n",
       "   '2101196063',\n",
       "   '2108646579',\n",
       "   '1888005072',\n",
       "   '2115022330',\n",
       "   '2097726984',\n",
       "   '1993977866'],\n",
       "  'title': 'The PageRank Citation Ranking : Bringing Order to the Web'},\n",
       " {'abstract': 'We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.',\n",
       "  'authors': ['Matthew D. Zeiler'],\n",
       "  'date': '2012',\n",
       "  'identifier': '6908809',\n",
       "  'references': ['2168231600',\n",
       "   '2147768505',\n",
       "   '1498436455',\n",
       "   '2120420045',\n",
       "   '19621276',\n",
       "   '1994616650'],\n",
       "  'title': 'ADADELTA: An Adaptive Learning Rate Method'},\n",
       " {'abstract': 'Abstract We give conditions ensuring that multilayer feedforward networks with as few as a single hidden layer and an appropriately smooth hidden layer activation function are capable of arbitrarily accurate approximation to an arbitrary function and its derivatives. In fact, these networks can approximate functions that are not differentiable in the classical sense, but possess only a generalized derivative, as is the case for certain piecewise differentiable functions. The conditions imposed on the hidden layer activation function are relatively mild; the conditions imposed on the domain of the function to be approximated have practical implications. Our approximation results provide a previously missing theoretical justification for the use of multilayer feedforward networks in applications requiring simultaneous approximation of a function and its derivatives.',\n",
       "  'authors': ['Kurt Hornik ', ' Maxwell Stinchcombe ', ' Halbert White'],\n",
       "  'date': '1990',\n",
       "  'identifier': '2027197837',\n",
       "  'references': ['2137983211',\n",
       "   '1971735090',\n",
       "   '2056099894',\n",
       "   '2416739038',\n",
       "   '2090270852',\n",
       "   '1613359937',\n",
       "   '1490039160',\n",
       "   '2090248140',\n",
       "   '1537887709',\n",
       "   '1571581645'],\n",
       "  'title': 'Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks'},\n",
       " {'abstract': \"Calderon's formula and a decomposition of $L^2(\\\\mathbb R^n)$ Decomposition of Lipschitz spaces Minimality of $\\\\dot B^0,1_1$ Littlewood-Paley theory The Besov and Triebel-Lizorkin spaces The $\\\\varphi$ -transform Wavelets Calderon-Zygmund operators Potential theory and a result of Muckenhoupt-Wheeden Further applications.\",\n",
       "  'authors': ['Michael Frazier ', ' Björn Jawerth ', ' Guido L. Weiss'],\n",
       "  'date': '1991',\n",
       "  'identifier': '654435104',\n",
       "  'references': ['2146842127',\n",
       "   '2158940042',\n",
       "   '2079724595',\n",
       "   '2069912449',\n",
       "   '1994906459',\n",
       "   '191129667',\n",
       "   '2109504624',\n",
       "   '2033484654',\n",
       "   '2168141504'],\n",
       "  'title': 'Littlewood-Paley Theory and the Study of Function Spaces'},\n",
       " {'abstract': 'We describe a technique for image encoding in which local operators of many scales but identical shape serve as the basis functions. The representation differs from established techniques in that the code elements are localized in spatial frequency as well as in space. Pixel-to-pixel correlations are first removed by subtracting a lowpass filtered copy of the image from the image itself. The result is a net data compression since the difference, or error, image has low variance and entropy, and the low-pass filtered image may represented at reduced sample density. Further data compression is achieved by quantizing the difference image. These steps are then repeated to compress the low-pass image. Iteration of the process at appropriately expanded scales generates a pyramid data structure. The encoding process is equivalent to sampling the image with Laplacian operators of many scales. Thus, the code tends to enhance salient image features. A further advantage of the present code is that it is well suited for many image analysis tasks as well as for image compression. Fast algorithms are described for coding and decoding.',\n",
       "  'authors': ['P. Burt 1', ' E. Adelson 2'],\n",
       "  'date': '1983',\n",
       "  'identifier': '2103504761',\n",
       "  'references': ['1622620102',\n",
       "   '2978983090',\n",
       "   '2078498116',\n",
       "   '2074163268',\n",
       "   '2024397673',\n",
       "   '2099590965',\n",
       "   '1486071255',\n",
       "   '2125311604',\n",
       "   '2089975134',\n",
       "   '118993750'],\n",
       "  'title': 'The Laplacian Pyramid as a Compact Image Code'},\n",
       " {'abstract': 'This article is about a curious phenomenon. Suppose we have a data matrix, which is the superposition of a low-rank component and a sparse component. Can we recover each component individuallyq We prove that under some suitable assumptions, it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the e1 norm. This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily corrupted. This extends to the situation where a fraction of the entries are missing as well. We discuss an algorithm for solving this optimization problem, and present applications in the area of video surveillance, where our methodology allows for the detection of objects in a cluttered background, and in the area of face recognition, where it offers a principled way of removing shadows and specularities in images of faces.',\n",
       "  'authors': ['Emmanuel J. Candès 1',\n",
       "   ' Xiaodong Li 1',\n",
       "   ' Yi Ma 2',\n",
       "   ' John Wright 3'],\n",
       "  'date': '2011',\n",
       "  'identifier': '2145962650',\n",
       "  'references': ['2145096794',\n",
       "   '2148694408',\n",
       "   '2100556411',\n",
       "   '2001141328',\n",
       "   '2124608575',\n",
       "   '2102625004',\n",
       "   '2078204800',\n",
       "   '2097308346',\n",
       "   '2103972604',\n",
       "   '2147152072'],\n",
       "  'title': 'Robust principal component analysis'},\n",
       " {'abstract': 'We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI--SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function.',\n",
       "  'authors': ['P F Felzenszwalb 1',\n",
       "   ' R B Girshick 1',\n",
       "   ' D McAllester 2',\n",
       "   ' D Ramanan 3'],\n",
       "  'date': '2010',\n",
       "  'identifier': '2168356304',\n",
       "  'references': ['2151103935',\n",
       "   '2161969291',\n",
       "   '3097096317',\n",
       "   '2154422044',\n",
       "   '2120419212',\n",
       "   '3111950349',\n",
       "   '3021469268',\n",
       "   '2145072179',\n",
       "   '2030536784',\n",
       "   '2115763357'],\n",
       "  'title': 'Object Detection with Discriminatively Trained Part-Based Models'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Sergey Brin ', ' Lawrence Page'],\n",
       "  'date': '1998',\n",
       "  'identifier': '3013264884',\n",
       "  'references': ['2142827986',\n",
       "   '2089554624',\n",
       "   '2108278206',\n",
       "   '2113112851',\n",
       "   '1984250866',\n",
       "   '2144629587',\n",
       "   '2048562911',\n",
       "   '2007541434',\n",
       "   '2017333496',\n",
       "   '1984137208'],\n",
       "  'title': 'The Anatomy of a Large-Scale Hypertextual Web Search Engine.'},\n",
       " {'abstract': 'Eigenvalues and the Laplacian of a graph Isoperimetric problems Diameters and eigenvalues Paths, flows, and routing Eigenvalues and quasi-randomness Expanders and explicit constructions Eigenvalues of symmetrical graphs Eigenvalues of subgraphs with boundary conditions Harnack inequalities Heat kernels Sobolev inequalities Advanced techniques for random walks on graphs Bibliography Index.',\n",
       "  'authors': ['Fan R K Chung'],\n",
       "  'date': '1996',\n",
       "  'identifier': '1578099820',\n",
       "  'references': ['2901284226', '2053631808', '2027808858'],\n",
       "  'title': 'Spectral Graph Theory'},\n",
       " {'abstract': 'When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.',\n",
       "  'authors': ['Geoffrey E. Hinton ',\n",
       "   ' Nitish Srivastava ',\n",
       "   ' Alex Krizhevsky ',\n",
       "   ' Ilya Sutskever ',\n",
       "   ' Ruslan R. Salakhutdinov'],\n",
       "  'date': '2012',\n",
       "  'identifier': '1904365287',\n",
       "  'references': ['2108598243',\n",
       "   '2911964244',\n",
       "   '3118608800',\n",
       "   '2100495367',\n",
       "   '2310919327',\n",
       "   '2912934387',\n",
       "   '2116064496',\n",
       "   '2147768505',\n",
       "   '1993882792',\n",
       "   '4919037'],\n",
       "  'title': 'Improving neural networks by preventing co-adaptation of feature detectors'},\n",
       " {'abstract': 'From the Publisher: Probabilistic Reasoning in Intelligent Systems is a complete andaccessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertainty\\x97and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognition\\x97in short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information. Probabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability.',\n",
       "  'authors': ['Judea Pearl'],\n",
       "  'date': '1988',\n",
       "  'identifier': '2159080219',\n",
       "  'references': ['2581275558',\n",
       "   '1997063559',\n",
       "   '1593793857',\n",
       "   '2797148637',\n",
       "   '2155322595',\n",
       "   '158727920',\n",
       "   '2138162238',\n",
       "   '2108309071',\n",
       "   '1986808060',\n",
       "   '2142901448'],\n",
       "  'title': 'Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Daniel Zügner ', ' Amir Akbarnejad ', ' Stephan Günnemann'],\n",
       "  'date': '2019',\n",
       "  'identifier': '2966149470',\n",
       "  'references': ['3081300507',\n",
       "   '3106390645',\n",
       "   '2984488829',\n",
       "   '3081203761',\n",
       "   '3012737746',\n",
       "   '3104667978',\n",
       "   '3103409210',\n",
       "   '3101871847'],\n",
       "  'title': 'Adversarial Attacks on Neural Networks for Graph Data.'},\n",
       " {'abstract': 'In recent years there has been a growing interest in the study of sparse representation of signals. Using an overcomplete dictionary that contains prototype signal-atoms, signals are described by sparse linear combinations of these atoms. Applications that use sparse representation are many and include compression, regularization in inverse problems, feature extraction, and more. Recent activity in this field has concentrated mainly on the study of pursuit algorithms that decompose signals with respect to a given dictionary. Designing dictionaries to better fit the above model can be done by either selecting one from a prespecified set of linear transforms or adapting the dictionary to a set of training signals. Both of these techniques have been considered, but this topic is largely still open. In this paper we propose a novel algorithm for adapting dictionaries in order to achieve sparse signal representations. Given a set of training signals, we seek the dictionary that leads to the best representation for each member in this set, under strict sparsity constraints. We present a new method-the K-SVD algorithm-generalizing the K-means clustering process. K-SVD is an iterative method that alternates between sparse coding of the examples based on the current dictionary and a process of updating the dictionary atoms to better fit the data. The update of the dictionary columns is combined with an update of the sparse representations, thereby accelerating convergence. The K-SVD algorithm is flexible and can work with any pursuit method (e.g., basis pursuit, FOCUSS, or matching pursuit). We analyze this algorithm and demonstrate its results both on synthetic tests and in applications on real image data',\n",
       "  'authors': ['M. Aharon ', ' M. Elad ', ' A. Bruckstein'],\n",
       "  'date': '2006',\n",
       "  'identifier': '2160547390',\n",
       "  'references': ['2078204800',\n",
       "   '2116148865',\n",
       "   '2099641086',\n",
       "   '2151693816',\n",
       "   '3110653090',\n",
       "   '2049633694',\n",
       "   '2097323375',\n",
       "   '1634005169',\n",
       "   '2050834445',\n",
       "   '2154332973'],\n",
       "  'title': '$rm K$ -SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation'},\n",
       " {'abstract': 'Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.',\n",
       "  'authors': ['Quoc Le ', ' Tomas Mikolov'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2131744502',\n",
       "  'references': ['2153579005',\n",
       "   '1614298861',\n",
       "   '2158899491',\n",
       "   '2131744502',\n",
       "   '2251939518',\n",
       "   '2141599568',\n",
       "   '2117130368',\n",
       "   '2132339004',\n",
       "   '2158139315',\n",
       "   '2113459411'],\n",
       "  'title': 'Distributed Representations of Sentences and Documents'},\n",
       " {'abstract': 'In this paper, we propose a novel neural network model called RNN Encoder‐ Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder‐Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.',\n",
       "  'authors': ['Kyunghyun Cho 1',\n",
       "   ' Bart van Merrienboer 1',\n",
       "   ' Caglar Gulcehre 1',\n",
       "   ' Dzmitry Bahdanau 2',\n",
       "   ' Fethi Bougares 2',\n",
       "   ' Holger Schwenk 2',\n",
       "   ' Yoshua Bengio 3',\n",
       "   ' 4',\n",
       "   ' 5'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2157331557',\n",
       "  'references': ['2618530766',\n",
       "   '2153579005',\n",
       "   '2064675550',\n",
       "   '2294059674',\n",
       "   '2147768505',\n",
       "   '2132339004',\n",
       "   '6908809',\n",
       "   '1753482797',\n",
       "   '2156387975',\n",
       "   '2963504252'],\n",
       "  'title': 'Learning Phrase Representations using RNN Encoder--Decoder for Statistical Machine Translation'},\n",
       " {'abstract': 'LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail.',\n",
       "  'authors': ['Chih-Chung Chang ', ' Chih-Jen Lin'],\n",
       "  'date': '2011',\n",
       "  'identifier': '2153635508',\n",
       "  'references': ['2148603752',\n",
       "   '2119821739',\n",
       "   '2109943925',\n",
       "   '2172000360',\n",
       "   '3021469268',\n",
       "   '1512098439',\n",
       "   '2104978738',\n",
       "   '2087347434',\n",
       "   '2132870739',\n",
       "   '2124351082'],\n",
       "  'title': 'LIBSVM: A library for support vector machines'},\n",
       " {'abstract': \"Stable local feature detection and representation is a fundamental component of many image registration and object recognition algorithms. Mikolajczyk and Schmid (June 2003) recently evaluated a variety of approaches and identified the SIFT [D. G. Lowe, 1999] algorithm as being the most resistant to common image deformations. This paper examines (and improves upon) the local image descriptor used by SIFT. Like SIFT, our descriptors encode the salient aspects of the image gradient in the feature point's neighborhood; however, instead of using SIFT's smoothed weighted histograms, we apply principal components analysis (PCA) to the normalized gradient patch. Our experiments demonstrate that the PCA-based local descriptors are more distinctive, more robust to image deformations, and more compact than the standard SIFT representation. We also present results showing that using these descriptors in an image retrieval application results in increased accuracy and faster matching.\",\n",
       "  'authors': ['Yan Ke ', ' R. Sukthankar'],\n",
       "  'date': '2004',\n",
       "  'identifier': '2145072179',\n",
       "  'references': ['2151103935',\n",
       "   '2148694408',\n",
       "   '2124386111',\n",
       "   '2177274842',\n",
       "   '1902027874',\n",
       "   '2154422044',\n",
       "   '2119747362',\n",
       "   '2111308925',\n",
       "   '2098693229',\n",
       "   '1541642243'],\n",
       "  'title': 'PCA-SIFT: a more distinctive representation for local image descriptors'},\n",
       " {'abstract': 'We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs, respectively.',\n",
       "  'authors': ['Dan C. Cireşan ',\n",
       "   ' Ueli Meier ',\n",
       "   ' Jonathan Masci ',\n",
       "   ' Luca M. Gambardella ',\n",
       "   ' Jürgen Schmidhuber'],\n",
       "  'date': '2011',\n",
       "  'identifier': '2148461049',\n",
       "  'references': ['3118608800',\n",
       "   '2310919327',\n",
       "   '2546302380',\n",
       "   '2118858186',\n",
       "   '2134557905',\n",
       "   '2156163116',\n",
       "   '2144161366',\n",
       "   '1624854622',\n",
       "   '2132424367',\n",
       "   '2105464873'],\n",
       "  'title': 'Flexible, high performance convolutional neural networks for image classification'},\n",
       " {'abstract': 'Computational properties of use to biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.',\n",
       "  'authors': ['John J. Hopfield'],\n",
       "  'date': '1999',\n",
       "  'identifier': '2293063825',\n",
       "  'references': ['1594524188', '2086789740', '2970228278', '2076870593'],\n",
       "  'title': 'Neural networks and physical systems with emergent collective computational abilities'},\n",
       " {'abstract': 'The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.',\n",
       "  'authors': ['Volodymyr Mnih ',\n",
       "   ' Koray Kavukcuoglu ',\n",
       "   ' David Silver ',\n",
       "   ' Andrei A. Rusu ',\n",
       "   ' Joel Veness ',\n",
       "   ' Marc G. Bellemare ',\n",
       "   ' Alex Graves ',\n",
       "   ' Martin Riedmiller ',\n",
       "   ' Andreas K. Fidjeland ',\n",
       "   ' Georg Ostrovski ',\n",
       "   ' Stig Petersen ',\n",
       "   ' Charles Beattie ',\n",
       "   ' Amir Sadik ',\n",
       "   ' Ioannis Antonoglou ',\n",
       "   ' Helen King ',\n",
       "   ' Dharshan Kumaran ',\n",
       "   ' Daan Wierstra ',\n",
       "   ' Shane Legg ',\n",
       "   ' Demis Hassabis'],\n",
       "  'date': '2015',\n",
       "  'identifier': '2145339207',\n",
       "  'references': ['2618530766',\n",
       "   '2100495367',\n",
       "   '2310919327',\n",
       "   '2187089797',\n",
       "   '1665214252',\n",
       "   '2072128103',\n",
       "   '2546302380',\n",
       "   '1652505363',\n",
       "   '2121863487',\n",
       "   '2952509347'],\n",
       "  'title': 'Human-level control through deep reinforcement learning'},\n",
       " {'abstract': 'This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described. >',\n",
       "  'authors': ['L.R. Rabiner'],\n",
       "  'date': '1989',\n",
       "  'identifier': '2125838338',\n",
       "  'references': ['2049633694',\n",
       "   '2105594594',\n",
       "   '1966812932',\n",
       "   '2142384583',\n",
       "   '2002182716',\n",
       "   '1966264494',\n",
       "   '2171850596',\n",
       "   '1991133427',\n",
       "   '2022554507',\n",
       "   '1877570817'],\n",
       "  'title': 'A tutorial on hidden Markov models and selected applications in speech recognition'},\n",
       " {'abstract': 'Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term--document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.',\n",
       "  'authors': ['Andrew L. Maas ',\n",
       "   ' Raymond E. Daly ',\n",
       "   ' Peter T. Pham ',\n",
       "   ' Dan Huang ',\n",
       "   ' Andrew Y. Ng ',\n",
       "   ' Christopher Potts'],\n",
       "  'date': '2011',\n",
       "  'identifier': '2113459411',\n",
       "  'references': ['1880262756',\n",
       "   '2118585731',\n",
       "   '2117130368',\n",
       "   '2166706824',\n",
       "   '2132339004',\n",
       "   '2158139315',\n",
       "   '2147152072',\n",
       "   '2114524997',\n",
       "   '1662133657',\n",
       "   '2163455955'],\n",
       "  'title': 'Learning Word Vectors for Sentiment Analysis'},\n",
       " {'abstract': 'While many models of biological object recognition share a common set of ‘‘broad-stroke’’ properties, the performance of any one model depends strongly on the choice of parameters in a particular instantiation of that model—e.g., the number of units per layer, the size of pooling kernels, exponents in normalization operations, etc. Since the number of such parameters (explicit or implicit) is typically large and the computational cost of evaluating one particular parameter set is high, the space of possible model instantiations goes largely unexplored. Thus, when a model fails to approach the abilities of biological visual systems, we are left uncertain whether this failure is because we are missing a fundamental idea or because the correct ‘‘parts’’ have not been tuned correctly, assembled at sufficient scale, or provided with enough training. Here, we present a high-throughput approach to the exploration of such parameter sets, leveraging recent advances in stream processing hardware (high-end NVIDIA graphic cards and the PlayStation 3’s IBM Cell Processor). In analogy to highthroughput screening approaches in molecular biology and genetics, we explored thousands of potential network architectures and parameter instantiations, screening those that show promising object recognition performance for further analysis. We show that this approach can yield significant, reproducible gains in performance across an array of basic object recognition tasks, consistently outperforming a variety of state-of-the-art purpose-built vision systems from the literature. As the scale of available computational power continues to expand, we argue that this approach has the potential to greatly accelerate progress in both artificial vision and our understanding of the computational underpinning of biological vision.',\n",
       "  'authors': ['Nicolas Pinto 1',\n",
       "   ' 2',\n",
       "   ' David Doukhan 1',\n",
       "   ' 2',\n",
       "   ' James J. DiCarlo 1',\n",
       "   ' 2',\n",
       "   ' David Daniel Cox 1',\n",
       "   ' 2',\n",
       "   ' 3'],\n",
       "  'date': '2009',\n",
       "  'identifier': '2144161366',\n",
       "  'references': ['2151103935',\n",
       "   '2124776405',\n",
       "   '2152195021',\n",
       "   '2162915993',\n",
       "   '1554663460',\n",
       "   '2613176274',\n",
       "   '1595498733',\n",
       "   '1782590233',\n",
       "   '2493581502',\n",
       "   '2166049352'],\n",
       "  'title': 'A High-Throughput Screening Approach to Discovering Good Forms of Biologically Inspired Visual Representation'},\n",
       " {'abstract': 'Abstract We attempt to recover a function of unknown smoothness from noisy sampled data. We introduce a procedure, SureShrink, that suppresses noise by thresholding the empirical wavelet coefficients. The thresholding is adaptive: A threshold level is assigned to each dyadic resolution level by the principle of minimizing the Stein unbiased estimate of risk (Sure) for threshold estimates. The computational effort of the overall procedure is order N · log(N) as a function of the sample size N. SureShrink is smoothness adaptive: If the unknown function contains jumps, then the reconstruction (essentially) does also; if the unknown function has a smooth piece, then the reconstruction is (essentially) as smooth as the mother wavelet will allow. The procedure is in a sense optimally smoothness adaptive: It is near minimax simultaneously over a whole interval of the Besov scale; the size of this interval depends on the choice of mother wavelet. We know from a previous paper by the authors that traditional smoot...',\n",
       "  'authors': ['David L. Donoho ', ' Iain M. Johnstone'],\n",
       "  'date': '1995',\n",
       "  'identifier': '2079724595',\n",
       "  'references': ['2062024414',\n",
       "   '2132984323',\n",
       "   '2158940042',\n",
       "   '2098914003',\n",
       "   '2102201073',\n",
       "   '191129667',\n",
       "   '139959648',\n",
       "   '2166982406',\n",
       "   '162854683',\n",
       "   '2024081693'],\n",
       "  'title': 'Adapting to Unknown Smoothness via Wavelet Shrinkage'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Yann Lecun'],\n",
       "  'date': '1989',\n",
       "  'identifier': '169539560',\n",
       "  'references': ['2310919327',\n",
       "   '2163922914',\n",
       "   '2734408173',\n",
       "   '1546771929',\n",
       "   '2147800946',\n",
       "   '1496559305',\n",
       "   '1806891645'],\n",
       "  'title': 'Generalization and network design strategies'},\n",
       " {'abstract': \"XSEarch, a semantic search engine for XML, is presented. XSEarch has a simple query language, suitable for a naive user. It returns semantically related document fragments that satisfy the user's query. Query answers are ranked using extended information-retrieval techniques and are generated in an order similar to the ranking. Advanced indexing techniques were developed to facilitate efficient implementation of XSEarch. The performance of the different techniques as well as the recall and the precision were measured experimentally. These experiments indicate that XSEarch is efficient, scalable and ranks quality results highly.\",\n",
       "  'authors': ['Sara Cohen ',\n",
       "   ' Jonathan Mamou ',\n",
       "   ' Yaron Kanza ',\n",
       "   ' Yehoshua Sagiv'],\n",
       "  'date': '2003',\n",
       "  'identifier': '2113112851',\n",
       "  'references': ['3013264884',\n",
       "   '1660390307',\n",
       "   '1973828215',\n",
       "   '2084243240',\n",
       "   '2074863013',\n",
       "   '2108572131',\n",
       "   '2124770891',\n",
       "   '1562549404',\n",
       "   '43380317',\n",
       "   '2131747693'],\n",
       "  'title': 'XSEarch: a semantic search engine for XML'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Arthur P. Dempster ', ' Nan M. Laird ', ' Donald B. Rubin'],\n",
       "  'date': '1977',\n",
       "  'identifier': '2049633694',\n",
       "  'references': ['2798643531',\n",
       "   '2100358124',\n",
       "   '2074673068',\n",
       "   '2403035479',\n",
       "   '2327022120',\n",
       "   '1982585616',\n",
       "   '1575431606',\n",
       "   '2086699924',\n",
       "   '2000084758',\n",
       "   '2144578442'],\n",
       "  'title': 'Maximum likelihood from incomplete data via the EM algorithm'},\n",
       " {'abstract': 'We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.',\n",
       "  'authors': ['John D. Lafferty ',\n",
       "   ' Andrew McCallum ',\n",
       "   ' Fernando C. N. Pereira'],\n",
       "  'date': '2001',\n",
       "  'identifier': '2147880316',\n",
       "  'references': ['2310919327',\n",
       "   '1988790447',\n",
       "   '1574901103',\n",
       "   '2009570821',\n",
       "   '2096175520',\n",
       "   '1934019294',\n",
       "   '1773803948',\n",
       "   '2160842254',\n",
       "   '2117400858',\n",
       "   '3021452258'],\n",
       "  'title': 'Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data'},\n",
       " {'abstract': 'The experimental evidence accumulated over the past 20 years indicates that textindexing systems based on the assignment of appropriately weighted single terms produce retrieval results that are superior to those obtainable with other more elaborate text representations. These results depend crucially on the choice of effective term weighting systems. This paper summarizes the insights gained in automatic term weighting, and provides baseline single term indexing models with which other more elaborate content analysis procedures can be compared.',\n",
       "  'authors': ['Gerard Salton ', ' Christopher Buckley'],\n",
       "  'date': '1988',\n",
       "  'identifier': '1978394996',\n",
       "  'references': ['1956559956',\n",
       "   '2043909051',\n",
       "   '2083605078',\n",
       "   '2068632118',\n",
       "   '3090556797',\n",
       "   '2095396650',\n",
       "   '2075006521',\n",
       "   '11171803',\n",
       "   '3091372544',\n",
       "   '1557757161'],\n",
       "  'title': 'Term Weighting Approaches in Automatic Text Retrieval'},\n",
       " {'abstract': 'In this paper we present several of the salient theoretical and practical issues associated with modeling a speech signal as a probabilistic function of a (hidden) Markov chain. First we give a concise review of the literature with emphasis on the Baum-Welch algorithm. This is followed by a detailed discussion of three issues not treated in the literature: alternatives to the Baum-Welch algorithm; critical facets of the implementation of the algorithms, with emphasis on their numerical properties; and behavior of Markov models on certain artificial but realistic problems. Special attention is given to a particular class of Markov models, which we call “left-to-right” models. This class of models is especially appropriate for isolated word recognition. The results of the application of these methods to an isolated word, speaker-independent speech recognition experiment are given in a companion paper.',\n",
       "  'authors': ['S. E. Levinson ', ' L. R. Rabiner ', ' M. M. Sondhi'],\n",
       "  'date': '1983',\n",
       "  'identifier': '2171850596',\n",
       "  'references': ['2022772618',\n",
       "   '1575431606',\n",
       "   '1990005915',\n",
       "   '2021760654',\n",
       "   '2086699924',\n",
       "   '1980800561',\n",
       "   '2163929346',\n",
       "   '2057833190',\n",
       "   '2007321142',\n",
       "   '2077574412'],\n",
       "  'title': 'An introduction to the application of the theory of probabilistic functions of a Markov process to automatic speech recognition'},\n",
       " {'abstract': 'Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.',\n",
       "  'authors': ['Christian Szegedy 1',\n",
       "   ' Vincent Vanhoucke 1',\n",
       "   ' Sergey Ioffe 1',\n",
       "   ' Jon Shlens 1',\n",
       "   ' Zbigniew Wojna 2'],\n",
       "  'date': '2016',\n",
       "  'identifier': '2183341477',\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '1836465849',\n",
       "   '2102605133',\n",
       "   '2117539524',\n",
       "   '1903029394',\n",
       "   '1677182931',\n",
       "   '2096733369',\n",
       "   '2016053056'],\n",
       "  'title': 'Rethinking the Inception Architecture for Computer Vision'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Anshu Sinha'],\n",
       "  'date': '1999',\n",
       "  'identifier': '51975515',\n",
       "  'references': ['2124776405',\n",
       "   '2227933188',\n",
       "   '2126727781',\n",
       "   '2256679588',\n",
       "   '2122827492',\n",
       "   '49742075',\n",
       "   '2072181047',\n",
       "   '2107600630',\n",
       "   '1519534430',\n",
       "   '2151513848'],\n",
       "  'title': 'An improved recognition module for the identification of handwritten digits'},\n",
       " {'abstract': 'Arithmetic coding is a data compression technique that encodes data (the data string) by creating a code string which represents a fractional value on the number line between 0 and 1. The coding algorithm is symbolwise recursive; i.e., it operates upon and encodes (decodes) one data symbol per iteration or recursion. On each recursion, the algorithm successively partitions an interval of the number line between 0 and 1, and retains one of the partitions as the new interval. Thus, the algorithm successively deals with smaller intervals, and the code string, viewed as a magnitude, lies in each of the nested intervals. The data string is recovered by using magnitude comparisons on the code string to recreate how the encoder must have successively partitioned and retained each nested subinterval. Arithmetic coding differs considerably from the more familiar compression coding techniques, such as prefix (Huffman) codes. Also, it should not be confused with error control coding, whose object is to detect and correct errors in computer operations. This paper presents the key notions of arithmetic compression coding by means of simple examples.',\n",
       "  'authors': ['Glen G. Langdon'],\n",
       "  'date': '1984',\n",
       "  'identifier': '2107927941',\n",
       "  'references': ['1995875735',\n",
       "   '2119047110',\n",
       "   '2165564574',\n",
       "   '2911940095',\n",
       "   '2046419776',\n",
       "   '1519253855',\n",
       "   '2034323860',\n",
       "   '2031366154',\n",
       "   '2006384477',\n",
       "   '158805393'],\n",
       "  'title': 'An introduction to arithmetic coding'},\n",
       " {'abstract': 'Abstract The results of a multi-year research program to identify the factors associated with variations in subjective workload within and between different types of tasks are reviewed. Subjective evaluations of 10 workload-related factors were obtained from 16 different experiments. The experimental tasks included simple cognitive and manual control tasks, complex laboratory and supervisory control tasks, and aircraft simulation. Task-, behavior-, and subject-related correlates of subjective workload experiences varied as a function of difficulty manipulations within experiments, different sources of workload between experiments, and individual differences in workload definition. A multi-dimensional rating scale is proposed in which information about the magnitude and sources of six workload-related factors are combined to derive a sensitive and reliable estimate of workload.',\n",
       "  'authors': ['Sandra G. Hart 1', ' Lowell E. Staveland 2'],\n",
       "  'date': '1988',\n",
       "  'identifier': '2157289187',\n",
       "  'references': ['158727920',\n",
       "   '2164558494',\n",
       "   '1983186110',\n",
       "   '2002664886',\n",
       "   '2041656211',\n",
       "   '1967670055',\n",
       "   '1532354624',\n",
       "   '1888386172',\n",
       "   '2099186322',\n",
       "   '1965700183'],\n",
       "  'title': 'Development of NASA-TLX (Task Load Index): Results of Empirical and Theoretical Research'},\n",
       " {'abstract': \"This article presents new results on using a greedy algorithm, orthogonal matching pursuit (OMP), to solve the sparse approximation problem over redundant dictionaries. It provides a sufficient condition under which both OMP and Donoho's basis pursuit (BP) paradigm can recover the optimal representation of an exactly sparse signal. It leverages this theory to show that both OMP and BP succeed for every sparse input signal from a wide class of dictionaries. These quasi-incoherent dictionaries offer a natural generalization of incoherent dictionaries, and the cumulative coherence function is introduced to quantify the level of incoherence. This analysis unifies all the recent results on BP and extends them to OMP. Furthermore, the paper develops a sufficient condition under which OMP can identify atoms from an optimal approximation of a nonsparse signal. From there, it argues that OMP is an approximation algorithm for the sparse problem over a quasi-incoherent dictionary. That is, for every input signal, OMP calculates a sparse approximant whose error is only a small factor worse than the minimal error that can be attained with the same number of terms.\",\n",
       "  'authors': ['J.A. Tropp'],\n",
       "  'date': '2004',\n",
       "  'identifier': '2116148865',\n",
       "  'references': ['2078204800',\n",
       "   '2610857016',\n",
       "   '2099641086',\n",
       "   '2151693816',\n",
       "   '2154332973',\n",
       "   '2156447271',\n",
       "   '2136235822',\n",
       "   '391578156',\n",
       "   '1605417594',\n",
       "   '2167839759'],\n",
       "  'title': 'Greed is good: algorithmic results for sparse approximation'},\n",
       " {'abstract': 'An internally or externally paced event results not only in the generation of an event-related potential (ERP) but also in a change in the ongoing EEG/MEG in form of an event-related desynchronization (ERD) or event-related synchronization (ERS). The ERP on the one side and the ERD/ERS on the other side are different responses of neuronal structures in the brain. While the former is phase-locked, the latter is not phase-locked to the event. The most important difference between both phenomena is that the ERD/ERS is highly frequency band-specific, whereby either the same or different locations on the scalp can display ERD and ERS simultaneously. Quantification of ERD/ERS in time and space is demonstrated on data from a number of movement experiments.',\n",
       "  'authors': ['G. Pfurtscheller 1', ' F.H. Lopes da Silva 2'],\n",
       "  'date': '1999',\n",
       "  'identifier': '2116308679',\n",
       "  'references': ['2331432542',\n",
       "   '1489213177',\n",
       "   '2079948225',\n",
       "   '2153134014',\n",
       "   '2059915684',\n",
       "   '2172513721',\n",
       "   '2006082701',\n",
       "   '2096722100',\n",
       "   '2090475450',\n",
       "   '2100741386'],\n",
       "  'title': 'Event-related EEG/MEG synchronization and desynchronization: basic principles.'},\n",
       " {'abstract': 'Hinton (in press) recently proposed a learning algorithm called contrastive divergence learning for a class of probabilistic models called product of experts (PoE). Whereas in standard mixture models the “beliefs” of individual experts are averaged, in PoEs the “beliefs” are multiplied together and then renormalized. One advantage of this approach is that the combined beliefs can be much sharper than the individual beliefs of each expert. It has been shown that a restricted version of the Boltzmann machine, in which there are no lateral connections between hidden units or between observation units, is a PoE. In this paper we generalize these results to diffusion networks, a continuous-time, continuous-state version of the Boltzmann machine. We show that when the unit activation functions are linear, this PoE architecture is equivalent to a factor analyzer. This result suggests novel non-linear generalizations of factor analysis and independent component analysis that could be implemented using interactive neural circuitry.',\n",
       "  'authors': ['Tim K. Marks ', ' Javier R. Movellan'],\n",
       "  'date': '2001',\n",
       "  'identifier': '145818128',\n",
       "  'references': ['2116064496',\n",
       "   '2138451337',\n",
       "   '3110653090',\n",
       "   '2137969290',\n",
       "   '2946776431',\n",
       "   '2102432043'],\n",
       "  'title': 'Diffusion Networks, Products of Experts, and Factor Analysis'},\n",
       " {'abstract': 'Abstract In this paper the authors describe the results of their investigation into the development of a recognition algorithm for identifying numerals that may be isolated or connected, broken or continuous. Using a structural classification scheme, the recognition algorithm is derived as a tree classifier. In an extensive test experiment, an accuracy of 99% was realized with isolated numerals. When connected numerals were also included a recognition accuracy of 93% was obtained.',\n",
       "  'authors': ['M. Shridhar ', ' A. Badreldin'],\n",
       "  'date': '1986',\n",
       "  'identifier': '2072181047',\n",
       "  'references': ['1623080549',\n",
       "   '2008313333',\n",
       "   '2159498975',\n",
       "   '2122827492',\n",
       "   '2062361515',\n",
       "   '2061315523',\n",
       "   '1966591781',\n",
       "   '1963937384',\n",
       "   '1536628706',\n",
       "   '1979819178'],\n",
       "  'title': 'Recognition of isolated and simply connected hand-written numerals'},\n",
       " {'abstract': 'Most face databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, background, camera quality, and gender. While there are many applications for face recognition technology in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database, Labeled Faces in the Wild, is provided as an aid in studying the latter, unconstrained, recognition problem. The database contains labeled face photographs spanning the range of conditions typically encountered in everyday life. The database exhibits “natural” variability in factors such as pose, lighting, race, accessories, occlusions, and background. In addition to describing the details of the database, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible. We provide baseline results, including results of a state of the art face recognition system combined with a face alignment system. To facilitate experimentation on the database, we provide several parallel databases, including an aligned version.',\n",
       "  'authors': ['Gary B. Huang 1',\n",
       "   ' Marwan Mattar 1',\n",
       "   ' Tamara Berg 2',\n",
       "   ' Eric Learned-Miller 1'],\n",
       "  'date': '2008',\n",
       "  'identifier': '1782590233',\n",
       "  'references': ['3097096317',\n",
       "   '1999478155',\n",
       "   '2121647436',\n",
       "   '2033419168',\n",
       "   '2137659841',\n",
       "   '3111480503',\n",
       "   '2098693229',\n",
       "   '2125310925',\n",
       "   '2994340921',\n",
       "   '2006793117'],\n",
       "  'title': 'Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments'},\n",
       " {'abstract': 'An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds.',\n",
       "  'authors': ['D.G. Lowe'],\n",
       "  'date': '1999',\n",
       "  'identifier': '2124386111',\n",
       "  'references': ['2914885528',\n",
       "   '2124087378',\n",
       "   '2123977795',\n",
       "   '2011891945',\n",
       "   '22745672',\n",
       "   '2096077837',\n",
       "   '2096600681',\n",
       "   '2131806657',\n",
       "   '2042243448',\n",
       "   '1553558465'],\n",
       "  'title': 'Object recognition from local scale-invariant features'},\n",
       " {'abstract': 'Introduction * Properties of Population Principal Components * Properties of Sample Principal Components * Interpreting Principal Components: Examples * Graphical Representation of Data Using Principal Components * Choosing a Subset of Principal Components or Variables * Principal Component Analysis and Factor Analysis * Principal Components in Regression Analysis * Principal Components Used with Other Multivariate Techniques * Outlier Detection, Influential Observations and Robust Estimation * Rotation and Interpretation of Principal Components * Principal Component Analysis for Time Series and Other Non-Independent Data * Principal Component Analysis for Special Types of Data * Generalizations and Adaptations of Principal Component Analysis',\n",
       "  'authors': ['Ian Jolliffe'],\n",
       "  'date': '2002',\n",
       "  'identifier': '2148694408',\n",
       "  'references': ['2122646361',\n",
       "   '2138621811',\n",
       "   '2145962650',\n",
       "   '2139047213',\n",
       "   '2140095548',\n",
       "   '2135029798',\n",
       "   '78159342',\n",
       "   '2145072179'],\n",
       "  'title': 'Principal Component Analysis'},\n",
       " {'abstract': 'We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to ½ everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.',\n",
       "  'authors': ['Ian Goodfellow 1',\n",
       "   ' Jean Pouget-Abadie 1',\n",
       "   ' Mehdi Mirza 1',\n",
       "   ' Bing Xu 1',\n",
       "   ' David Warde-Farley 1',\n",
       "   ' Sherjil Ozair 2',\n",
       "   ' Aaron Courville 1',\n",
       "   ' Yoshua Bengio 1'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2099471712',\n",
       "  'references': ['2618530766',\n",
       "   '2136922672',\n",
       "   '1959608418',\n",
       "   '3118608800',\n",
       "   '2310919327',\n",
       "   '1904365287',\n",
       "   '2964153729',\n",
       "   '2072128103',\n",
       "   '2546302380',\n",
       "   '2294059674'],\n",
       "  'title': 'Generative Adversarial Nets'},\n",
       " {'abstract': 'We describe a set of pyramid transforms that decompose an image into a set of basis functions that are (a) spatial-frequency tuned, (b) orientation tuned, (c) spatially localized, and (d) self-similar. For computational reasons the set is also (e) orthogonal and lends itself to (f) rapid computation. The systems are derived from concepts in matrix algebra, but are closely connected to decompositions based on quadrature mirror filters. Our computations take place hierarchically, leading to a pyramid representation in which all of the basis functions have the same basic shape, and appear at many scales. By placing the high-pass and low-pass kernels on staggered grids, we can derive odd-tap QMF kernels that are quite compact. We have developed pyramids using separable, quincunx, and hexagonal kernels. Image data compression with the pyramids gives excellent results, both in terms of MSE and visual appearance. A non-orthogonal variant allows good performance with 3-tap basis kernels and the appropriate inverse sampling kernels.',\n",
       "  'authors': ['Edward H. Adelson 1',\n",
       "   ' Eero Simoncelli 1',\n",
       "   ' Rajesh Hingorani 2'],\n",
       "  'date': '1987',\n",
       "  'identifier': '2058719583',\n",
       "  'references': ['2132984323',\n",
       "   '2103504761',\n",
       "   '2006500012',\n",
       "   '2139797453',\n",
       "   '2005245503',\n",
       "   '1690240707',\n",
       "   '78443961',\n",
       "   '1588710487',\n",
       "   '2154455356'],\n",
       "  'title': 'Orthogonal Pyramid Transforms For Image Coding.'},\n",
       " {'abstract': 'Much recent effort has sought asymptotically minimax methods for recovering infinite dimensional objects-curves, densities, spectral densities, images-from noisy data. A now rich and complex body of work develops nearly or exactly minimax estimators for an array of interesting problems. Unfortunately, the results have rarely moved into practice, for a variety of reasons-among them being similarity to known methods, computational intractability and lack of spatial adaptivity. We discuss a method for curve estimation based on n noisy data: translate the empirical wavelet coefficients towards the origin by an amount √(2 log n) /√n. The proposal differs from those in current use, is computationally practical and is spatially adaptive; it thus avoids several of the previous objections. Further, the method is nearly minimax both for a wide variety of loss functions-pointwise error, global error measured in L p -norms, pointwise and global error in estimation of derivatives-and for a wide range of smoothness classes, including standard Holder and Sobolev classes, and bounded variation. This is a much broader near optimality than anything previously proposed: we draw loose parallels with near optimality in robustness and also with the broad near eigenfunction properties of wavelets themselves. Finally, the theory underlying the method is interesting, as it exploits a correspondence between statistical questions and questions of optimal recovery and information-based complexity',\n",
       "  'authors': ['David L. Donoho 1',\n",
       "   ' Iain M. Johnstone 1',\n",
       "   ' Gérard Kerkyacharian 2',\n",
       "   ' Dominique Picard 3'],\n",
       "  'date': '1995',\n",
       "  'identifier': '191129667',\n",
       "  'references': ['2146842127',\n",
       "   '2158940042',\n",
       "   '2151693816',\n",
       "   '2079724595',\n",
       "   '2102201073',\n",
       "   '2033484654',\n",
       "   '654435104',\n",
       "   '2092543127',\n",
       "   '2050947749',\n",
       "   '2015930615'],\n",
       "  'title': 'Wavelet Shrinkage: Asymptopia?'},\n",
       " {'abstract': 'We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.',\n",
       "  'authors': ['Bo Pang 1', ' Lillian Lee 1', ' Shivakumar Vaithyanathan 2'],\n",
       "  'date': '2002',\n",
       "  'identifier': '2166706824',\n",
       "  'references': ['2155328222',\n",
       "   '2149684865',\n",
       "   '3021469268',\n",
       "   '2096175520',\n",
       "   '1550206324',\n",
       "   '2140785063',\n",
       "   '2199803028',\n",
       "   '2160842254',\n",
       "   '1924689489',\n",
       "   '202303397'],\n",
       "  'title': 'Thumbs up? Sentiment Classification using Machine Learning Techniques'},\n",
       " {'abstract': 'Abstract This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.',\n",
       "  'authors': ['K. Hornik 1', ' M. Stinchcombe 2', ' H. White 2'],\n",
       "  'date': '1989',\n",
       "  'identifier': '2137983211',\n",
       "  'references': ['1652505363',\n",
       "   '2103496339',\n",
       "   '2056099894',\n",
       "   '2416739038',\n",
       "   '2090270852',\n",
       "   '1654142532',\n",
       "   '1581292930',\n",
       "   '135768573',\n",
       "   '2097415784'],\n",
       "  'title': 'Multilayer feedforward networks are universal approximators'},\n",
       " {'abstract': 'Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?.',\n",
       "  'authors': ['Vladimir N. Vapnik'],\n",
       "  'date': '1995',\n",
       "  'identifier': '2156909104',\n",
       "  'references': ['2140190241',\n",
       "   '2148603752',\n",
       "   '2164278908',\n",
       "   '2310919327',\n",
       "   '2129812935',\n",
       "   '2076063813',\n",
       "   '1746819321',\n",
       "   '1570448133',\n",
       "   '2072128103',\n",
       "   '2139212933'],\n",
       "  'title': 'The Nature of Statistical Learning Theory'},\n",
       " {'abstract': 'Current computational approaches to learning visual object categories require thousands of training images, are slow, cannot learn in an incremental manner and cannot incorporate prior information into the learning process. In addition, no algorithm presented in the literature has been tested on more than a handful of object categories. We present an method for learning object categories from just a few training images. It is quick and it uses prior information in a principled way. We test it on a dataset composed of images of objects belonging to 101 widely varied categories. Our proposed method is based on making use of prior information, assembled from (unrelated) object categories which were previously learnt. A generative probabilistic model is used, which represents the shape and appearance of a constellation of features belonging to the object. The parameters of the model are learnt incrementally in a Bayesian manner. Our incremental algorithm is compared experimentally to an earlier batch Bayesian algorithm, as well as to one based on maximum likelihood. The incremental and batch versions have comparable classification performance on small training sets, but incremental learning is significantly faster, making real-time learning feasible. Both Bayesian methods outperform maximum likelihood on small training sets.',\n",
       "  'authors': ['Li Fei-Fei 1', ' Rob Fergus 2', ' Pietro Perona 3'],\n",
       "  'date': '2007',\n",
       "  'identifier': '2166049352',\n",
       "  'references': ['2164598857',\n",
       "   '2124386111',\n",
       "   '2154422044',\n",
       "   '2124087378',\n",
       "   '2155511848',\n",
       "   '1516111018',\n",
       "   '1949116567',\n",
       "   '1746680969',\n",
       "   '2567948266',\n",
       "   '1699734612'],\n",
       "  'title': 'Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories'},\n",
       " {'abstract': 'Discriminative learning is challenging when examples are sets of features, and the sets vary in cardinality and lack any sort of meaningful ordering. Kernel-based classification methods can learn complex decision boundaries, but a kernel over unordered set inputs must somehow solve for correspondences epsivnerally a computationally expensive task that becomes impractical for large set sizes. We present a new fast kernel function which maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in this space. This \"pyramid match\" computation is linear in the number of features, and it implicitly finds correspondences based on the finest resolution histogram cell where a matched pair first appears. Since the kernel does not penalize the presence of extra features, it is robust to clutter. We show the kernel function is positive-definite, making it valid for use in learning algorithms whose optimal solutions are guaranteed only for Mercer kernels. We demonstrate our algorithm on object recognition tasks and show it to be accurate and dramatically faster than current approaches',\n",
       "  'authors': ['K. Grauman ', ' T. Darrell'],\n",
       "  'date': '2005',\n",
       "  'identifier': '2104978738',\n",
       "  'references': ['2151103935',\n",
       "   '2153635508',\n",
       "   '2148603752',\n",
       "   '2752885492',\n",
       "   '2131846894',\n",
       "   '1563088657',\n",
       "   '2057175746',\n",
       "   '1510073064',\n",
       "   '2145072179',\n",
       "   '2914885528'],\n",
       "  'title': 'The pyramid match kernel: discriminative classification with sets of image features'},\n",
       " {'abstract': 'Motivation Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora. Results We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts. Availability and implementation We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert.',\n",
       "  'authors': ['Jinhyuk Lee 1',\n",
       "   ' Wonjin Yoon 1',\n",
       "   ' Sungdong Kim 2',\n",
       "   ' Donghyeon Kim 1',\n",
       "   ' Sunkyu Kim 1',\n",
       "   ' Chan Ho So 1',\n",
       "   ' Jaewoo Kang 1'],\n",
       "  'date': '2019',\n",
       "  'identifier': '2911489562',\n",
       "  'references': ['2919115771',\n",
       "   '2963403868',\n",
       "   '2963341956',\n",
       "   '2153579005',\n",
       "   '2250539671',\n",
       "   '2962739339',\n",
       "   '2963748441',\n",
       "   '2132339004',\n",
       "   '2525778437',\n",
       "   '2963756346'],\n",
       "  'title': 'BioBERT: a pre-trained biomedical language representation model for biomedical text mining.'},\n",
       " {'abstract': \"Theano is a linear algebra compiler that optimizes a user's symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano's performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurrent neural networks.\",\n",
       "  'authors': ['Frédéric Bastien ',\n",
       "   ' Pascal Lamblin ',\n",
       "   ' Razvan Pascanu ',\n",
       "   ' James Bergstra ',\n",
       "   ' Ian J. Goodfellow ',\n",
       "   ' Arnaud Bergeron ',\n",
       "   ' Nicolas Bouchard ',\n",
       "   ' David Warde-Farley ',\n",
       "   ' Yoshua Bengio'],\n",
       "  'date': '2012',\n",
       "  'identifier': '1606347560',\n",
       "  'references': ['2011301426',\n",
       "   '2154642048',\n",
       "   '753012316',\n",
       "   '2061939373',\n",
       "   '3005347330',\n",
       "   '2110114082',\n",
       "   '1408639475',\n",
       "   '2185726469',\n",
       "   '2254715784',\n",
       "   '2006903949'],\n",
       "  'title': 'Theano: new features and speed improvements'},\n",
       " {'abstract': 'An efficient single-pass adaptive bandwidth compression technique using the discrete cosine transform is described. The coding process involves a simple thresholding and normalization operation on the transform coefficients. Adaptivity is achieved by using a rate buffer for channel rate equalization. The buffer status and input rate are monitored to generate a feedback normalization factor. Excellent results are demonstrated for coding of color images at 0.4 bits/pixel corresponding to real-time color television transmission over a 1.5 Mbit/s channel.',\n",
       "  'authors': ['Wen-Hsiung Chen 1', ' W. Pratt 2'],\n",
       "  'date': '1984',\n",
       "  'identifier': '2005245503',\n",
       "  'references': ['2031614119',\n",
       "   '2105815873',\n",
       "   '3023802253',\n",
       "   '2118274709',\n",
       "   '2154555188',\n",
       "   '2161816980',\n",
       "   '2134809980',\n",
       "   '2142930126',\n",
       "   '2144640755',\n",
       "   '2160050411'],\n",
       "  'title': 'Scene Adaptive Coder'},\n",
       " {'abstract': \"We consider prediction and uncertainty analysis for systems which are approximated using complex mathematical models. Such models, implemented as computer codes, are often generic in the sense that by a suitable choice of some of the model's input parameters the code can be used to predict the behaviour of the system in a variety of specific applications. However, in any specific application the values of necessary parameters may be unknown. In this case, physical observations of the system in the specific context are used to learn about the unknown parameters. The process of fitting the model to the observed data by adjusting the parameters is known as calibration. Calibration is typically effected by ad hoc fitting, and after calibration the model is used, with the fitted input values, to predict the future behaviour of the system. We present a Bayesian calibration technique which improves on this traditional approach in two respects. First, the predictions allow for all sources of uncertainty, including the remaining uncertainty over the fitted parameters. Second, they attempt to correct for any inadequacy of the model which is revealed by a discrepancy between the observed data and the model predictions from even the best‐fitting parameter values. The method is illustrated by using data from a nuclear radiation release at Tomsk, and from a more complex simulated nuclear accident exercise.\",\n",
       "  'authors': ['Marc C. Kennedy ', \" Anthony O'Hagan\"],\n",
       "  'date': '2001',\n",
       "  'identifier': '1973333099',\n",
       "  'references': ['2018044188',\n",
       "   '2038669746',\n",
       "   '2143022286',\n",
       "   '1567512734',\n",
       "   '2027792629',\n",
       "   '2033900415',\n",
       "   '999207820',\n",
       "   '1977046327',\n",
       "   '1585773866',\n",
       "   '2056145269'],\n",
       "  'title': 'Bayesian Calibration of computer models'},\n",
       " {'abstract': 'Abstract: In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.',\n",
       "  'authors': ['Alec Radford 1', ' Luke Metz 1', ' Soumith Chintala 2'],\n",
       "  'date': '2016',\n",
       "  'identifier': '2963684088',\n",
       "  'references': ['2962793481',\n",
       "   '2331128040',\n",
       "   '2963470893',\n",
       "   '2963420272',\n",
       "   '2405756170',\n",
       "   '2963800363',\n",
       "   '2963836885',\n",
       "   '2738588019',\n",
       "   '2962947361'],\n",
       "  'title': 'Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks'},\n",
       " {'abstract': 'This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not recommended (thumbs down). The classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs. A phrase has a positive semantic orientation when it has good associations (e.g., \"subtle nuances\") and a negative semantic orientation when it has bad associations (e.g., \"very cavalier\"). In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word \"excellent\" minus the mutual information between the given phrase and the word \"poor\". A review is classified as recommended if the average semantic orientation of its phrases is positive. The algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations). The accuracy ranges from 84% for automobile reviews to 66% for movie reviews.',\n",
       "  'authors': ['Peter Turney'],\n",
       "  'date': '2002',\n",
       "  'identifier': '2155328222',\n",
       "  'references': ['1983578042',\n",
       "   '2199803028',\n",
       "   '1567365482',\n",
       "   '1493790738',\n",
       "   '1593045043',\n",
       "   '1998442272',\n",
       "   '1565863475',\n",
       "   '1638659076',\n",
       "   '2170381724',\n",
       "   '1997855593'],\n",
       "  'title': 'Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews'},\n",
       " {'abstract': 'The fundamental algorithms in data mining and analysis form the basis for the emerging field of data science, which includes automated methods to analyze patterns and models for all kinds of data, with applications ranging from scientific discovery to business intelligence and analytics. This textbook for senior undergraduate and graduate data mining courses provides a broad yet in-depth overview of data mining, integrating related concepts from machine learning and statistics. The main parts of the book include exploratory data analysis, pattern mining, clustering, and classification. The book lays the basic foundations of these tasks, and also covers cutting-edge topics such as kernel methods, high-dimensional data analysis, and complex graphs and networks. With its comprehensive coverage, algorithmic perspective, and wealth of examples, this book offers solid guidance in data mining for students, researchers, and practitioners alike. Key features: Covers both core methods and cutting-edge research Algorithmic approach with open-source implementations Minimal prerequisites: all key mathematical concepts are presented, as is the intuition behind the formulas Short, self-contained chapters with class-tested examples and exercises allow for flexibility in designing a course and for easy reference Supplementary website with lecture slides, videos, project ideas, and more',\n",
       "  'authors': ['Mohammed J. Zaki 1', ' Wagner Meira 2'],\n",
       "  'date': '2014',\n",
       "  'identifier': '255556494',\n",
       "  'references': ['2911964244',\n",
       "   '1663973292',\n",
       "   '2140190241',\n",
       "   '2148694408',\n",
       "   '2331432542',\n",
       "   '2112090702',\n",
       "   '2008620264',\n",
       "   '1570448133',\n",
       "   '3013264884',\n",
       "   '1565377632'],\n",
       "  'title': 'Data Mining and Analysis: Fundamental Concepts and Algorithms'},\n",
       " {'abstract': 'Abstract:Objective:The specific objective for this research was to determine initial psychometric properties of the Faces Pain Scale (FPS) as a measure of pain intensity for use with the elderly.Design:The study was descriptive correlational in nature, with nonrandom sampling. A total sample of',\n",
       "  'authors': ['Keela A. Herr ',\n",
       "   ' Paula R. Mobily ',\n",
       "   ' Frank J. Kohout ',\n",
       "   ' Diane Wagenaar'],\n",
       "  'date': '1998',\n",
       "  'identifier': '1973031502',\n",
       "  'references': ['1847168837',\n",
       "   '2064475745',\n",
       "   '2030619935',\n",
       "   '1975238145',\n",
       "   '1994143652',\n",
       "   '2249078763',\n",
       "   '1899606784',\n",
       "   '2024501220',\n",
       "   '2024218186',\n",
       "   '2102155358'],\n",
       "  'title': 'Evaluation of the Faces Pain Scale for Use with the Elderly'},\n",
       " {'abstract': 'In this paper we propose a new approach for constructing efficient schemes for non-smooth convex optimization. It is based on a special smoothing technique, which can be applied to functions with explicit max-structure. Our approach can be considered as an alternative to black-box minimization. From the viewpoint of efficiency estimates, we manage to improve the traditional bounds on the number of iterations of the gradient schemes from ** keeping basically the complexity of each iteration unchanged.',\n",
       "  'authors': ['Yu Nesterov'],\n",
       "  'date': '2005',\n",
       "  'identifier': '2167732364',\n",
       "  'references': ['2124541940',\n",
       "   '1568307856',\n",
       "   '1553702074',\n",
       "   '1568288633',\n",
       "   '1669104078',\n",
       "   '2015263936',\n",
       "   '2150126561',\n",
       "   '2969945254',\n",
       "   '2008164266'],\n",
       "  'title': 'Smooth minimization of non-smooth functions'},\n",
       " {'abstract': \"Twitter, a microblogging service less than three years old, commands more than 41 million users as of July 2009 and is growing fast. Twitter users tweet about any topic within the 140-character limit and follow others to receive their tweets. The goal of this paper is to study the topological characteristics of Twitter and its power as a new medium of information sharing. We have crawled the entire Twitter site and obtained 41.7 million user profiles, 1.47 billion social relations, 4,262 trending topics, and 106 million tweets. In its follower-following topology analysis we have found a non-power-law follower distribution, a short effective diameter, and low reciprocity, which all mark a deviation from known characteristics of human social networks [28]. In order to identify influentials on Twitter, we have ranked users by the number of followers and by PageRank and found two rankings to be similar. Ranking by retweets differs from the previous two rankings, indicating a gap in influence inferred from the number of followers and that from the popularity of one's tweets. We have analyzed the tweets of top trending topics and reported on their temporal behavior and user participation. We have classified the trending topics based on the active period and the tweets and show that the majority (over 85%) of topics are headline news or persistent news in nature. A closer look at retweets reveals that any retweeted tweet is to reach an average of 1,000 users no matter what the number of followers is of the original tweet. Once retweeted, a tweet gets retweeted almost instantly on next hops, signifying fast diffusion of information after the 1st retweet. To the best of our knowledge this work is the first quantitative study on the entire Twittersphere and information diffusion on it.\",\n",
       "  'authors': ['Haewoon Kwak ',\n",
       "   ' Changhyun Lee ',\n",
       "   ' Hosung Park ',\n",
       "   ' Sue Moon'],\n",
       "  'date': '2010',\n",
       "  'identifier': '2101196063',\n",
       "  'references': ['2112090702',\n",
       "   '1854214752',\n",
       "   '2402962589',\n",
       "   '2046804949',\n",
       "   '2130354913',\n",
       "   '2076219102',\n",
       "   '1994473607',\n",
       "   '2127492100',\n",
       "   '2134746982',\n",
       "   '2047443612'],\n",
       "  'title': 'What is Twitter, a social network or a news media?'},\n",
       " {'abstract': 'We develop a face recognition algorithm which is insensitive to large variation in lighting direction and facial expression. Taking a pattern classification approach, we consider each pixel in an image as a coordinate in a high-dimensional space. We take advantage of the observation that the images of a particular face, under varying illumination but fixed pose, lie in a 3D linear subspace of the high dimensional image space-if the face is a Lambertian surface without shadowing. However, since faces are not truly Lambertian surfaces and do indeed produce self-shadowing, images will deviate from this linear subspace. Rather than explicitly modeling this deviation, we linearly project the image into a subspace in a manner which discounts those regions of the face with large deviation. Our projection method is based on Fisher\\'s linear discriminant and produces well separated classes in a low-dimensional subspace, even under severe variation in lighting and facial expressions. The eigenface technique, another method based on linearly projecting the image space to a low dimensional subspace, has similar computational requirements. Yet, extensive experimental results demonstrate that the proposed \"Fisherface\" method has error rates that are lower than those of the eigenface technique for tests on the Harvard and Yale face databases.',\n",
       "  'authors': ['P.N. Belhumeur ', ' J.P. Hespanha ', ' D.J. Kriegman'],\n",
       "  'date': '1997',\n",
       "  'identifier': '2121647436',\n",
       "  'references': ['2138451337',\n",
       "   '2098693229',\n",
       "   '2123977795',\n",
       "   '2115689562',\n",
       "   '3017143921',\n",
       "   '2113341759',\n",
       "   '2098947662',\n",
       "   '2740373864',\n",
       "   '2130259898',\n",
       "   '2159173611'],\n",
       "  'title': 'Eigenfaces vs. Fisherfaces: recognition using class specific linear projection'},\n",
       " {'abstract': 'In honor of the twenty-fifth anniversary of Huffman coding, four new results about Huffman codes are presented. The first result shows that a binary prefix condition code is a Huffman code iff the intermediate and terminal nodes in the code tree can be listed by nonincreasing probability so that each node in the list is adjacent to its sibling. The second result upper bounds the redundancy (expected length minus entropy) of a binary Huffman code by P_{1}+ \\\\log_{2}[2(\\\\log_{2}e)/e]=P_{1}+0.086 , where P_{1} is the probability of the most likely source letter. The third result shows that one can always leave a codeword of length two unused and still have a redundancy of at most one. The fourth result is a simple algorithm for adapting a Huffman code to slowly varying esthnates of the source probabilities. In essence, one maintains a running count of uses of each node in the code tree and lists the nodes in order of these counts. Whenever the occurrence of a message increases a node count above the count of the next node in the list, the nodes, with their attached subtrees, are interchanged.',\n",
       "  'authors': ['R. Gallager'],\n",
       "  'date': '1978',\n",
       "  'identifier': '2038649859',\n",
       "  'references': ['2142901448', '1992371956'],\n",
       "  'title': 'Variations on a theme by Huffman'},\n",
       " {'abstract': 'This paper presents a method for recognizing scene categories based on approximate global geometric correspondence. This technique works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The resulting \"spatial pyramid\" is a simple and computationally efficient extension of an orderless bag-of-features image representation, and it shows significantly improved performance on challenging scene categorization tasks. Specifically, our proposed method exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories. The spatial pyramid framework also offers insights into the success of several recently proposed image descriptions, including Torralba\\x92s \"gist\" and Lowe\\x92s SIFT descriptors.',\n",
       "  'authors': ['S. Lazebnik 1', ' C. Schmid 2', ' J. Ponce 3'],\n",
       "  'date': '2006',\n",
       "  'identifier': '2162915993',\n",
       "  'references': ['1880262756',\n",
       "   '2154422044',\n",
       "   '2107034620',\n",
       "   '1566135517',\n",
       "   '2166049352',\n",
       "   '2104978738',\n",
       "   '2914885528',\n",
       "   '2168002178',\n",
       "   '2134731454',\n",
       "   '2165828254'],\n",
       "  'title': 'Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Dimitri Bertsekas'],\n",
       "  'date': '1995',\n",
       "  'identifier': '2798766386',\n",
       "  'references': ['2164278908',\n",
       "   '2146502635',\n",
       "   '3029645440',\n",
       "   '2100556411',\n",
       "   '2067191022',\n",
       "   '1964357740',\n",
       "   '2202343345',\n",
       "   '2109449402',\n",
       "   '1601740268',\n",
       "   '2132870739'],\n",
       "  'title': 'Nonlinear Programming'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Paul Resnick 1', ' Hal R. Varian 2'],\n",
       "  'date': '1997',\n",
       "  'identifier': '2341865734',\n",
       "  'references': ['1966553486'],\n",
       "  'title': 'Recommender systems'},\n",
       " {'abstract': 'This paper proposes a new approach to learning a discriminative model of object classes, incorporating appearance, shape and context information efficiently. The learned model is used for automatic visual recognition and semantic segmentation of photographs. Our discriminative model exploits novel features, based on textons, which jointly model shape and texture. Unary classification and feature selection is achieved using shared boosting to give an efficient classifier which can be applied to a large number of classes. Accurate image segmentation is achieved by incorporating these classifiers in a conditional random field. Efficient training of the model on very large datasets is achieved by exploiting both random feature selection and piecewise training methods. High classification and segmentation accuracy are demonstrated on three different databases: i) our own 21-object class database of photographs of real objects viewed under general lighting conditions, poses and viewpoints, ii) the 7-class Corel subset and iii) the 7-class Sowerby database used in [1]. The proposed algorithm gives competitive results both for highly textured (e.g. grass, trees), highly structured (e.g. cars, faces, bikes, aeroplanes) and articulated objects (e.g. body, cow).',\n",
       "  'authors': ['Jamie Shotton 1',\n",
       "   ' John Winn 2',\n",
       "   ' Carsten Rother 2',\n",
       "   ' Antonio Criminisi 2'],\n",
       "  'date': '2006',\n",
       "  'identifier': '1528789833',\n",
       "  'references': ['2164598857',\n",
       "   '2147880316',\n",
       "   '2057175746',\n",
       "   '2154422044',\n",
       "   '2124351162',\n",
       "   '2024046085',\n",
       "   '2169551590',\n",
       "   '1666447063',\n",
       "   '2168002178',\n",
       "   '1484228140'],\n",
       "  'title': 'TextonBoost : joint appearance, shape and context modeling for multi-class object recognition and segmentation'},\n",
       " {'abstract': 'This article described three heuristics that are employed in making judgements under uncertainty: (i) representativeness, which is usually employed when people are asked to judge the probability that an object or event A belongs to class or process B; (ii) availability of instances or scenarios, which is often employed when people are asked to assess the frequency of a class or the plausibility of a particular development; and (iii) adjustment from an anchor, which is usually employed in numerical prediction when a relevant value is available. These heuristics are highly economical and usually effective, but they lead to systematic and predictable errors. A better understanding of these heuristics and of the biases to which they lead could improve judgements and decisions in situations of uncertainty.',\n",
       "  'authors': ['A. Tversky ', ' D. Kahneman'],\n",
       "  'date': '1974',\n",
       "  'identifier': '158727920',\n",
       "  'references': ['2035782089',\n",
       "   '1980054641',\n",
       "   '2016377072',\n",
       "   '1976624377',\n",
       "   '2079199322',\n",
       "   '1965761421',\n",
       "   '2018507693',\n",
       "   '1972320590',\n",
       "   '1965740984',\n",
       "   '2035863199'],\n",
       "  'title': 'Judgment Under Uncertainty: Heuristics and Biases'},\n",
       " {'abstract': 'Donoho and Johnstone (1994) proposed a method for reconstructing an unknown function f on [0,1] from noisy data d/sub i/=f(t/sub i/)+/spl sigma/z/sub i/, i=0, ..., n-1,t/sub i/=i/n, where the z/sub i/ are independent and identically distributed standard Gaussian random variables. The reconstruction f/spl circ/*/sub n/ is defined in the wavelet domain by translating all the empirical wavelet coefficients of d toward 0 by an amount /spl sigma//spl middot//spl radic/(2log (n)/n). The authors prove two results about this type of estimator. [Smooth]: with high probability f/spl circ/*/sub n/ is at least as smooth as f, in any of a wide variety of smoothness measures. [Adapt]: the estimator comes nearly as close in mean square to f as any measurable estimator can come, uniformly over balls in each of two broad scales of smoothness classes. These two properties are unprecedented in several ways. The present proof of these results develops new facts about abstract statistical inference and its connection with an optimal recovery model. >',\n",
       "  'authors': ['D.L. Donoho'],\n",
       "  'date': '1995',\n",
       "  'identifier': '2146842127',\n",
       "  'references': ['2098914003',\n",
       "   '2079724595',\n",
       "   '2152328854',\n",
       "   '3005363104',\n",
       "   '191129667',\n",
       "   '2107790757',\n",
       "   '2033484654',\n",
       "   '654435104',\n",
       "   '2092543127',\n",
       "   '2050880896'],\n",
       "  'title': 'De-noising by soft-thresholding'},\n",
       " {'abstract': 'K.M. Hornik, M. Stinchcombe, and H. White (Univ. of California at San Diego, Dept. of Economics Discussion Paper, June 1988; to appear in Neural Networks) showed that multilayer feedforward networks with as few as one hidden layer, no squashing at the output layer, and arbitrary sigmoid activation function at the hidden layer are universal approximators: they are capable of arbitrarily accurate approximation to arbitrary mappings, provided sufficiently many hidden units are available. The present authors obtain identical conclusions but do not require the hidden-unit activation to be sigmoid. Instead, it can be a rather general nonlinear function. Thus, multilayer feedforward networks possess universal approximation capabilities by virtue of the presence of intermediate layers with sufficiently many parallel processors; the properties of the intermediate-layer activation function are not so crucial. In particular, sigmoid activation functions are not necessary for universal approximation. >',\n",
       "  'authors': ['Stinchcombe ', ' White'],\n",
       "  'date': '1989',\n",
       "  'identifier': '2090270852',\n",
       "  'references': ['2137983211', '1654142532'],\n",
       "  'title': 'Universal approximation using feedforward networks with non-sigmoid hidden layer activation functions'},\n",
       " {'abstract': 'We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 [1] was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) artifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, then benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching [2] algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions.',\n",
       "  'authors': ['Gregory Griffin ', ' Alex Holub ', ' Pietro Perona'],\n",
       "  'date': '2007',\n",
       "  'identifier': '1576445103',\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2117539524',\n",
       "   '2108598243',\n",
       "   '1861492603',\n",
       "   '2031489346',\n",
       "   '2963173190',\n",
       "   '2295107390'],\n",
       "  'title': 'Caltech-256 Object Category Dataset'},\n",
       " {'abstract': 'Abstract: We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.',\n",
       "  'authors': ['Pierre Sermanet ',\n",
       "   ' David Eigen ',\n",
       "   ' Xiang Zhang ',\n",
       "   ' Michael Mathieu ',\n",
       "   ' Rob Fergus ',\n",
       "   ' Yann LeCun'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2963542991',\n",
       "  'references': ['2194775991',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '639708223',\n",
       "   '1903029394',\n",
       "   '2155893237'],\n",
       "  'title': 'OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Kunihiko Fukushima ', ' Sei Miyake ', ' Takayuki Ito'],\n",
       "  'date': '1988',\n",
       "  'identifier': '2256679588',\n",
       "  'references': ['2153158097',\n",
       "   '2149659980',\n",
       "   '2023795513',\n",
       "   '2142075376',\n",
       "   '2785684543',\n",
       "   '2558231299',\n",
       "   '2019258010',\n",
       "   '2101785709',\n",
       "   '2071365591',\n",
       "   '2125311303'],\n",
       "  'title': 'Neocognitron: a neural network model for a mechanism of visual pattern recognition'},\n",
       " {'abstract': 'An optimum method of coding an ensemble of messages consisting of a finite number of members is developed. A minimum-redundancy code is one constructed in such a way that the average number of coding digits per message is minimized.',\n",
       "  'authors': ['David A. Huffman'],\n",
       "  'date': '2006',\n",
       "  'identifier': '1992371956',\n",
       "  'references': ['1995875735', '2752870707', '1509016759'],\n",
       "  'title': 'A method for the construction of minimum-redundancy codes'},\n",
       " {'abstract': '',\n",
       "  'authors': ['David M. Blei'],\n",
       "  'date': '2012',\n",
       "  'identifier': '2174706414',\n",
       "  'references': ['1880262756',\n",
       "   '2098126593',\n",
       "   '2107034620',\n",
       "   '2147152072',\n",
       "   '2158266063',\n",
       "   '2120340025',\n",
       "   '1516111018',\n",
       "   '2072644219',\n",
       "   '1612003148',\n",
       "   '2098062695'],\n",
       "  'title': 'Probabilistic topic models'},\n",
       " {'abstract': 'This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the \"integral image\" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a \"cascade\" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.',\n",
       "  'authors': ['P. Viola 1', ' M. Jones 2'],\n",
       "  'date': '2001',\n",
       "  'identifier': '2164598857',\n",
       "  'references': ['1988790447',\n",
       "   '2128272608',\n",
       "   '2217896605',\n",
       "   '2115763357',\n",
       "   '1975846642',\n",
       "   '2124351082',\n",
       "   '2159686933',\n",
       "   '2155511848',\n",
       "   '2101522199',\n",
       "   '1588351438'],\n",
       "  'title': 'Rapid object detection using a boosted cascade of simple features'},\n",
       " {'abstract': \"Collaborative filtering (CF) is valuable in e-commerce, and for direct recommendations for music, movies, news etc. But today's systems have several disadvantages, including privacy risks. As we move toward ubiquitous computing, there is a great potential for individuals to share all kinds of information about places and things to do, see and buy, but the privacy risks are severe. In this paper we describe a new method for collaborative filtering which protects the privacy of individual data. The method is based on a probabilistic factor analysis model. Privacy protection is provided by a peer-to-peer protocol which is described elsewhere, but outlined in this paper. The factor analysis approach handles missing data without requiring default values for them. We give several experiments that suggest that this is most accurate method for CF to date. The new algorithm has other advantages in speed and storage over previous algorithms. Finally, we suggest applications of the approach to other kinds of statistical analyses of survey or questionaire data.\",\n",
       "  'authors': ['John Canny'],\n",
       "  'date': '2002',\n",
       "  'identifier': '2070786785',\n",
       "  'references': ['2110325612',\n",
       "   '2104210894',\n",
       "   '2049633694',\n",
       "   '2085937320',\n",
       "   '1832221731',\n",
       "   '2124029832',\n",
       "   '1532852017',\n",
       "   '2097129520',\n",
       "   '1510348757',\n",
       "   '2109992782'],\n",
       "  'title': 'Collaborative filtering with privacy via factor analysis'},\n",
       " {'abstract': 'This paper presents a general trainable framework for object detection in static images of cluttered scenes. The detection technique we develop is based on a wavelet representation of an object class derived from a statistical analysis of the class instances. By learning an object class in terms of a subset of an overcomplete dictionary of wavelet basis functions, we derive a compact representation of an object class which is used as an input to a support vector machine classifier. This representation overcomes both the problem of in-class variability and provides a low false detection rate in unconstrained environments. We demonstrate the capabilities of the technique in two domains whose inherent information content differs significantly. The first system is face detection and the second is the domain of people which, in contrast to faces, vary greatly in color, texture, and patterns. Unlike previous approaches, this system learns from examples and does not rely on any a priori (hand-crafted) models or motion-based segmentation. The paper also presents a motion-based extension to enhance the performance of the detection algorithm over video sequences. The results presented here suggest that this architecture may well be quite general.',\n",
       "  'authors': ['C.P. Papageorgiou ', ' M. Oren ', ' T. Poggio'],\n",
       "  'date': '1998',\n",
       "  'identifier': '2115763357',\n",
       "  'references': ['2132984323',\n",
       "   '2087347434',\n",
       "   '2124351082',\n",
       "   '2104671481',\n",
       "   '3113292254',\n",
       "   '2159173611',\n",
       "   '2137346077',\n",
       "   '2056695679',\n",
       "   '1676612073',\n",
       "   '2030989822'],\n",
       "  'title': 'A general framework for object detection'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Gerard Salton ', ' Michael J. McGill'],\n",
       "  'date': '1983',\n",
       "  'identifier': '1956559956',\n",
       "  'references': ['1880262756',\n",
       "   '2140190241',\n",
       "   '2031489346',\n",
       "   '2037227137',\n",
       "   '1902027874',\n",
       "   '2031454541',\n",
       "   '2110325612',\n",
       "   '2147152072',\n",
       "   '2158266063',\n",
       "   '2107743791'],\n",
       "  'title': 'Introduction to Modern Information Retrieval'},\n",
       " {'abstract': \"Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.\",\n",
       "  'authors': ['William L. Hamilton ', ' Rex Ying ', ' Jure Leskovec'],\n",
       "  'date': '2017',\n",
       "  'identifier': '2624431344',\n",
       "  'references': ['2964121744',\n",
       "   '2153579005',\n",
       "   '2101234009',\n",
       "   '2112090702',\n",
       "   '2064675550',\n",
       "   '2302255633',\n",
       "   '2130410032',\n",
       "   '2271840356',\n",
       "   '2962756421',\n",
       "   '2560609797'],\n",
       "  'title': 'Inductive Representation Learning on Large Graphs'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Jia Li 1',\n",
       "   ' Honglei Zhang 2',\n",
       "   ' Zhichao Han 1',\n",
       "   ' Yu Rong 3',\n",
       "   ' Hong Cheng 1',\n",
       "   ' Junzhou Huang 3'],\n",
       "  'date': '2020',\n",
       "  'identifier': '3101871847',\n",
       "  'references': ['1959608418',\n",
       "   '2962756421',\n",
       "   '2964015378',\n",
       "   '3104097132',\n",
       "   '2963857521',\n",
       "   '2244663144',\n",
       "   '2164998314',\n",
       "   '2110620844',\n",
       "   '2089554624',\n",
       "   '2165515835'],\n",
       "  'title': 'Adversarial Attack on Community Detection by Hiding Individuals.'},\n",
       " {'abstract': 'This paper addresses the problem of retrieving images from large image databases. The method is based on local grayvalue invariants which are computed at automatically detected interest points. A voting algorithm and semilocal constraints make retrieval possible. Indexing allows for efficient retrieval from a database of more than 1,000 images. Experimental results show correct retrieval in the case of partial visibility, similarity transformations, extraneous features, and small perspective deformations.',\n",
       "  'authors': ['C. Schmid 1', ' R. Mohr 2'],\n",
       "  'date': '1997',\n",
       "  'identifier': '2124087378',\n",
       "  'references': ['2914885528',\n",
       "   '2111308925',\n",
       "   '2098693229',\n",
       "   '2123977795',\n",
       "   '2095757522',\n",
       "   '2011891945',\n",
       "   '2109863423',\n",
       "   '2112328181',\n",
       "   '2022735534',\n",
       "   '3111324825'],\n",
       "  'title': 'Local grayvalue invariants for image retrieval'},\n",
       " {'abstract': 'The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.',\n",
       "  'authors': ['Adam L. Berger 1',\n",
       "   ' Vincent J. Della Pietra 2',\n",
       "   ' Stephen A. Della Pietra 2'],\n",
       "  'date': '1996',\n",
       "  'identifier': '2096175520',\n",
       "  'references': ['2099111195',\n",
       "   '2049633694',\n",
       "   '2006969979',\n",
       "   '2121227244',\n",
       "   '2160842254',\n",
       "   '2097333193',\n",
       "   '1597533204',\n",
       "   '2099345940',\n",
       "   '2167434254',\n",
       "   '1976241232'],\n",
       "  'title': 'A maximum entropy approach to natural language processing'},\n",
       " {'abstract': 'This paper describes the PASCAL Network of Excellence first Recognising Textual Entailment (RTE-1) Challenge benchmark 1 . The RTE task is defined as recognizing, given two text fragments, whether the meaning of one text can be inferred (entailed) from the other. This application-independent task is suggested as capturing major inferences about the variability of semantic expression which are commonly needed across multiple applications. The Challenge has raised noticeable attention in the research community, attracting 17 submissions from diverse groups, suggesting the generic relevance of the task.',\n",
       "  'authors': ['Ido Dagan ', ' Oren Glickman ', ' Bernardo Magnini'],\n",
       "  'date': '2006',\n",
       "  'identifier': '2525127255',\n",
       "  'references': ['1840435438',\n",
       "   '2965373594',\n",
       "   '1970381522',\n",
       "   '2923014074',\n",
       "   '2185175083',\n",
       "   '2963846996',\n",
       "   '2125436846',\n",
       "   '2996428491'],\n",
       "  'title': 'The PASCAL Recognising Textual Entailment Challenge'},\n",
       " {'abstract': 'We consider the problem of using a large unlabeled sample to boost performance of a learning algorit,hrn when only a small set of labeled examples is available. In particular, we consider a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views. For example, the description of a web page can be partitioned into the words occurring on that page, and the words occurring in hyperlinks t,hat point to that page. We assume that either view of the example would be sufficient for learning if we had enough labeled data, but our goal is to use both views together to allow inexpensive unlabeled data to augment, a much smaller set of labeled examples. Specifically, the presence of two distinct views of each example suggests strategies in which two learning algorithms are trained separately on each view, and then each algorithm’s predictions on new unlabeled examples are used to enlarge the training set of the other. Our goal in this paper is to provide a PAC-style analysis for this setting, and, more broadly, a PAC-style framework for the general problem of learning from both labeled and unlabeled data. We also provide empirical results on real web-page data indicating that this use of unlabeled examples can lead to significant improvement of hypotheses in practice. *This research was supported in part by the DARPA HPKB program under contract F30602-97-1-0215 and by NSF National Young investigator grant CCR-9357793. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. TO copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. COLT 98 Madison WI USA Copyright ACM 1998 l-58113-057--0/98/ 7...%5.00 92 Tom Mitchell School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-3891 mitchell+@cs.cmu.edu',\n",
       "  'authors': ['Avrim Blum ', ' Tom Mitchell'],\n",
       "  'date': '1998',\n",
       "  'identifier': '2048679005',\n",
       "  'references': ['2049633694',\n",
       "   '3017143921',\n",
       "   '2101210369',\n",
       "   '2167044614',\n",
       "   '2128221272',\n",
       "   '1995897489',\n",
       "   '2103555337',\n",
       "   '2150516767',\n",
       "   '2020764470',\n",
       "   '1550302919'],\n",
       "  'title': 'Combining labeled and unlabeled data with co-training'},\n",
       " {'abstract': \"Statistical methods useful in automatic recognition of continuous speech are described. They concern modeling of a speaker and of an acoustic processor, extraction of the models' statistical parameters and hypothesis search procedures and likelihood computations of linguistic decoding. Experimental results are presented that indicate the power of the methods.\",\n",
       "  'authors': ['F. Jelinek'],\n",
       "  'date': '1976',\n",
       "  'identifier': '1990005915',\n",
       "  'references': ['2142384583',\n",
       "   '1562979145',\n",
       "   '2142901448',\n",
       "   '2045407304',\n",
       "   '1991133427',\n",
       "   '1969483458',\n",
       "   '2086699924',\n",
       "   '2007321142',\n",
       "   '2157477135',\n",
       "   '2134587001'],\n",
       "  'title': 'Continuous speech recognition by statistical methods'},\n",
       " {'abstract': \"An approach to the detection and identification of human faces is presented, and a working, near-real-time face recognition system which tracks a subject's head and then recognizes the person by comparing characteristics of the face to those of known individuals is described. This approach treats face recognition as a two-dimensional recognition problem, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. Face images are projected onto a feature space ('face space') that best encodes the variation among known face images. The face space is defined by the 'eigenfaces', which are the eigenvectors of the set of faces; they do not necessarily correspond to isolated features such as eyes, ears, and noses. The framework provides the ability to learn to recognize new faces in an unsupervised manner. >\",\n",
       "  'authors': ['M.A. Turk ', ' A.P. Pentland'],\n",
       "  'date': '1991',\n",
       "  'identifier': '2098693229',\n",
       "  'references': ['2138451337',\n",
       "   '2130259898',\n",
       "   '2125848778',\n",
       "   '2055712799',\n",
       "   '2125999363',\n",
       "   '1507699566',\n",
       "   '1998186877',\n",
       "   '2169718527'],\n",
       "  'title': 'Face recognition using eigenfaces'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Pranav Rajpurkar ',\n",
       "   ' Jian Zhang ',\n",
       "   ' Konstantin Lopyrev ',\n",
       "   ' Percy Liang'],\n",
       "  'date': '2016',\n",
       "  'identifier': '2963748441',\n",
       "  'references': ['2108598243',\n",
       "   '1544827683',\n",
       "   '1632114991',\n",
       "   '2125436846',\n",
       "   '2964267515',\n",
       "   '2962809918',\n",
       "   '2171278097',\n",
       "   '2962790689',\n",
       "   '2251818205',\n",
       "   '2251349042'],\n",
       "  'title': 'SQuAD: 100,000+ Questions for Machine Comprehension of Text'},\n",
       " {'abstract': 'Part 1 Inference: introduction to inference for Bayesian networks, Robert Cowell advanced inference in Bayesian networks, Robert Cowell inference in Bayesian networks using nested junction trees, Uffe Kjoerulff bucket elimination - a unifying framework for probabilistic inference, R. Dechter an introduction to variational methods for graphical models, Michael I. Jordan et al improving the mean field approximation via the use of mixture distributions, Tommi S. Jaakkola and Michael I. Jordan introduction to Monte Carlo methods, D.J.C. MacKay suppressing random walls in Markov chain Monte Carlo using ordered overrelaxation, Radford M. Neal. Part 2 Independence: chain graphs and symmetric associations, Thomas S. Richardson the multiinformation function as a tool for measuring stochastic dependence, M. Studeny and J. Vejnarova. Part 3 Foundations for learning: a tutorial on learning with Bayesian networks, David Heckerman a view of the EM algorithm that justifies incremental, sparse and other variants, Radford M. Neal and Geoffrey E. Hinton. Part 4 Learning from data: latent variable models, Christopher M. Bishop stochastic algorithms for exploratory data analysis - data clustering and data visualization, Joachim M. Buhmann learning Bayesian networks with local structure, Nir Friedman and Moises Goldszmidt asymptotic model selection for directed networks with hidden variables, Dan Geiger et al a hierarchical community of experts, Geoffrey E. Hinton et al an information-theoretic analysis of hard and soft assignment methods for clustering, Michael J. Kearns et al learning hybrid Bayesian networks from data, Stefano Monti and Gregory F. Cooper a mean field learning algorithm for unsupervised neural networks, Lawrence Saul and Michael Jordan edge exclusion tests for graphical Gaussian models, Peter W.F. Smith and Joe Whittaker hepatitis B - a case study in MCMC, D.J. Spiegelhalter et al prediction with Gaussian processes - from linear regression to linear prediction and beyond, C.K.I. Williams.',\n",
       "  'authors': ['Michael I. Jordan'],\n",
       "  'date': '1999',\n",
       "  'identifier': '1746680969',\n",
       "  'references': ['1880262756',\n",
       "   '2072128103',\n",
       "   '2116064496',\n",
       "   '2166049352',\n",
       "   '2166851633',\n",
       "   '2097089247',\n",
       "   '1755360231',\n",
       "   '1873332500'],\n",
       "  'title': 'Learning in graphical models'},\n",
       " {'abstract': 'Shape indexing is a way of making rapid associations between features detected in an image and object models that could have produced them. When model databases are large, the use of high-dimensional features is critical, due to the improved level of discrimination they can provide. Unfortunately, finding the nearest neighbour to a query point rapidly becomes inefficient as the dimensionality of the feature space increases. Past indexing methods have used hash tables for hypothesis recovery, but only in low-dimensional situations. In this paper we show that a new variant of the k-d tree search algorithm makes indexing in higher-dimensional spaces practical. This Best Bin First, or BBF search is an approximate algorithm which finds the nearest neighbour for a large fraction of the queries, and a very close neighbour in the remaining cases. The technique has been integrated into a fully developed recognition system, which is able to detect complex objects in real, cluttered scenes in just a few seconds.',\n",
       "  'authors': ['J.S. Beis ', ' D.G. Lowe'],\n",
       "  'date': '1997',\n",
       "  'identifier': '2096077837',\n",
       "  'references': ['2154204736',\n",
       "   '2033834476',\n",
       "   '2024668293',\n",
       "   '1513966746',\n",
       "   '2154186675',\n",
       "   '2090398718',\n",
       "   '2998097659',\n",
       "   '2159709546',\n",
       "   '3111322113',\n",
       "   '3111246455'],\n",
       "  'title': 'Shape indexing using approximate nearest-neighbour search in high-dimensional spaces'},\n",
       " {'abstract': 'Statistical Problems. Applications of Finite Mixture Models. Mathematical Aspects of Mixtures. Learning About the Parameters of a Mixture. Learning About the Components of a Mixture. Sequential Problems and Procedures.',\n",
       "  'authors': ['D. M. Titterington ', ' Adrian F. M. Smith ', ' U. E. Makov'],\n",
       "  'date': '1986',\n",
       "  'identifier': '2166698530',\n",
       "  'references': ['1992419399',\n",
       "   '2132549764',\n",
       "   '1479807131',\n",
       "   '2156267802',\n",
       "   '1966385142',\n",
       "   '2144898279',\n",
       "   '830076066',\n",
       "   '2110575115',\n",
       "   '2093390569'],\n",
       "  'title': 'Statistical analysis of finite mixture distributions'},\n",
       " {'abstract': 'Let S be a given subset of binary n-sequences. We provide an explicit scheme for calculating the index of any sequence in S according to its position in the lexicographic ordering of S . A simple inverse algorithm is also given. Particularly nice formulas arise when S is the set of all n -sequences of weight k and also when S is the set of all sequences having a given empirical Markov property. Schalkwijk and Lynch have investigated the former case. The envisioned use of this indexing scheme is to transmit or store the index rather than the sequence, thus resulting in a data compression of (\\\\log\\\\midS\\\\mid)/n .',\n",
       "  'authors': ['T. Cover'],\n",
       "  'date': '1973',\n",
       "  'identifier': '2034323860',\n",
       "  'references': ['2163294786',\n",
       "   '1673464365',\n",
       "   '2130956967',\n",
       "   '2110266514',\n",
       "   '1579764623',\n",
       "   '1949346071',\n",
       "   '2163389339',\n",
       "   '2161628678',\n",
       "   '2910758021'],\n",
       "  'title': 'Enumerative source encoding'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Rebecca J. Passonneau'],\n",
       "  'date': '2004',\n",
       "  'identifier': '1985463960',\n",
       "  'references': ['3021916629',\n",
       "   '2153804780',\n",
       "   '2102065370',\n",
       "   '1965693266',\n",
       "   '1669912781',\n",
       "   '2396571892',\n",
       "   '1967545963',\n",
       "   '2128530885',\n",
       "   '121091192',\n",
       "   '1481038076'],\n",
       "  'title': 'Computing Reliability for Coreference Annotation'},\n",
       " {'abstract': 'A recognition scheme that scales efficiently to a large number of objects is presented. The efficiency and quality is exhibited in a live demonstration that recognizes CD-covers from a database of 40000 images of popular music CD\\x92s. The scheme builds upon popular techniques of indexing descriptors extracted from local regions, and is robust to background clutter and occlusion. The local region descriptors are hierarchically quantized in a vocabulary tree. The vocabulary tree allows a larger and more discriminatory vocabulary to be used efficiently, which we show experimentally leads to a dramatic improvement in retrieval quality. The most significant property of the scheme is that the tree directly defines the quantization. The quantization and the indexing are therefore fully integrated, essentially being one and the same. The recognition quality is evaluated through retrieval on a database with ground truth, showing the power of the vocabulary tree approach, going as high as 1 million images.',\n",
       "  'authors': ['D. Nister ', ' H. Stewenius'],\n",
       "  'date': '2006',\n",
       "  'identifier': '2128017662',\n",
       "  'references': ['2151103935',\n",
       "   '2177274842',\n",
       "   '2131846894',\n",
       "   '1980911747',\n",
       "   '2104978738',\n",
       "   '2172188317',\n",
       "   '2124404372',\n",
       "   '2147717514',\n",
       "   '2162006472',\n",
       "   '2165497495'],\n",
       "  'title': 'Scalable Recognition with a Vocabulary Tree'},\n",
       " {'abstract': \"Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package--PMTK (probabilistic modeling toolkit)--that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.\",\n",
       "  'authors': ['Kevin P. Murphy'],\n",
       "  'date': '2012',\n",
       "  'identifier': '1503398984',\n",
       "  'references': ['2526781987',\n",
       "   '2180612164',\n",
       "   '2166851633',\n",
       "   '607505555',\n",
       "   '2951446714',\n",
       "   '2150341604',\n",
       "   '2167839676',\n",
       "   '2337093093'],\n",
       "  'title': 'Machine Learning : A Probabilistic Perspective'},\n",
       " {'abstract': 'An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT’15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. 1',\n",
       "  'authors': ['Minh-Thang Luong ', ' Hieu Pham ', ' Christopher D. Manning'],\n",
       "  'date': '2015',\n",
       "  'identifier': '1902237438',\n",
       "  'references': ['2964308564',\n",
       "   '2130942839',\n",
       "   '2157331557',\n",
       "   '2101105183',\n",
       "   '1514535095',\n",
       "   '1753482797',\n",
       "   '2153653739',\n",
       "   '2100664567',\n",
       "   '2962741254',\n",
       "   '2147527908'],\n",
       "  'title': 'Effective Approaches to Attention-based Neural Machine Translation'},\n",
       " {'abstract': 'This note describes a scoring scheme for the coreference task in MUC6. It improves on the original approach by: (1) grounding the scoring scheme in terms of a model; (2) producing more intuitive recall and precision scores; and (3) not requiring explicit computation of the transitive closure of coreference. The principal conceptual difference is that we have moved from a syntactic scoring model based on following coreference links to an approach defined by the model theory of those links.',\n",
       "  'authors': ['Marc Vilain ',\n",
       "   ' John Burger ',\n",
       "   ' John Aberdeen ',\n",
       "   ' Dennis Connolly ',\n",
       "   ' Lynette Hirschman'],\n",
       "  'date': '1995',\n",
       "  'identifier': '1965693266',\n",
       "  'references': ['2147196093',\n",
       "   '158057341',\n",
       "   '2141766660',\n",
       "   '2147218300',\n",
       "   '2149956050',\n",
       "   '2140676672',\n",
       "   '2963167649',\n",
       "   '2155069789',\n",
       "   '2251035762'],\n",
       "  'title': 'A model-theoretic coreference scoring scheme'},\n",
       " {'abstract': 'The Tapestry experimental mail system developed at the Xerox Palo Alto Research Center is predicated on the belief that information filtering can be more effective when humans are involved in the filtering process. Tapestry was designed to support both content-based filtering and collaborative filtering, which entails people collaborating to help each other perform filtering by recording their reactions to documents they read. The reactions are called annotations; they can be accessed by other people’s filters. Tapestry is intended to handle any incoming stream of electronic documents and serves both as a mail filter and repository; its components are the indexer, document store, annotation store, filterer, little box, remailer, appraiser and reader/browser. Tapestry’s client/server architecture, its various components, and the Tapestry query language are described.',\n",
       "  'authors': ['David Goldberg 1',\n",
       "   ' David Nichols 1',\n",
       "   ' Brian M. Oki 2',\n",
       "   ' Douglas Terry 1'],\n",
       "  'date': '1992',\n",
       "  'identifier': '1966553486',\n",
       "  'references': ['2002110561',\n",
       "   '2037717074',\n",
       "   '2019580127',\n",
       "   '2006299591',\n",
       "   '2066636971',\n",
       "   '1976379365',\n",
       "   '1516049114',\n",
       "   '2053772216',\n",
       "   '1568230512'],\n",
       "  'title': 'Using collaborative filtering to weave an information tapestry'},\n",
       " {'abstract': 'We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.',\n",
       "  'authors': ['Ronan Collobert ', ' Jason Weston'],\n",
       "  'date': '2008',\n",
       "  'identifier': '2117130368',\n",
       "  'references': ['2310919327',\n",
       "   '2132339004',\n",
       "   '2158847908',\n",
       "   '2130903752',\n",
       "   '2107008379',\n",
       "   '2914746235',\n",
       "   '2173629880',\n",
       "   '2885050925',\n",
       "   '2158823144',\n",
       "   '2163568299'],\n",
       "  'title': 'A unified architecture for natural language processing: deep neural networks with multitask learning'},\n",
       " {'abstract': 'We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large datasets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of datasets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the datasets.',\n",
       "  'authors': ['Laurens van der Maaten ', ' Geoffrey Hinton'],\n",
       "  'date': '2008',\n",
       "  'identifier': '2187089797',\n",
       "  'references': ['2100495367',\n",
       "   '2072128103',\n",
       "   '2053186076',\n",
       "   '2001141328',\n",
       "   '2156718197',\n",
       "   '2125637308',\n",
       "   '2139823104',\n",
       "   '2137570937',\n",
       "   '2157444450',\n",
       "   '1742512077'],\n",
       "  'title': 'Visualizing Data using t-SNE'},\n",
       " {'abstract': 'About 1% of the people in the world suffer from epilepsy and 30% of epileptics are not helped by medication. Careful analyses of the electroencephalograph (EEG) records can provide valuable insight and improved understanding of the mechanisms causing epileptic disorders. Wavelet transform is particularly effective for representing various aspects of non-stationary signals such as trends, discontinuities, and repeated patterns where other signal processing approaches fail or are not as effective. In this research, discrete Daubechies and harmonic wavelets are investigated for analysis of epileptic EEG records. Wavelet transform is used to analyze and characterize epileptiform discharges in the form of 3-Hz spike and wave complex in patients with absence seizure. Through wavelet decomposition of the EEG records, transient features are accurately captured and localized in both time and frequency context. The capability of this mathematical microscope to analyze different scales of neural rhythms is shown to be a powerful tool for investigating small-scale oscillations of the brain signals. Wavelet analyses of EEGs obtained from a population of patients can potentially suggest the physiological processes undergoing in the brain in epilepsy onset. A better understanding of the dynamics of the human brain through EEG analysis can be obtained through further analysis of such EEG records.',\n",
       "  'authors': ['Hojjat Adeli ', ' Ziqin Zhou ', ' Nahid Dadmehr'],\n",
       "  'date': '2003',\n",
       "  'identifier': '2119234283',\n",
       "  'references': ['2331432542',\n",
       "   '2062024414',\n",
       "   '2132984323',\n",
       "   '2098914003',\n",
       "   '1658679052',\n",
       "   '1489213177',\n",
       "   '1512748702',\n",
       "   '2096684483',\n",
       "   '1499506103',\n",
       "   '2153134014'],\n",
       "  'title': 'Analysis of EEG records in an epileptic patient using wavelet transform.'},\n",
       " {'abstract': 'Krippendorff views content analysis (one of the most important techniques in communication research) in historical perspective, in contrast to other techniques and in terms of what it can and cannot do.',\n",
       "  'authors': ['Klaus Krippendorff'],\n",
       "  'date': '1980',\n",
       "  'identifier': '3021916629',\n",
       "  'references': ['2142225512',\n",
       "   '2284386481',\n",
       "   '2153377606',\n",
       "   '2498731006',\n",
       "   '1912037221',\n",
       "   '2118033245',\n",
       "   '2783275777',\n",
       "   '1533168346',\n",
       "   '2046550787',\n",
       "   '2918215371'],\n",
       "  'title': 'Content analysis : an introduction to its methodology'},\n",
       " {'abstract': 'Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis.',\n",
       "  'authors': ['Richard O. Duda ', ' Peter E. Hart'],\n",
       "  'date': '1973',\n",
       "  'identifier': '3017143921',\n",
       "  'references': ['2310919327',\n",
       "   '1746819321',\n",
       "   '1570448133',\n",
       "   '2163352848',\n",
       "   '2067191022',\n",
       "   '2110158442',\n",
       "   '1992419399',\n",
       "   '2117812871',\n",
       "   '2121647436'],\n",
       "  'title': 'Pattern classification and scene analysis'},\n",
       " {'abstract': 'In this paper, we propose a new self-supervised learning method for competitive learning as well as self-organizing maps. In this model, a network enhances its state by itself, and this enhanced state is to be imitated by another state of the network. We set up an enhanced and a relaxed state, and the relaxed state tries to imitate the enhanced state as much as possible by minimizing the free energy. To demonstrate the effectiveness of this method, we apply information enhancement learning to the SOM. For this purpose, we introduce collective-ness, in which all neurons collectively respond to input patterns, into an enhanced state. Then, this enhanced and collective state should be imitated by the other non-enhanced and relaxed state. We applied the method to an artificial data and three data from the well-known machine learning database. Experimental results showed that the U-matrices obtained were significantly similar to those produced by the conventional SOM. However, better performance could be obtained in terms of quantitative and topological errors. The experimental results suggest the possibility for self-supervised learning to be applied to many different neural network models.',\n",
       "  'authors': ['Ryotaro Kamimura'],\n",
       "  'date': '2009',\n",
       "  'identifier': '2125311303',\n",
       "  'references': ['1554663460',\n",
       "   '1679913846',\n",
       "   '1479807131',\n",
       "   '2136504847',\n",
       "   '2046079134',\n",
       "   '1991848143',\n",
       "   '2110802877',\n",
       "   '2166322089',\n",
       "   '23758216',\n",
       "   '2116801843'],\n",
       "  'title': 'Self-supervised learning by information enhancement: Target-generating and spontaneous learning for competitive learning'},\n",
       " {'abstract': 'Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.',\n",
       "  'authors': ['Sam T. Roweis 1', ' Lawrence K. Saul 2'],\n",
       "  'date': '2000',\n",
       "  'identifier': '2053186076',\n",
       "  'references': ['1902027874',\n",
       "   '2610857016',\n",
       "   '1991848143',\n",
       "   '2107636931',\n",
       "   '2121122425',\n",
       "   '2122538988',\n",
       "   '2047870719',\n",
       "   '2070320140',\n",
       "   '1513400187',\n",
       "   '2019020850'],\n",
       "  'title': 'Nonlinear dimensionality reduction by locally linear embedding.'},\n",
       " {'abstract': 'A general non-parametric technique is proposed for the analysis of a complex multimodal feature space and to delineate arbitrarily shaped clusters in it. The basic computational module of the technique is an old pattern recognition procedure: the mean shift. For discrete data, we prove the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and, thus, its utility in detecting the modes of the density. The relation of the mean shift procedure to the Nadaraya-Watson estimator from kernel regression and the robust M-estimators; of location is also established. Algorithms for two low-level vision tasks discontinuity-preserving smoothing and image segmentation - are described as applications. In these algorithms, the only user-set parameter is the resolution of the analysis, and either gray-level or color images are accepted as input. Extensive experimental results illustrate their excellent performance.',\n",
       "  'authors': ['D. Comaniciu ', ' P. Meer'],\n",
       "  'date': '2002',\n",
       "  'identifier': '2067191022',\n",
       "  'references': ['2798766386',\n",
       "   '2140235142',\n",
       "   '2099244020',\n",
       "   '2132549764',\n",
       "   '2150134853',\n",
       "   '2159128898',\n",
       "   '1971784203',\n",
       "   '2999729612',\n",
       "   '2129905273',\n",
       "   '1499877760'],\n",
       "  'title': 'Mean shift: a robust approach toward feature space analysis'},\n",
       " {'abstract': 'Two types of sampling plans are examined as alternatives to simple random sampling in Monte Carlo studies. These plans are shown to be improvements over simple random sampling with respect to variance for a class of estimators which includes the sample mean and the empirical distribution function.',\n",
       "  'authors': ['M. D. McKay 1', ' R. J. Beckman 1', ' W. J. Conover 2'],\n",
       "  'date': '2000',\n",
       "  'identifier': '2038669746',\n",
       "  'references': ['3041461815',\n",
       "   '2096992152',\n",
       "   '2116414161',\n",
       "   '1638594382',\n",
       "   '2059347608',\n",
       "   '2330498442',\n",
       "   '2022498557',\n",
       "   '2090895976',\n",
       "   '1977573324',\n",
       "   '2010374567'],\n",
       "  'title': 'A comparison of three methods for selecting values of input variables in the analysis of output from a computer code'},\n",
       " {'abstract': 'The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.',\n",
       "  'authors': ['Mark Everingham 1',\n",
       "   ' Luc Gool 2',\n",
       "   ' Christopher K. Williams 3',\n",
       "   ' John Winn 4',\n",
       "   ' Andrew Zisserman 5'],\n",
       "  'date': '2010',\n",
       "  'identifier': '2031489346',\n",
       "  'references': ['2151103935',\n",
       "   '2161969291',\n",
       "   '3097096317',\n",
       "   '2162915993',\n",
       "   '2038721957',\n",
       "   '2131846894',\n",
       "   '2104974755',\n",
       "   '2110764733',\n",
       "   '1576445103',\n",
       "   '1565746575'],\n",
       "  'title': 'The Pascal Visual Object Classes (VOC) Challenge'},\n",
       " {'abstract': \"Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations. We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.\",\n",
       "  'authors': ['Aditya Grover ', ' Jure Leskovec'],\n",
       "  'date': '2016',\n",
       "  'identifier': '2962756421',\n",
       "  'references': ['2153579005',\n",
       "   '1614298861',\n",
       "   '2250539671',\n",
       "   '2163922914',\n",
       "   '2053186076',\n",
       "   '3102641634',\n",
       "   '2001141328',\n",
       "   '3104097132',\n",
       "   '1888005072',\n",
       "   '2156718197'],\n",
       "  'title': 'node2vec: Scalable Feature Learning for Networks'},\n",
       " {'abstract': 'In the field of machine learning, semi-supervised learning (SSL) occupies the middle ground, between supervised learning (in which all training examples are labeled) and unsupervised learning (in which no label data are given). Interest in SSL has increased in recent years, particularly because of application domains in which unlabeled data are plentiful, such as images, text, and bioinformatics. This first comprehensive overview of SSL presents state-of-the-art algorithms, a taxonomy of the field, selected applications, benchmark experiments, and perspectives on ongoing and future research. Semi-Supervised Learning first presents the key assumptions and ideas underlying the field: smoothness, cluster or low-density separation, manifold structure, and transduction. The core of the book is the presentation of SSL methods, organized according to algorithmic strategies. After an examination of generative models, the book describes algorithms that implement the low-density separation assumption, graph-based methods, and algorithms that perform two-step learning. The book then discusses SSL applications and offers guidelines for SSL practitioners by analyzing the results of extensive benchmark experiments. Finally, the book looks at interesting directions for SSL research. The book closes with a discussion of the relationship between semi-supervised learning and transduction. Adaptive Computation and Machine Learning series',\n",
       "  'authors': ['Olivier Chapelle ', ' Bernhard Schlkopf ', ' Alexander Zien'],\n",
       "  'date': '2010',\n",
       "  'identifier': '1479807131',\n",
       "  'references': ['2296319761',\n",
       "   '2156909104',\n",
       "   '2158714788',\n",
       "   '2148694408',\n",
       "   '2055043387',\n",
       "   '1480376833',\n",
       "   '2119821739',\n",
       "   '2053186076',\n",
       "   '2121947440',\n",
       "   '1988790447'],\n",
       "  'title': 'Semi-Supervised Learning'},\n",
       " {'abstract': 'Tracking new topics, ideas, and \"memes\" across the Web has been an issue of considerable interest. Recent work has developed methods for tracking topic shifts over long time scales, as well as abrupt spikes in the appearance of particular named entities. However, these approaches are less well suited to the identification of content that spreads widely and then fades over time scales on the order of days - the time scale at which we perceive news and events. We develop a framework for tracking short, distinctive phrases that travel relatively intact through on-line text; developing scalable algorithms for clustering textual variants of such phrases, we identify a broad class of memes that exhibit wide spread and rich variation on a daily basis. As our principal domain of study, we show how such a meme-tracking approach can provide a coherent representation of the news cycle - the daily rhythms in the news media that have long been the subject of qualitative interpretation but have never been captured accurately enough to permit actual quantitative analysis. We tracked 1.6 million mainstream media sites and blogs over a period of three months with the total of 90 million articles and we find a set of novel and persistent temporal patterns in the news cycle. In particular, we observe a typical lag of 2.5 hours between the peaks of attention to a phrase in the news media and in blogs respectively, with divergent behavior around the overall peak and a \"heartbeat\"-like pattern in the handoff between news and blogs. We also develop and analyze a mathematical model for the kinds of temporal variation that the system exhibits.',\n",
       "  'authors': ['Jure Leskovec 1', ' Lars Backstrom 2', ' Jon Kleinberg 2'],\n",
       "  'date': '2009',\n",
       "  'identifier': '2127492100',\n",
       "  'references': ['1880262756',\n",
       "   '2124637492',\n",
       "   '1521478692',\n",
       "   '2072644219',\n",
       "   '2152284345',\n",
       "   '2021314079',\n",
       "   '2171343266',\n",
       "   '2113889316',\n",
       "   '2058465497',\n",
       "   '2073689275'],\n",
       "  'title': 'Meme-tracking and the dynamics of the news cycle'},\n",
       " {'abstract': 'Edge detection is the process that attempts to characterize the intensity changes in the image in terms of the physical processes that have originated them. A critical, intermediate goal of edge detection is the detection and characterization of significant intensity changes. This paper discusses this part of the edge detection problem. To characterize the types of intensity changes derivatives of different types, and possibly different scales, are needed. Thus, we consider this part of edge detection as a problem in numerical differentiation. We show that numerical differentiation of images is an ill-posed problem in the sense of Hadamard. Differentiation needs to be regularized by a regularizing filtering operation before differentiation. This shows that this part of edge detection consists of two steps, a filtering step and a differentiation step. Following this perspective, the paper discusses in detail the following theoretical aspects of edge detection. 1) The properties of different types of filters-with minimal uncertainty, with a bandpass spectrum, and with limited support-are derived. Minimal uncertainty filters optimize a tradeoff between computational efficiency and regularizing properties. 2) Relationships among several 2-D differential operators are established. In particular, we characterize the relation between the Laplacian and the second directional derivative along the gradient. Zero crossings of the Laplacian are not the only features computed in early vision. 3) Geometrical and topological properties of the zero crossings of differential operators are studied in terms of transversality and Morse theory.',\n",
       "  'authors': ['Vincent Torre 1', ' Tomaso A. Poggio 2'],\n",
       "  'date': '1986',\n",
       "  'identifier': '2155487652',\n",
       "  'references': ['2740373864',\n",
       "   '2109863423',\n",
       "   '2003370853',\n",
       "   '2006500012',\n",
       "   '1995756857',\n",
       "   '2133155955',\n",
       "   '1530383550',\n",
       "   '2007057443',\n",
       "   '2121203842',\n",
       "   '2038584908'],\n",
       "  'title': 'On Edge Detection'},\n",
       " {'abstract': 'Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k > 2 values (i.e., k \"classes\"). The definition is acquired by studying collections of training examples of the form (xi, f(xi)). Existing approaches to multiclass learning problems include direct application of multiclass algorithms such as the decision-tree algorithms C4.5 and CART, application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and application of binary concept learning algorithms with distributed output representations. This paper compares these three approaches to a new technique in which error-correcting codes are employed as a distributed output representation. We show that these output representations improve the generalization performance of both C4.5 and backpropagation on a wide range of multiclass learning tasks. We also demonstrate that this approach is robust with respect to changes in the size of the training sample, the assignment of distributed representations to particular classes, and the application of overfitting avoidance techniques such as decision-tree pruning. Finally, we show that--like the other methods--the error-correcting code technique can provide reliable class probability estimates. Taken together, these results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems.',\n",
       "  'authors': ['Thomas G. Dietterich 1', ' Ghulum Bakiri 2'],\n",
       "  'date': '1994',\n",
       "  'identifier': '1676820704',\n",
       "  'references': ['2154642048',\n",
       "   '3085162807',\n",
       "   '2147800946',\n",
       "   '2798643531',\n",
       "   '2173629880',\n",
       "   '2019363670',\n",
       "   '2093717447',\n",
       "   '3036751298',\n",
       "   '2176028050',\n",
       "   '1667614912'],\n",
       "  'title': 'Solving multiclass learning problems via error-correcting output codes'},\n",
       " {'abstract': \"Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.\",\n",
       "  'authors': ['Sepp Hochreiter 1', ' Jürgen Schmidhuber 2'],\n",
       "  'date': '1997',\n",
       "  'identifier': '2064675550',\n",
       "  'references': ['2107878631',\n",
       "   '2128499899',\n",
       "   '2007431958',\n",
       "   '2123716044',\n",
       "   '2143503258',\n",
       "   '2154890045',\n",
       "   '194249466',\n",
       "   '2048060899',\n",
       "   '2103452139',\n",
       "   '1674799117'],\n",
       "  'title': 'Long short-term memory'},\n",
       " {'abstract': 'Information filtering systems are designed for unstructured or semistructured data, as opposed to database applications, which use very structured data. The systems also deal primarily with textual information, but they may also entail images, voice, video or other data types that are part of multimedia information systems. Information filtering systems also involve a large amount of data and streams of incoming data, whether broadcast from a remote source or sent directly by other sources. Filtering is based on descriptions of individual or group information preferences, or profiles, that typically represent long-term interests. Filtering also implies removal of data from an incoming stream rather than finding data in the stream; users see only the data that is extracted. Models of information retrieval and filtering, and lessons for filtering from retrieval research are presented.',\n",
       "  'authors': ['Nicholas J. Belkin 1', ' W. Bruce Croft 2'],\n",
       "  'date': '1992',\n",
       "  'identifier': '2106365165',\n",
       "  'references': ['2159080219',\n",
       "   '2147152072',\n",
       "   '1956559956',\n",
       "   '1978394996',\n",
       "   '1770825568',\n",
       "   '2000672666',\n",
       "   '2135346934',\n",
       "   '2048045485',\n",
       "   '2078875869',\n",
       "   '2145036943'],\n",
       "  'title': 'Information filtering and information retrieval: two sides of the same coin?'},\n",
       " {'abstract': 'Publisher Summary This chapter presents an overview of Optical Character Recognition (OCR) for statisticians interested in extending their endeavors from the traditional realm of pattern classification to the many other alluring aspects of OCR. The most important dimensions of data entry from the point of view of a project manager considering the acquisition of an OCR system are described in the chapter. The major applications are categorized according to the type of data to be converted to computer-readable form and optical scanners are described. The preprocessing necessary before the actual character classification can take place is discussed in the chapter. It outlines the classical decision-theoretic formulation of the character classification problem. The various statistical approximations to the optimal classifier, including dimensionality reduction, feature extraction, and feature selection is discussed with references to the appropriate statistical techniques. The importance of accurate estimation of the error and reject rates are discussed in the chapter and a fundamental relation between the error rate and the reject rate in optimal systems are described in the chapter.',\n",
       "  'authors': ['George Nagy'],\n",
       "  'date': '1982',\n",
       "  'identifier': '1519534430',\n",
       "  'references': ['2163166770',\n",
       "   '1989584148',\n",
       "   '2058929792',\n",
       "   '2079145130',\n",
       "   '2118377301',\n",
       "   '2061681083',\n",
       "   '2027298168',\n",
       "   '2104474593',\n",
       "   '2147947791',\n",
       "   '2097486270'],\n",
       "  'title': '29 Optical character recognition—Theory and practice'},\n",
       " {'abstract': '',\n",
       "  'authors': ['James Kennedy'],\n",
       "  'date': '2017',\n",
       "  'identifier': '2613176274',\n",
       "  'references': ['2185533914',\n",
       "   '3032913569',\n",
       "   '2969981579',\n",
       "   '2981455014',\n",
       "   '2915435825',\n",
       "   '2252127210',\n",
       "   '2276269997',\n",
       "   '3027279639',\n",
       "   '2621668204',\n",
       "   '3106163595'],\n",
       "  'title': 'Particle Swarm Optimization.'},\n",
       " {'abstract': '1. Various Aspects of Memory.- 1.1 On the Purpose and Nature of Biological Memory.- 1.1.1 Some Fundamental Concepts.- 1.1.2 The Classical Laws of Association.- 1.1.3 On Different Levels of Modelling.- 1.2 Questions Concerning the Fundamental Mechanisms of Memory.- 1.2.1 Where Do the Signals Relating to Memory Act Upon?.- 1.2.2 What Kind of Encoding is Used for Neural Signals?.- 1.2.3 What are the Variable Memory Elements?.- 1.2.4 How are Neural Signals Addressed in Memory?.- 1.3 Elementary Operations Implemented by Associative Memory.- 1.3.1 Associative Recall.- 1.3.2 Production of Sequences from the Associative Memory.- 1.3.3 On the Meaning of Background and Context.- 1.4 More Abstract Aspects of Memory.- 1.4.1 The Problem of Infinite-State Memory.- 1.4.2 Invariant Representations.- 1.4.3 Symbolic Representations.- 1.4.4 Virtual Images.- 1.4.5 The Logic of Stored Knowledge.- 2. Pattern Mathematics.- 2.1 Mathematical Notations and Methods.- 2.1.1 Vector Space Concepts.- 2.1.2 Matrix Notations.- 2.1.3 Further Properties of Matrices.- 2.1.4 Matrix Equations.- 2.1.5 Projection Operators.- 2.1.6 On Matrix Differential Calculus.- 2.2 Distance Measures for Patterns.- 2.2.1 Measures of Similarity and Distance in Vector Spaces.- 2.2.2 Measures of Similarity and Distance Between Symbol Strings.- 2.2.3 More Accurate Distance Measures for Text.- 3. Classical Learning Systems.- 3.1 The Adaptive Linear Element (Adaline).- 3.1.1 Description of Adaptation by the Stochastic Approximation.- 3.2 The Perceptron.- 3.3 The Learning Matrix.- 3.4 Physical Realization of Adaptive Weights.- 3.4.1 Perceptron and Adaline.- 3.4.2 Classical Conditioning.- 3.4.3 Conjunction Learning Switches.- 3.4.4 Digital Representation of Adaptive Circuits.- 3.4.5 Biological Components.- 4. A New Approach to Adaptive Filters.- 4.1 Survey of Some Necessary Functions.- 4.2 On the \"Transfer Function\" of the Neuron.- 4.3 Models for Basic Adaptive Units.- 4.3.1 On the Linearization of the Basic Unit.- 4.3.2 Various Cases of Adaptation Laws.- 4.3.3 Two Limit Theorems.- 4.3.4 The Novelty Detector.- 4.4 Adaptive Feedback Networks.- 4.4.1 The Autocorrelation Matrix Memory.- 4.4.2 The Novelty Filter.- 5. Self-Organizing Feature Maps.- 5.1 On the Feature Maps of the Brain.- 5.2 Formation of Localized Responses by Lateral Feedback.- 5.3 Computational Simplification of the Process.- 5.3.1 Definition of the Topology-Preserving Mapping.- 5.3.2 A Simple Two-Dimensional Self-Organizing System.- 5.4 Demonstrations of Simple Topology-Preserving Mappings.- 5.4.1 Images of Various Distributions of Input Vectors.- 5.4.2 \"The Magic TV\".- 5.4.3 Mapping by a Feeler Mechanism.- 5.5 Tonotopic Map.- 5.6 Formation of Hierarchical Representations.- 5.6.1 Taxonomy Example.- 5.6.2 Phoneme Map.- 5.7 Mathematical Treatment of Self-Organization.- 5.7.1 Ordering of Weights.- 5.7.2 Convergence Phase.- 5.8 Automatic Selection of Feature Dimensions.- 6. Optimal Associative Mappings.- 6.1 Transfer Function of an Associative Network.- 6.2 Autoassociative Recall as an Orthogonal Projection.- 6.2.1 Orthogonal Projections.- 6.2.2 Error-Correcting Properties of Projections.- 6.3 The Novelty Filter.- 6.3.1 Two Examples of Novelty Filter.- 6.3.2 Novelty Filter as an Autoassociative Memory.- 6.4 Autoassociative Encoding.- 6.4.1 An Example of Autoassociative Encoding.- 6.5 Optimal Associative Mappings.- 6.5.1 The Optimal Linear Associative Mapping.- 6.5.2 Optimal Nonlinear Associative Mappings.- 6.6 Relationship Between Associative Mapping, Linear Regression, and Linear Estimation.- 6.6.1 Relationship of the Associative Mapping to Linear Regression.- 6.6.2 Relationship of the Regression Solution to the Linear Estimator.- 6.7 Recursive Computation of the Optimal Associative Mapping.- 6.7.1 Linear Corrective Algorithms.- 6.7.2 Best Exact Solution (Gradient Projection).- 6.7.3 Best Approximate Solution (Regression).- 6.7.4 Recursive Solution in the General Case.- 6.8 Special Cases.- 6.8.1 The Correlation Matrix Memory.- 6.8.2 Relationship Between Conditional Averages and Optimal Estimator.- 7. Pattern Recognition.- 7.1 Discriminant Functions.- 7.2 Statistical Formulation of Pattern Classification.- 7.3 Comparison Methods.- 7.4 The Subspace Methods of Classification.- 7.4.1 The Basic Subspace Method.- 7.4.2 The Learning Subspace Method (LSM).- 7.5 Learning Vector Quantization.- 7.6 Feature Extraction.- 7.7 Clustering.- 7.7.1 Simple Clustering (Optimization Approach).- 7.7.2 Hierarchical Clustering (Taxonomy Approach).- 7.8 Structural Pattern Recognition Methods.- 8. More About Biological Memory.- 8.1 Physiological Foundations of Memory.- 8.1.1 On the Mechanisms of Memory in Biological Systems.- 8.1.2 Structural Features of Some Neural Networks.- 8.1.3 Functional Features of Neurons.- 8.1.4 Modelling of the Synaptic Plasticity.- 8.1.5 Can the Memory Capacity Ensue from Synaptic Changes?.- 8.2 The Unified Cortical Memory Model.- 8.2.1 The Laminar Network Organization.- 8.2.2 On the Roles of Interneurons.- 8.2.3 Representation of Knowledge Over Memory Fields.- 8.2.4 Self-Controlled Operation of Memory.- 8.3 Collateral Reading.- 8.3.1 Physiological Results Relevant to Modelling.- 8.3.2 Related Modelling.- 9. Notes on Neural Computing.- 9.1 First Theoretical Views of Neural Networks.- 9.2 Motives for the Neural Computing Research.- 9.3 What Could the Purpose of the Neural Networks be?.- 9.4 Definitions of Artificial \"Neural Computing\" and General Notes on Neural Modelling.- 9.5 Are the Biological Neural Functions Localized or Distributed?.- 9.6 Is Nonlinearity Essential to Neural Computing?.- 9.7 Characteristic Differences Between Neural and Digital Computers.- 9.7.1 The Degree of Parallelism of the Neural Networks is Still Higher than that of any \"Massively Parallel\" Digital Computer.- 9.7.2 Why the Neural Signals Cannot be Approximated by Boolean Variables.- 9.7.3 The Neural Circuits do not Implement Finite Automata.- 9.7.4 Undue Views of the Logic Equivalence of the Brain and Computers on a High Level.- 9.8 \"Connectionist Models\".- 9.9 How can the Neural Computers be Programmed?.- 10. Optical Associative Memories.- 10.1 Nonholographic Methods.- 10.2 General Aspects of Holographic Memories.- 10.3 A Simple Principle of Holographic Associative Memory.- 10.4 Addressing in Holographic Memories.- 10.5 Recent Advances of Optical Associative Memories.- Bibliography on Pattern Recognition.- References.',\n",
       "  'authors': ['Teuvo Kohonen'],\n",
       "  'date': '1984',\n",
       "  'identifier': '1991848143',\n",
       "  'references': ['2076063813',\n",
       "   '2053186076',\n",
       "   '1992419399',\n",
       "   '2141125852',\n",
       "   '3111038685',\n",
       "   '2161160262',\n",
       "   '2186428165',\n",
       "   '2153791616',\n",
       "   '2115689562'],\n",
       "  'title': 'Self Organization And Associative Memory'},\n",
       " {'abstract': 'One long-term goal of machine learning research is to produce methods that are applicable to highly complex tasks, such as perception (vision, audition), reasoning, intelligent control, and other artificially intelligent behaviors. We argue that in order to progress toward this goal, the Machine Learning community must endeavor to discover algorithms that can learn highly complex functions, with minimal need for prior knowledge, and with minimal human intervention. We present mathematical and empirical evidence suggesting that many popular approaches to non-parametric learning, particularly kernel methods, are fundamentally limited in their ability to learn complex high-dimensional functions. Our analysis focuses on two problems. First, kernel machines are shallow architectures, in which one large layer of simple template matchers is followed by a single layer of trainable coefficients. We argue that shallow architectures can be very inefficient in terms of required number of computational elements and examples. Second, we analyze a limitation of kernel machines with a local kernel, linked to the curse of dimensionality, that applies to supervised, unsupervised (manifold learning) and semi-supervised kernel machines. Using empirical results on invariant image recognition tasks, kernel methods are compared with deep architectures, in which lower-level features or concepts are progressively combined into more abstract and higher-level representations. We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence.',\n",
       "  'authors': ['Yoshua Bengio 1', ' 2', ' 3', ' Yann Lecun'],\n",
       "  'date': '2007',\n",
       "  'identifier': '2613634265',\n",
       "  'references': ['2136922672',\n",
       "   '2148603752',\n",
       "   '2310919327',\n",
       "   '2119821739',\n",
       "   '2053186076',\n",
       "   '2001141328',\n",
       "   '2116064496',\n",
       "   '2057175746',\n",
       "   '2110798204',\n",
       "   '2140095548'],\n",
       "  'title': 'Scaling learning algorithms towards AI'},\n",
       " {'abstract': 'Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%, [26]) on this dataset.',\n",
       "  'authors': ['Kaiming He 1',\n",
       "   ' Xiangyu Zhang 2',\n",
       "   ' Shaoqing Ren 1',\n",
       "   ' Jian Sun 1'],\n",
       "  'date': '2015',\n",
       "  'identifier': '1677182931',\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '1836465849',\n",
       "   '2102605133',\n",
       "   '2117539524',\n",
       "   '2095705004',\n",
       "   '2108598243',\n",
       "   '2155893237',\n",
       "   '1536680647'],\n",
       "  'title': 'Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification'},\n",
       " {'abstract': 'This paper describes an offline cursive handwritten word recognition system that combines hidden Markov models (HMM) and neural networks (NN). Using a fast left-right slicing method, we generate a segmentation graph that describes all possible ways to segment a word into letters. The NN computes the observation probabilities for each letter hypothesis in the segmentation graph. Then, the HMM compute the likelihood for each word in the lexicon by summing the probabilities over all possible paths through the graph. We present the preprocessing and the recognition process as well as the training procedure for the NN-HMM hybrid system. Another recognition system based on discrete HMM is also presented for performance comparison. The latter is also used for bootstrapping the NN-HMM hybrid system. Recognition performances of the two recognition systems using two image databases of French isolated words are presented. This paper is one of the first publications using the IRONOFF database, and thus can be used as a reference for future work on this database.',\n",
       "  'authors': ['Yong Haur Tay ',\n",
       "   ' P.M. Lallican ',\n",
       "   ' M. Khalid ',\n",
       "   ' C. Viard-Gaudin ',\n",
       "   ' S. Kneer'],\n",
       "  'date': '2001',\n",
       "  'identifier': '2147345686',\n",
       "  'references': ['2310919327',\n",
       "   '2125838338',\n",
       "   '2142069714',\n",
       "   '183625566',\n",
       "   '2149597185',\n",
       "   '2077863651',\n",
       "   '2148295954',\n",
       "   '101240229',\n",
       "   '2064838583',\n",
       "   '1599953749'],\n",
       "  'title': 'An offline cursive handwritten word recognition system'},\n",
       " {'abstract': 'The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.',\n",
       "  'authors': ['Tomas Mikolov ',\n",
       "   ' Ilya Sutskever ',\n",
       "   ' Kai Chen ',\n",
       "   ' Greg S Corrado ',\n",
       "   ' Jeff Dean'],\n",
       "  'date': '2013',\n",
       "  'identifier': '2153579005',\n",
       "  'references': ['1614298861',\n",
       "   '2141599568',\n",
       "   '2117130368',\n",
       "   '2132339004',\n",
       "   '2158139315',\n",
       "   '1423339008',\n",
       "   '1498436455',\n",
       "   '1662133657',\n",
       "   '1889268436',\n",
       "   '2131462252'],\n",
       "  'title': 'Distributed Representations of Words and Phrases and their Compositionality'},\n",
       " {'abstract': 'This article proposes a novel framework for representing and measuring local coherence. Central to this approach is the entity-grid representation of discourse, which captures patterns of entity distribution in a text. The algorithm introduced in the article automatically abstracts a text into a set of entity transition sequences and records distributional, syntactic, and referential information about discourse entities. We re-conceptualize coherence assessment as a learning task and show that our entity-based representation is well-suited for ranking-based generation and text classification tasks. Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment.',\n",
       "  'authors': ['Regina Barzilay 1', ' 2', ' Mirella Lapata 3'],\n",
       "  'date': '2008',\n",
       "  'identifier': '2140676672',\n",
       "  'references': ['2148603752',\n",
       "   '2101105183',\n",
       "   '2149684865',\n",
       "   '2123504579',\n",
       "   '2047221353',\n",
       "   '1983578042',\n",
       "   '1535015163',\n",
       "   '2107890099',\n",
       "   '2150824314',\n",
       "   '3021452258'],\n",
       "  'title': 'Modeling local coherence: An entity-based approach'},\n",
       " {'abstract': 'The standard data fusion methods may not be satisfactory to merge a high-resolution panchromatic image and a low-resolution multispectral image because they can distort the spectral characteristics of the multispectral data. The authors developed a technique, based on multiresolution wavelet decomposition, for the merging and data fusion of such images. The method presented consists of adding the wavelet coefficients of the high-resolution image to the multispectral (low-resolution) data. They have studied several possibilities concluding that the method which produces the best results consists in adding the high order coefficients of the wavelet transform of the panchromatic image to the intensity component (defined as L=(R+G+B)/3) of the multispectral image. The method is, thus, an improvement on standard intensity-hue-saturation (IHS or LHS) mergers. They used the \"a trous\" algorithm which allows the use of a dyadic wavelet to merge nondyadic data in a simple and efficient scheme. They used the method to merge SPOT and LANDSAT/sup TM/ images. The technique presented is clearly better than the IHS and LHS mergers in preserving both spectral and spatial information.',\n",
       "  'authors': ['J. Nunez 1',\n",
       "   ' X. Otazu 1',\n",
       "   ' O. Fors 1',\n",
       "   ' A. Prades 2',\n",
       "   ' V. Pala 3',\n",
       "   ' R. Arbiol 3'],\n",
       "  'date': '1999',\n",
       "  'identifier': '2152254169',\n",
       "  'references': ['2062024414',\n",
       "   '2132984323',\n",
       "   '1489213177',\n",
       "   '2103504761',\n",
       "   '2101789093',\n",
       "   '2021722152',\n",
       "   '1991460509',\n",
       "   '2171845746',\n",
       "   '1553305639',\n",
       "   '1522046140'],\n",
       "  'title': 'Multiresolution-based image fusion with additive wavelet decomposition'},\n",
       " {'abstract': 'Format-preserving encryption (FPE) encrypts a plaintext of some specified format into a ciphertext of identical format--for example, encrypting a valid credit-card number into a valid credit-card number. The problem has been known for some time, but it has lacked a fully general and rigorous treatment. We provide one, starting off by formally defining FPE and security goals for it. We investigate the natural approach for achieving FPE on complex domains, the \"rank-then-encipher\" approach, and explore what it can and cannot do. We describe two flavors of unbalanced Feistel networks that can be used for achieving FPE, and we prove new security results for each. We revisit the cycle-walking approach for enciphering on a non-sparse subset of an encipherable domain, showing that the timing information that may be divulged by cycle walking is not a damaging thing to leak.',\n",
       "  'authors': ['Mihir Bellare 1',\n",
       "   ' Thomas Ristenpart 1',\n",
       "   ' Phillip Rogaway 2',\n",
       "   ' Till Stegers 2'],\n",
       "  'date': '2009',\n",
       "  'identifier': '1673464365',\n",
       "  'references': ['2011039300',\n",
       "   '2891212941',\n",
       "   '2911767655',\n",
       "   '2752853835',\n",
       "   '1994584977',\n",
       "   '2117362057',\n",
       "   '2126290606',\n",
       "   '2161611531',\n",
       "   '2611357843',\n",
       "   '2077300005'],\n",
       "  'title': 'Format-Preserving Encryption'},\n",
       " {'abstract': 'A deceptively simple kind of optical pattern recognition deals with print and script. What seemed at one time to be a fairly easy problem area in automated reading of line-like patterns has turned out to be difficult and expensive. The evolution and present state of the art of machine recognition of print and script is examined. On-hand systems relieve large amounts of human drudgery, and both theory and engineering design have advanced greatly in response to pressures caused by our paper explosion. But major problems of cost and effectiveness still exist.',\n",
       "  'authors': ['L.D. Harmon'],\n",
       "  'date': '1972',\n",
       "  'identifier': '2097486270',\n",
       "  'references': ['2133246412',\n",
       "   '2159498975',\n",
       "   '2158240273',\n",
       "   '1966591781',\n",
       "   '2067958620',\n",
       "   '2079145130',\n",
       "   '2002448074',\n",
       "   '2150642297',\n",
       "   '2083417201',\n",
       "   '2104474593'],\n",
       "  'title': 'Automatic recognition of print and script'},\n",
       " {'abstract': '',\n",
       "  'authors': ['John Van Ryzin ',\n",
       "   ' Leo Breiman ',\n",
       "   ' Jerome H. Friedman ',\n",
       "   ' Richard A. Olshen ',\n",
       "   ' Charles J. Stone'],\n",
       "  'date': '1986',\n",
       "  'identifier': '3085162807',\n",
       "  'references': ['2133990480',\n",
       "   '2140190241',\n",
       "   '1570448133',\n",
       "   '2072128103',\n",
       "   '2135046866',\n",
       "   '1484413656',\n",
       "   '2158698691',\n",
       "   '2912934387',\n",
       "   '2166559705',\n",
       "   '2063978378'],\n",
       "  'title': 'Classification and Regression Trees.'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Benjamin Marlin'],\n",
       "  'date': '2004',\n",
       "  'identifier': '2118079529',\n",
       "  'references': ['1880262756',\n",
       "   '2119479037',\n",
       "   '2125055259',\n",
       "   '2798909945',\n",
       "   '2110325612',\n",
       "   '2159080219',\n",
       "   '2135029798',\n",
       "   '2155106456',\n",
       "   '2049633694',\n",
       "   '2017337590'],\n",
       "  'title': 'Collaborative Filtering: A Machine Learning Perspective'},\n",
       " {'abstract': 'Abstract: One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.',\n",
       "  'authors': ['Jason Weston ',\n",
       "   ' Antoine Bordes ',\n",
       "   ' Sumit Chopra ',\n",
       "   ' Alexander M. Rush ',\n",
       "   ' Bart van Merriënboer ',\n",
       "   ' Armand Joulin ',\n",
       "   ' Tomas Mikolov'],\n",
       "  'date': '2016',\n",
       "  'identifier': '2962790689',\n",
       "  'references': ['1933349210',\n",
       "   '2962809918',\n",
       "   '2561715562',\n",
       "   '2964091467',\n",
       "   '2768661419',\n",
       "   '2963448850',\n",
       "   '2962717047'],\n",
       "  'title': 'Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks'},\n",
       " {'abstract': \"Based on a recursive process of reducing the entropy, the general decision tree classifier with overlap has been analyzed. Several theorems have been proposed and proved. When the number of pattern classes is very large, the theorems can reveal both the advantages of a tree classifier and the main difficulties in its implementation. Suppose H is Shannon's entropy measure of the given problem. The theoretical results indicate that the tree searching time can be minimized to the order O(H), but the error rate is also in the same order O(H) due to error accumulation. However, the memory requirement is in the order 0(H exp(H)) which poses serious problems in the implementation of a tree classifier for a large number of classes. To solve these problems, several theorems related to the bounds on the search time, error rate, memory requirement and overlap factor in the design of a decision tree have been proposed and some principles have been established to analyze the behaviors of the decision tree. When applied to classify sets of 64, 450, and 3200 Chinese characters, respectively, the experimental results support the theoretical predictions. For 3200 classes, a very high recognition rate of 99.88 percent was achieved at a high speed of 873 samples/s when the experiment was conducted on a Cyber 172 computer using a high-level language.\",\n",
       "  'authors': ['Qing Ren Wang ', ' Ching Y. Suen'],\n",
       "  'date': '1984',\n",
       "  'identifier': '1963937384',\n",
       "  'references': ['1995875735',\n",
       "   '1976123439',\n",
       "   '2088534753',\n",
       "   '1970361289',\n",
       "   '2073237809',\n",
       "   '1966215606',\n",
       "   '2022167939',\n",
       "   '2062033901',\n",
       "   '2077450947',\n",
       "   '2064356212'],\n",
       "  'title': 'Analysis and Design of a Decision Tree Based on Entropy Reduction and Its Application to Large Character Set Recognition'},\n",
       " {'abstract': 'A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. initial tests find this completely automatic method for retrieval to be promising.',\n",
       "  'authors': ['Scott Deerwester 1',\n",
       "   ' Susan T. Dumais 2',\n",
       "   ' George W. Furnas 2',\n",
       "   ' Thomas K. Landauer 2',\n",
       "   ' Richard Harshman 3'],\n",
       "  'date': '1990',\n",
       "  'identifier': '2147152072',\n",
       "  'references': ['1956559956',\n",
       "   '1984565341',\n",
       "   '1964262399',\n",
       "   '2000215628',\n",
       "   '2114804204',\n",
       "   '2151561903',\n",
       "   '3012395598',\n",
       "   '2024683548',\n",
       "   '1965061793',\n",
       "   '2096411881'],\n",
       "  'title': 'Indexing by Latent Semantic Analysis'},\n",
       " {'abstract': 'A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail.',\n",
       "  'authors': ['L. Itti 1', ' C. Koch 1', ' E. Niebur 2'],\n",
       "  'date': '1998',\n",
       "  'identifier': '2128272608',\n",
       "  'references': ['2093353037',\n",
       "   '2149095485',\n",
       "   '2160903697',\n",
       "   '1486735428',\n",
       "   '2089597841',\n",
       "   '1497599070',\n",
       "   '2115441154',\n",
       "   '2152752164',\n",
       "   '1502980253',\n",
       "   '2142768220'],\n",
       "  'title': 'A model of saliency-based visual attention for rapid scene analysis'},\n",
       " {'abstract': '',\n",
       "  'authors': ['G. Salton'],\n",
       "  'date': '1971',\n",
       "  'identifier': '1557757161',\n",
       "  'references': ['1662133657',\n",
       "   '2150102617',\n",
       "   '2097089247',\n",
       "   '2053463056',\n",
       "   '1978394996',\n",
       "   '1550258693',\n",
       "   '2075597533',\n",
       "   '1924689489',\n",
       "   '2138662031',\n",
       "   '2171104838'],\n",
       "  'title': 'The SMART Retrieval System—Experiments in Automatic Document Processing'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Gene H. Golub'],\n",
       "  'date': '1983',\n",
       "  'identifier': '2798909945',\n",
       "  'references': ['2296616510',\n",
       "   '2145096794',\n",
       "   '2164278908',\n",
       "   '1746819321',\n",
       "   '2127271355',\n",
       "   '2167667767',\n",
       "   '2121947440',\n",
       "   '3102641634'],\n",
       "  'title': 'Matrix computations'},\n",
       " {'abstract': \"In k-means clustering, we are given a set of n data points in d-dimensional space R/sup d/ and an integer k and the problem is to determine a set of k points in Rd, called centers, so as to minimize the mean squared distance from each data point to its nearest center. A popular heuristic for k-means clustering is Lloyd's (1982) algorithm. We present a simple and efficient implementation of Lloyd's k-means clustering algorithm, which we call the filtering algorithm. This algorithm is easy to implement, requiring a kd-tree as the only major data structure. We establish the practical efficiency of the filtering algorithm in two ways. First, we present a data-sensitive analysis of the algorithm's running time, which shows that the algorithm runs faster as the separation between clusters increases. Second, we present a number of empirical studies both on synthetically generated data and on real data sets from applications in color quantization, data compression, and image segmentation.\",\n",
       "  'authors': ['T. Kanungo 1',\n",
       "   ' D.M. Mount 2',\n",
       "   ' N.S. Netanyahu 3',\n",
       "   ' C.D. Piatko 4',\n",
       "   ' R. Silverman 2',\n",
       "   ' A.Y. Wu 5'],\n",
       "  'date': '2002',\n",
       "  'identifier': '2161160262',\n",
       "  'references': ['1992419399',\n",
       "   '2011039300',\n",
       "   '2132549764',\n",
       "   '1971784203',\n",
       "   '1634005169',\n",
       "   '2999729612',\n",
       "   '1991848143',\n",
       "   '3017143921',\n",
       "   '1575476631',\n",
       "   '1770825568'],\n",
       "  'title': 'An efficient k-means clustering algorithm: analysis and implementation'},\n",
       " {'abstract': 'The authors investigate the convergence and pruning performance of multilayer feedforward neural networks with different types of neuronal activation functions in solving various problems. Three types of activation functions are adopted in the network, namely, the traditional sigmoid function, the sinusoidal function and a periodic function that can be considered as a combination of the first two functions. To speed up the learning, as well as to reduce the network size, the extended Kalman filter (EKF) algorithm conjunct with a pruning method is used to train the network. The corresponding networks are applied to solve five typical problems, namely, 4-point XOR logic function, parity generation, handwritten digit recognition, piecewise linear function approximation and sunspot series prediction. Simulation results show that periodic activation functions perform better than monotonic ones in solving multicluster classification problems. Moreover, the combined periodic activation function is found to possess the fast convergence and multicluster classification capabilities of the sinusoidal activation function while keeping the robustness property of the sigmoid function required in the modelling of unknown systems.',\n",
       "  'authors': ['K.-W. Wong ', ' C.-S. Leung ', ' S.-J. Chang'],\n",
       "  'date': '2002',\n",
       "  'identifier': '2023795513',\n",
       "  'references': ['2139212933',\n",
       "   '2147800946',\n",
       "   '3036751298',\n",
       "   '2145085734',\n",
       "   '2167982865',\n",
       "   '1995930498',\n",
       "   '2006903949',\n",
       "   '2125389748',\n",
       "   '2169415433',\n",
       "   '2256679588'],\n",
       "  'title': 'Use of periodic and monotonic activation functions in multilayer feedforward neural networks trained by extended Kalman filter algorithm'},\n",
       " {'abstract': 'We attempt to recover an unknown function from noisy, sampled data. Using orthonormal bases of compactly supported wavelets, we develop a nonlinear method which works in the wavelet domain by simple nonlinear shrinkage of the empirical wavelet coefficients. The shrinkage can be tuned to be nearly minimax over any member of a wide range of Triebel- and Besov-type smoothness constraints and asymptotically minimax over Besov bodies with p < q. Linear estimates cannot achieve even the minimax rates over Triebel and Besov classes with p < 2, so the method can significantly outperform every linear method (e.g., kernel, smoothing spline, sieve) in a minimax sense. Variants of our method based on simple threshold nonlinear estimators are nearly minimax. Our method possesses the interpretation of spatial adaptivity; it reconstructs using a kernel which may vary in shape and bandwidth from point to point, depending on the data. Least favorable distributions for certain of the Triebel and Besov scales generate objects with sparse wavelet transforms. Many real objects have similarly sparse transforms, which suggests that these minimax results are relevant for practical problems. Sequels to this paper, which was first drafted in November 1990, discuss practical implementation, spatial adaptation properties, universal near minimaxity and applications to inverse problems.',\n",
       "  'authors': ['David L. Donoho ', ' Iain M. Johnstone'],\n",
       "  'date': '1998',\n",
       "  'identifier': '2033484654',\n",
       "  'references': ['2132984323',\n",
       "   '2098914003',\n",
       "   '2079724595',\n",
       "   '1980149518',\n",
       "   '2166982406',\n",
       "   '654435104',\n",
       "   '2092543127',\n",
       "   '2013987111',\n",
       "   '2088277224',\n",
       "   '2036144352'],\n",
       "  'title': 'Minimax estimation via wavelet shrinkage'},\n",
       " {'abstract': 'Drawing on the correspondence between the graph Laplacian, the Laplace-Beltrami operator on a manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for constructing a representation for data sampled from a low dimensional manifold embedded in a higher dimensional space. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality preserving properties and a natural connection to clustering. Several applications are considered.',\n",
       "  'authors': ['Mikhail Belkin ', ' Partha Niyogi'],\n",
       "  'date': '2001',\n",
       "  'identifier': '2156718197',\n",
       "  'references': ['2053186076',\n",
       "   '2121947440',\n",
       "   '2001141328',\n",
       "   '1578099820',\n",
       "   '108654854'],\n",
       "  'title': 'Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering'},\n",
       " {'abstract': 'Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algorithm can be used both to provide a competitive syntactic parser for natural language sentences from the Penn Treebank and to outperform alternative approaches for semantic scene segmentation, annotation and classification. For segmentation and annotation our algorithm obtains a new level of state-of-the-art performance on the Stanford background dataset (78.1%). The features from the image parse tree outperform Gist descriptors for scene classification by 4%.',\n",
       "  'authors': ['Richard Socher ',\n",
       "   ' Cliff C. Lin ',\n",
       "   ' Chris Manning ',\n",
       "   ' Andrew Y. Ng'],\n",
       "  'date': '2011',\n",
       "  'identifier': '1423339008',\n",
       "  'references': ['2100495367',\n",
       "   '2162915993',\n",
       "   '2067191022',\n",
       "   '2117130368',\n",
       "   '1574901103',\n",
       "   '2132339004',\n",
       "   '2130325614',\n",
       "   '1566135517',\n",
       "   '1528789833',\n",
       "   '2536208356'],\n",
       "  'title': 'Parsing Natural Scenes and Natural Language with Recursive Neural Networks'},\n",
       " {'abstract': 'This paper discusses a family of filters that have been designed for Quadrature Mirror Filter (QMF) Banks. These filters provide a significant improvement over conventional optimal equiripple and window designs when used in QMF banks. The performance criterion for these filters differ from those usually used for filter design in a way which makes the usual filter design techniques difficult to apply. Two filters are actually designed simultaneously, with constraints on the stop band rejection, transition band width, and pass and transition band performance of the QMF filter structure made from those filters. Unlike most filter design problems, the behavior of the transition band is constrained, which places unusual requirements on the design algorithm. The requirement that the overall passband behavior of the QMF bank be constrained (which is a function of the passband and stop band behavior of the filter) also places very unusual requirements on the filter design. The filters were designed using a Hooke and Jeaves optimization routine with a Hanning window prototype. Theoretical results suggest that exactly flat frequency designs cannot be created for filter lengths greater than 2, however, using the discussed procedure, one can obtain QMF banks with as little as ±.0015dB ripple in their frequency response. Due to the nature of QMF filter applications, a small set of filters can be derived which will fit most applications.',\n",
       "  'authors': ['J. Johnston'],\n",
       "  'date': '1980',\n",
       "  'identifier': '2153639720',\n",
       "  'references': ['1690240707', '1627215994', '2152710595', '1857503707'],\n",
       "  'title': 'A filter family designed for use in quadrature mirror filter banks'},\n",
       " {'abstract': 'A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.',\n",
       "  'authors': ['Yoshua Bengio ',\n",
       "   ' Réjean Ducharme ',\n",
       "   ' Pascal Vincent ',\n",
       "   ' Christian Janvin'],\n",
       "  'date': '2003',\n",
       "  'identifier': '2132339004',\n",
       "  'references': ['2038721957',\n",
       "   '2116064496',\n",
       "   '2147152072',\n",
       "   '1631260214',\n",
       "   '2096175520',\n",
       "   '2110485445',\n",
       "   '1575350781',\n",
       "   '2158195707',\n",
       "   '2121227244',\n",
       "   '2914484425'],\n",
       "  'title': 'A neural probabilistic language model'},\n",
       " {'abstract': 'Random balance experimental design has been used in industrial applications of statistical methods since 1956. The purpose of this report is to record and discuss in specific form key points regarding this technique. This report will be divided into several parts that are essentially separate. The topics to be covered include the following: 1. Random design and the motivations for use of random designs in industrial, engineering and research activities. 2. Techniques for analysis of random data and the apparent application domains for which each technique is appropriate. 3. Fundamental problems of statistical analysis that are of critical importance in some random balance applications.',\n",
       "  'authors': ['F. E. Satterthwaite'],\n",
       "  'date': '1959',\n",
       "  'identifier': '2022498557',\n",
       "  'references': ['2038669746',\n",
       "   '2184024640',\n",
       "   '999207820',\n",
       "   '159436435',\n",
       "   '2098043077',\n",
       "   '1969937794',\n",
       "   '2044639673',\n",
       "   '2169658509',\n",
       "   '2078495506',\n",
       "   '611766052'],\n",
       "  'title': 'Random Balance Experimentation'},\n",
       " {'abstract': 'Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overviewof pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval.',\n",
       "  'authors': ['A. K. Jain 1', ' M. N. Murty 2', ' P. J. Flynn 3'],\n",
       "  'date': '1999',\n",
       "  'identifier': '1992419399',\n",
       "  'references': ['2912565176',\n",
       "   '1639032689',\n",
       "   '1497256448',\n",
       "   '2581275558',\n",
       "   '2152150600',\n",
       "   '2049633694',\n",
       "   '2133671888',\n",
       "   '1971784203',\n",
       "   '2095897464',\n",
       "   '1991848143'],\n",
       "  'title': 'Data clustering: a review'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Sepp Hochreiter'],\n",
       "  'date': '1991',\n",
       "  'identifier': '194249466',\n",
       "  'references': ['2194775991',\n",
       "   '2964308564',\n",
       "   '2130942839',\n",
       "   '2076063813',\n",
       "   '2064675550',\n",
       "   '2072128103',\n",
       "   '1924770834',\n",
       "   '1026270304'],\n",
       "  'title': 'Untersuchungen zu dynamischen neuronalen Netzen'},\n",
       " {'abstract': 'Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder‐Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.',\n",
       "  'authors': ['Kyunghyun Cho 1',\n",
       "   ' Bart van Merrienboer 1',\n",
       "   ' Dzmitry Bahdanau 2',\n",
       "   ' Yoshua Bengio 3',\n",
       "   ' 4',\n",
       "   ' 5'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2964199361',\n",
       "  'references': ['2130942839',\n",
       "   '2157331557',\n",
       "   '6908809',\n",
       "   '1753482797',\n",
       "   '1810943226',\n",
       "   '2153653739',\n",
       "   '2395935897',\n",
       "   '1905522558',\n",
       "   '1828163288',\n",
       "   '2341457423'],\n",
       "  'title': 'On the Properties of Neural Machine Translation: Encoder--Decoder Approaches'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Norman Abramson'],\n",
       "  'date': '1963',\n",
       "  'identifier': '158805393',\n",
       "  'references': ['2129652681',\n",
       "   '1515040589',\n",
       "   '1970800786',\n",
       "   '1997988602',\n",
       "   '1996903695',\n",
       "   '2019972422',\n",
       "   '1974293884',\n",
       "   '2130055503',\n",
       "   '2101005742',\n",
       "   '2156626857'],\n",
       "  'title': 'Information theory and coding'},\n",
       " {'abstract': 'An unsupervised learning algorithm for a multilayer network of stochastic neurons is described. Bottom-up \"recognition\" connections convert the input into representations in successive hidden layers, and top-down \"generative\" connections reconstruct the representation in one layer from the representation in the layer above. In the \"wake\" phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the \"sleep\" phase, neurons are driven by generative connections, and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above.',\n",
       "  'authors': ['Geoffrey E. Hinton ',\n",
       "   ' Peter Dayan ',\n",
       "   ' Brendan J. Frey ',\n",
       "   ' Radford M. Neal'],\n",
       "  'date': '1995',\n",
       "  'identifier': '1993845689',\n",
       "  'references': ['2740373864',\n",
       "   '2177040213',\n",
       "   '1533169541',\n",
       "   '2044875682',\n",
       "   '94647076'],\n",
       "  'title': 'The \"Wake-Sleep\" Algorithm for Unsupervised Neural Networks'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Xianfeng Tang 1',\n",
       "   ' Huaxiu Yao 1',\n",
       "   ' Yiwei Sun 1',\n",
       "   ' Yiqi Wang 2',\n",
       "   ' Jiliang Tang 2',\n",
       "   ' Charu C. Aggarwal 3',\n",
       "   ' Prasenjit Mitra 1',\n",
       "   ' Suhang Wang 1'],\n",
       "  'date': '2020',\n",
       "  'identifier': '3103409210',\n",
       "  'references': ['2124637492',\n",
       "   '2964015378',\n",
       "   '2000042664',\n",
       "   '2963420272',\n",
       "   '1976969221',\n",
       "   '2964321699',\n",
       "   '2115022330',\n",
       "   '2624431344',\n",
       "   '2154455818',\n",
       "   '2558748708'],\n",
       "  'title': 'Investigating and Mitigating Degree-Related Biases in Graph Convoltuional Networks.'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Wallace L. Chafe'],\n",
       "  'date': '1980',\n",
       "  'identifier': '1967545963',\n",
       "  'references': ['2164290393',\n",
       "   '2152654022',\n",
       "   '2125447031',\n",
       "   '1481483215',\n",
       "   '2025419023',\n",
       "   '2098773554',\n",
       "   '2014840417',\n",
       "   '1985463960',\n",
       "   '1537542164',\n",
       "   '2990698409'],\n",
       "  'title': 'The Pear Stories: Cognitive, Cultural and Linguistic Aspects of Narrative Production'},\n",
       " {'abstract': 'In this paper, a machine-to-machine (M2M) networks are arranged hierarchically to support an energy-efficient routing protocol for data transmission from terminal nodes to a sink node via cluster heads in a wireless sensor network (WSN) at perio network congestion caused by heavy M2M traffic is tackled using the load balancing solutions to maintain high levels of network performance. First, a multilevel clustering multiple sinks with IPv6 protocol over low wireless personal area networks is promoted to prolong network lifetime. Second, the enhanced network performance is achieved through non-linear integer-based optimization. A self-organizing cluster head to sink algorithm (SOCHSA) is proposed, hosting discrete particle swarm optimization (DPSO) and genetic algorithm (GA) as evolutionary algorithms to solve the network performance optimization problem. Network Performance is measured based on key performance indicators for load fairness and average residual network energy. The SOCHSA algorithm is tested by two benchmark problems with two and three sinks. DPSO and GA are compared with the exhaustive search algorithm to analyze their performances for each benchmark problem. Both algorithms achieve optimum network performance evaluation values of 108.059 and 108.1686 in the benchmark problems P1 and P2, respectively. Using three sinks under the same simulation settings, the average residual energy is improved by 2% when compared with two sinks. Computational results prove that DPSO outperforms GA regarding complexity and convergence, thus being best suited for a proactive Internet of Things network. The proposed mechanism satisfies different network performance requirements of M2M traffic by instant identification and dynamic rerouting.',\n",
       "  'authors': ['Wasan Twayej ', ' Muhammad Khan ', ' Hamed S. Al-Raweshidy'],\n",
       "  'date': '2017',\n",
       "  'identifier': '2621668204',\n",
       "  'references': ['2104846640',\n",
       "   '2613176274',\n",
       "   '2088087163',\n",
       "   '2120629158',\n",
       "   '1708394971',\n",
       "   '1977752700',\n",
       "   '2049475540',\n",
       "   '2119548540',\n",
       "   '2052115917',\n",
       "   '1968729698'],\n",
       "  'title': 'Network Performance Evaluation of M2M With Self Organizing Cluster Head to Sink Mapping'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Alexander Waibel ',\n",
       "   ' Toshiyuki Hanazawa ',\n",
       "   ' Geoffrey Hinton ',\n",
       "   ' Kiyohiro Shikano ',\n",
       "   ' Kevin J. Lang'],\n",
       "  'date': '1995',\n",
       "  'identifier': '2173629880',\n",
       "  'references': ['2217896605',\n",
       "   '2973127116',\n",
       "   '1606209288',\n",
       "   '2970350205',\n",
       "   '3007502375',\n",
       "   '2019202657',\n",
       "   '2098694627',\n",
       "   '3016138882',\n",
       "   '3115898234'],\n",
       "  'title': 'Phoneme recognition using time-delay neural networks'},\n",
       " {'abstract': 'Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.',\n",
       "  'authors': ['Hugo Larochelle ',\n",
       "   ' Dumitru Erhan ',\n",
       "   ' Aaron Courville ',\n",
       "   ' James Bergstra ',\n",
       "   ' Yoshua Bengio'],\n",
       "  'date': '2007',\n",
       "  'identifier': '1994197834',\n",
       "  'references': ['2153635508',\n",
       "   '2136922672',\n",
       "   '2100495367',\n",
       "   '2116064496',\n",
       "   '2110798204',\n",
       "   '2134557905',\n",
       "   '2147800946',\n",
       "   '2613634265',\n",
       "   '2159737176',\n",
       "   '2124914669'],\n",
       "  'title': 'An empirical evaluation of deep architectures on problems with many factors of variation'},\n",
       " {'abstract': 'Abstract: We propose a novel deep network structure called \"Network In Network\" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.',\n",
       "  'authors': ['Min Lin ', ' Qiang Chen ', ' Shuicheng Yan'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2963911037',\n",
       "  'references': ['2962835968',\n",
       "   '2097117768',\n",
       "   '2117539524',\n",
       "   '1536680647',\n",
       "   '2963446712',\n",
       "   '2963037989',\n",
       "   '2109255472',\n",
       "   '2096733369',\n",
       "   '2302255633'],\n",
       "  'title': 'Network In Network'},\n",
       " {'abstract': 'From the Publisher: Artificial \"neural networks\" are now widely used as flexible models for regression classification applications, but questions remain regarding what these models mean, and how they can safely be used when training data is limited. Bayesian Learning for Neural Networks shows that Bayesian methods allow complex neural network models to be used without fear of the \"overfitting\" that can occur with traditional neural network learning methods. Insight into the nature of these complex Bayesian models is provided by a theoretical investigation of the priors over functions that underlie them. Use of these models in practice is made possible using Markov chain Monte Carlo techniques. Both the theoretical and computational aspects of this work are of wider statistical interest, as they contribute to a better understanding of how Bayesian methods can be applied to complex problems. Presupposing only the basic knowledge of probability and statistics, this book should be of interest to many researchers in statistics, engineering, and artificial intelligence. Software for Unix systems that implements the methods described is freely available over the Internet.',\n",
       "  'authors': ['Geoffrey Hinton ', ' Radford M. Neal'],\n",
       "  'date': '1995',\n",
       "  'identifier': '1567512734',\n",
       "  'references': ['2095705004',\n",
       "   '1904365287',\n",
       "   '2158899491',\n",
       "   '2076063813',\n",
       "   '1746819321',\n",
       "   '2072128103',\n",
       "   '1964357740',\n",
       "   '2964059111',\n",
       "   '1648445109'],\n",
       "  'title': 'Bayesian learning for neural networks'},\n",
       " {'abstract': 'Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics.The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.',\n",
       "  'authors': ['Carl Edward Rasmussen 1', ' Christopher K I Williams 2'],\n",
       "  'date': '2005',\n",
       "  'identifier': '1746819321',\n",
       "  'references': ['2296319761',\n",
       "   '2156909104',\n",
       "   '2148603752',\n",
       "   '2170120409',\n",
       "   '1554663460',\n",
       "   '3023786531',\n",
       "   '2165363188',\n",
       "   '2798909945',\n",
       "   '2117812871',\n",
       "   '2078206416'],\n",
       "  'title': 'Gaussian Processes for Machine Learning'},\n",
       " {'abstract': 'This paper presents an approach to automatically optimizing the retrieval quality of search engines using clickthrough data. Intuitively, a good information retrieval system should present relevant documents high in the ranking, with less relevant documents following below. While previous approaches to learning retrieval functions from examples exist, they typically require training data generated from relevance judgments by experts. This makes them difficult and expensive to apply. The goal of this paper is to develop a method that utilizes clickthrough data for training, namely the query-log of the search engine in connection with the log of links the users clicked on in the presented ranking. Such clickthrough data is available in abundance and can be recorded at very low cost. Taking a Support Vector Machine (SVM) approach, this paper presents a method for learning retrieval functions. From a theoretical perspective, this method is shown to be well-founded in a risk minimization framework. Furthermore, it is shown to be feasible even for large sets of queries and features. The theoretical results are verified in a controlled experiment. It shows that the method can effectively adapt the retrieval function of a meta-search engine to a particular group of users, outperforming Google in terms of retrieval quality after only a couple of hundred training examples.',\n",
       "  'authors': ['Thorsten Joachims'],\n",
       "  'date': '2002',\n",
       "  'identifier': '2047221353',\n",
       "  'references': ['2156909104',\n",
       "   '2148603752',\n",
       "   '2119821739',\n",
       "   '1660390307',\n",
       "   '3021469268',\n",
       "   '2087347434',\n",
       "   '1576520375',\n",
       "   '1978394996',\n",
       "   '2107890099',\n",
       "   '2988119488'],\n",
       "  'title': 'Optimizing search engines using clickthrough data'},\n",
       " {'abstract': 'A discrete cosine transform (DCT) is defined and an algorithm to compute it using the fast Fourier transform is developed. It is shown that the discrete cosine transform can be used in the area of digital processing for the purposes of pattern recognition and Wiener filtering. Its performance is compared with that of a class of orthogonal transforms and is found to compare closely to that of the Karhunen-Loeve transform, which is known to be optimal. The performances of the Karhunen-Loeve and discrete cosine transforms are also found to compare closely with respect to the rate-distortion criterion.',\n",
       "  'authors': ['N. Ahmed 1', ' T. Natarajan 1', ' K.R. Rao 2'],\n",
       "  'date': '1974',\n",
       "  'identifier': '2031614119',\n",
       "  'references': ['2161816980',\n",
       "   '2000108174',\n",
       "   '2142930126',\n",
       "   '2069800184',\n",
       "   '2088534753',\n",
       "   '2069267080'],\n",
       "  'title': 'Discrete Cosine Transform'},\n",
       " {'abstract': 'Introduction Preliminaries and notation The what, why, and how of wavelets The continuous wavelet transform Discrete wavelet transforms: Frames Time-frequency density and orthonormal bases Orthonormal bases of wavelets and multiresolutional analysis Orthonormal bases of compactly supported wavelets More about the regularity of compactly supported wavelets Symmetry for compactly supported wavelet bases Characterization of functional spaces by means of wavelets Generalizations and tricks for orthonormal wavelet bases References Indexes.',\n",
       "  'authors': ['Ingrid Daubechies'],\n",
       "  'date': '1992',\n",
       "  'identifier': '2062024414',\n",
       "  'references': ['2296616510',\n",
       "   '2078204800',\n",
       "   '2158940042',\n",
       "   '2099641086',\n",
       "   '2130660124',\n",
       "   '2151693816',\n",
       "   '2034139177',\n",
       "   '2186428165',\n",
       "   '2106002835'],\n",
       "  'title': 'Ten Lectures on Wavelets'},\n",
       " {'abstract': 'We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, non-linear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low-energy states (‘annealing’), or what is the same thing, the most probable states under the Gib...',\n",
       "  'authors': ['Stuart Geman 1', ' Donald Geman 2'],\n",
       "  'date': '1993',\n",
       "  'identifier': '1997063559',\n",
       "  'references': ['2581275558',\n",
       "   '2150060382',\n",
       "   '1622620102',\n",
       "   '2154061444',\n",
       "   '2114220616',\n",
       "   '2065301447',\n",
       "   '1979622972',\n",
       "   '2107792892',\n",
       "   '2014208555',\n",
       "   '1567885833'],\n",
       "  'title': 'Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images*'},\n",
       " {'abstract': 'We present a polynomial-time randomized algorithm for estimating the permanent of an arbitrary n × n matrix with nonnegative entries. This algorithm---technically a \"fully-polynomial randomized approximation scheme\"---computes an approximation that is, with high probability, within arbitrarily small specified relative error of the true value of the permanent.',\n",
       "  'authors': ['Mark Jerrum 1', ' Alistair Sinclair 2', ' Eric Vigoda 3'],\n",
       "  'date': '2004',\n",
       "  'identifier': '2161611531',\n",
       "  'references': ['2295428206',\n",
       "   '2068871408',\n",
       "   '2330548796',\n",
       "   '2043694962',\n",
       "   '1607251184',\n",
       "   '2106285343',\n",
       "   '634962210',\n",
       "   '2006912660',\n",
       "   '1976418263',\n",
       "   '3111890340'],\n",
       "  'title': 'A polynomial-time approximation algorithm for the permanent of a matrix with nonnegative entries'},\n",
       " {'abstract': 'Neural Machine Translation (NMT) is a new approach to machine translation that has shown promising results that are comparable to traditional approaches. A significant weakness in conventional NMT systems is their inability to correctly translate very rare words: end-to-end NMTs tend to have relatively small vocabularies with a single unk symbol that represents every possible out-of-vocabulary (OOV) word. In this paper, we propose and implement an effective technique to address this problem. We train an NMT system on data that is augmented by the output of a word alignment algorithm, allowing the NMT system to emit, for each OOV word in the target sentence, the position of its corresponding word in the source sentence. This information is later utilized in a post-processing step that translates every OOV word using a dictionary. Our experiments on the WMT’14 English to French translation task show that this method provides a substantial improvement of up to 2.8 BLEU points over an equivalent NMT system that does not use this technique. With 37.5 BLEU points, our NMT system is the first to surpass the best result achieved on a WMT’14 contest task.',\n",
       "  'authors': ['Thang Luong 1',\n",
       "   ' Ilya Sutskever 2',\n",
       "   ' Quoc Le 2',\n",
       "   ' Oriol Vinyals 2',\n",
       "   ' Wojciech Zaremba 3'],\n",
       "  'date': '2015',\n",
       "  'identifier': '2118434577',\n",
       "  'references': ['2964308564',\n",
       "   '2130942839',\n",
       "   '2157331557',\n",
       "   '2101105183',\n",
       "   '1753482797',\n",
       "   '1810943226',\n",
       "   '2124807415',\n",
       "   '2153653739',\n",
       "   '1815076433',\n",
       "   '2100664567'],\n",
       "  'title': 'Addressing the Rare Word Problem in Neural Machine Translation'},\n",
       " {'abstract': 'We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without handengineering our loss functions either.',\n",
       "  'authors': ['Phillip Isola ',\n",
       "   ' Jun-Yan Zhu ',\n",
       "   ' Tinghui Zhou ',\n",
       "   ' Alexei A. Efros'],\n",
       "  'date': '2017',\n",
       "  'identifier': '2963073614',\n",
       "  'references': ['2964121744',\n",
       "   '1836465849',\n",
       "   '2117539524',\n",
       "   '1901129140',\n",
       "   '1903029394',\n",
       "   '2099471712',\n",
       "   '2133665775',\n",
       "   '2100495367',\n",
       "   '2963684088',\n",
       "   '2340897893'],\n",
       "  'title': 'Image-to-Image Translation with Conditional Adversarial Networks'},\n",
       " {'abstract': 'A major goal of research on networks of neuron-like processing units is to discover efficient learning procedures that allow these networks to construct complex internal representations of their environment. The learning procedures must be capable of modifying the connection strengths in such a way that internal units which are not part of the input or output come to represent important features of the task domain. Several interesting gradient-descent procedures have recently been discovered. Each connection computes the derivative, with respect to the connection strength, of a global measure of the error in the performance of the network. The strength is then adjusted in the direction that decreases the error. These relatively simple, gradient-descent learning procedures work well for small tasks and the new challenge is to find ways of improving their convergence rate and their generalization abilities so that they can be applied to larger, more realistic tasks.',\n",
       "  'authors': ['Geoffrey E. Hinton'],\n",
       "  'date': '1990',\n",
       "  'identifier': '2176028050',\n",
       "  'references': ['1497256448',\n",
       "   '2581275558',\n",
       "   '2154642048',\n",
       "   '1498436455',\n",
       "   '1997063559',\n",
       "   '2049633694',\n",
       "   '2293063825',\n",
       "   '2895674046',\n",
       "   '1597286183',\n",
       "   '22297218'],\n",
       "  'title': 'Connectionist learning procedures'},\n",
       " {'abstract': 'The rapid uptake of mobile devices and the rising popularity of mobile applications and services pose unprecedented demands on mobile and wireless networking infrastructure. Upcoming 5G systems are evolving to support exploding mobile traffic volumes, real-time extraction of fine-grained analytics, and agile management of network resources, so as to maximize user experience. Fulfilling these tasks is challenging, as mobile environments are increasingly complex, heterogeneous, and evolving. One potential solution is to resort to advanced machine learning techniques, in order to help manage the rise in data volumes and algorithm-driven applications. The recent success of deep learning underpins new and powerful tools that tackle problems in this space. In this paper, we bridge the gap between deep learning and mobile and wireless networking research, by presenting a comprehensive survey of the crossovers between the two areas. We first briefly introduce essential background and state-of-the-art in deep learning techniques with potential applications to networking. We then discuss several techniques and platforms that facilitate the efficient deployment of deep learning onto mobile systems. Subsequently, we provide an encyclopedic review of mobile and wireless networking research based on deep learning, which we categorize by different domains. Drawing from our experience, we discuss how to tailor deep learning to mobile environments. We complete this survey by pinpointing current challenges and open future directions for research.',\n",
       "  'authors': ['Chaoyun Zhang 1', ' Paul Patras 1', ' Hamed Haddadi 2'],\n",
       "  'date': '2019',\n",
       "  'identifier': '2962883549',\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '2964121744',\n",
       "   '2097117768',\n",
       "   '2919115771',\n",
       "   '1836465849',\n",
       "   '2117539524',\n",
       "   '2099471712',\n",
       "   '2145339207',\n",
       "   '2130942839'],\n",
       "  'title': 'Deep Learning in Mobile and Wireless Networking: A Survey'},\n",
       " {'abstract': 'There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.',\n",
       "  'authors': ['Razvan Pascanu 1', ' Tomas Mikolov 2', ' Yoshua Bengio 1'],\n",
       "  'date': '2013',\n",
       "  'identifier': '1815076433',\n",
       "  'references': ['2146502635',\n",
       "   '2064675550',\n",
       "   '1498436455',\n",
       "   '1606347560',\n",
       "   '196214544',\n",
       "   '2110485445',\n",
       "   '2171865010',\n",
       "   '2118706537',\n",
       "   '2122585011',\n",
       "   '2107878631'],\n",
       "  'title': 'On the difficulty of training recurrent neural networks'},\n",
       " {'abstract': 'Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent \"High Throughput\" methods achieve surprising success--they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.',\n",
       "  'authors': ['James Bergstra ', ' Yoshua Bengio'],\n",
       "  'date': '2012',\n",
       "  'identifier': '2097998348',\n",
       "  'references': ['2153635508',\n",
       "   '2136922672',\n",
       "   '2310919327',\n",
       "   '1746819321',\n",
       "   '1554663460',\n",
       "   '2072128103',\n",
       "   '1533861849',\n",
       "   '2025768430',\n",
       "   '2581275558',\n",
       "   '44815768'],\n",
       "  'title': 'Random search for hyper-parameter optimization'},\n",
       " {'abstract': 'In this tutorial we give an overview of the basic ideas underlying Support Vector (SV) machines for function estimation. Furthermore, we include a summary of currently used algorithms for training SV machines, covering both the quadratic (or convex) programming part and advanced methods for dealing with large datasets. Finally, we mention some modifications and extensions that have been applied to the standard SV algorithm, and discuss the aspect of regularization from a SV perspective.',\n",
       "  'authors': ['Alex J. Smola 1', ' Bernhard Schölkopf 2'],\n",
       "  'date': '2004',\n",
       "  'identifier': '1964357740',\n",
       "  'references': ['2153635508',\n",
       "   '2156909104',\n",
       "   '2148603752',\n",
       "   '2124776405',\n",
       "   '2331432542',\n",
       "   '1554663460',\n",
       "   '2119821739',\n",
       "   '2139212933',\n",
       "   '1563088657',\n",
       "   '3023786531'],\n",
       "  'title': 'A tutorial on support vector regression'},\n",
       " {'abstract': 'Can machine learning deliver AI? Theoretical results, inspiration from the brain and cognition, as well as machine learning experiments suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one would need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers, graphical models with many levels of latent variables, or in complicated propositional formulae re-using many sub-formulae. Each level of the architecture represents features at a different level of abstraction, defined as a composition of lower-level features. Searching the parameter space of deep architectures is a difficult task, but new algorithms have been discovered and a new sub-area has emerged in the machine learning community since 2006, following these discoveries. Learning algorithms such as those for Deep Belief Networks and other related unsupervised learning algorithms have recently been proposed to train deep architectures, yielding exciting results and beating the state-of-the-art in certain areas. Learning Deep Architectures for AI discusses the motivations for and principles of learning algorithms for deep architectures. By analyzing and comparing recent results with different learning algorithms for deep architectures, explanations for their success are proposed and discussed, highlighting challenges and suggesting avenues for future explorations in this area.',\n",
       "  'authors': ['Yoshua Bengio'],\n",
       "  'date': '2009',\n",
       "  'identifier': '2072128103',\n",
       "  'references': ['2156909104',\n",
       "   '2911964244',\n",
       "   '2296616510',\n",
       "   '2136922672',\n",
       "   '2100495367',\n",
       "   '2310919327',\n",
       "   '2187089797',\n",
       "   '2129131372',\n",
       "   '2119821739',\n",
       "   '2053186076'],\n",
       "  'title': 'Learning Deep Architectures for AI'},\n",
       " {'abstract': \"We investigate the application of Support Vector Machines (SVMs) in computer vision. SVM is a learning technique developed by V. Vapnik and his team (AT&T Bell Labs., 1985) that can be seen as a new method for training polynomial, neural network, or Radial Basis Functions classifiers. The decision surfaces are found by solving a linearly constrained quadratic programming problem. This optimization problem is challenging because the quadratic form is completely dense and the memory requirements grow with the square of the number of data points. We present a decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of optimality conditions which are used both to generate improved iterative values, and also establish the stopping criteria for the algorithm. We present experimental results of our implementation of SVM, and demonstrate the feasibility of our approach on a face detection problem that involves a data set of 50,000 data points.\",\n",
       "  'authors': ['E. Osuna ', ' R. Freund ', ' F. Girosit'],\n",
       "  'date': '1997',\n",
       "  'identifier': '2124351082',\n",
       "  'references': ['2156909104',\n",
       "   '2119821739',\n",
       "   '2087347434',\n",
       "   '2159686933',\n",
       "   '26816478',\n",
       "   '2159173611',\n",
       "   '2137346077',\n",
       "   '2084844503',\n",
       "   '2056695679',\n",
       "   '2125713050'],\n",
       "  'title': 'Training support vector machines: an application to face detection'},\n",
       " {'abstract': 'From the Publisher: This book represents the most comprehensive treatment available of neural networks from an engineering perspective. Thorough, well-organized, and completely up to date, it examines all the important aspects of this emerging technology, including the learning process, back-propagation learning, radial-basis function networks, self-organizing systems, modular networks, temporal processing and neurodynamics, and VLSI implementation of neural networks. Written in a concise and fluid manner, by a foremost engineering textbook author, to make the material more accessible, this book is ideal for professional engineers and graduate students entering this exciting field. Computer experiments, problems, worked examples, a bibliography, photographs, and illustrations reinforce key concepts.',\n",
       "  'authors': ['Simon Haykin'],\n",
       "  'date': '1998',\n",
       "  'identifier': '2124776405',\n",
       "  'references': ['2071707134',\n",
       "   '2111072639',\n",
       "   '1964357740',\n",
       "   '2026131661',\n",
       "   '2097308346',\n",
       "   '2118978333',\n",
       "   '2153233077',\n",
       "   '2132549764',\n",
       "   '3110653090'],\n",
       "  'title': 'Neural Networks: A Comprehensive Foundation'},\n",
       " {'abstract': 'We present a neural network-based upright frontal face detection system. A retinally connected neural network examines small windows of an image and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We present a straightforward procedure for aligning positive face examples for training. To collect negative examples, we use a bootstrap algorithm, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting nonface training examples, which must be chosen to span the entire space of nonface images. Simple heuristics, such as using the fact that faces rarely overlap in images, can further improve the accuracy. Comparisons with several other state-of-the-art face detection systems are presented, showing that our system has comparable performance in terms of detection and false-positive rates.',\n",
       "  'authors': ['H.A. Rowley 1', ' S. Baluja 2', ' T. Kanade 1'],\n",
       "  'date': '1998',\n",
       "  'identifier': '2217896605',\n",
       "  'references': ['2139212933',\n",
       "   '2313307644',\n",
       "   '2133671888',\n",
       "   '2124351082',\n",
       "   '2147800946',\n",
       "   '2098947662',\n",
       "   '1997011019',\n",
       "   '2173629880',\n",
       "   '2042371054',\n",
       "   '2159173611'],\n",
       "  'title': 'Neural network-based face detection'},\n",
       " {'abstract': 'There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .',\n",
       "  'authors': ['Olaf Ronneberger ', ' Philipp Fischer ', ' Thomas Brox'],\n",
       "  'date': '2015',\n",
       "  'identifier': '1901129140',\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '1903029394',\n",
       "   '2155893237',\n",
       "   '1677182931',\n",
       "   '1948751323',\n",
       "   '2167510172',\n",
       "   '1893585201',\n",
       "   '2148349024',\n",
       "   '2147800946'],\n",
       "  'title': 'U-Net: Convolutional Networks for Biomedical Image Segmentation'},\n",
       " {'abstract': 'Probabilistic Latent Semantic Indexing is a novel approach to automated document indexing which is based on a statistical latent class model for factor analysis of count data. Fitted from a training corpus of text documents by a generalization of the Expectation Maximization algorithm, the utilized model is able to deal with domain{specific synonymy as well as with polysemous words. In contrast to standard Latent Semantic Indexing (LSI) by Singular Value Decomposition, the probabilistic variant has a solid statistical foundation and defines a proper generative data model. Retrieval experiments on a number of test collections indicate substantial performance gains over direct term matching methods as well as over LSI. In particular, the combination of models with different dimensionalities has proven to be advantageous.',\n",
       "  'authors': ['Thomas Hofmann'],\n",
       "  'date': '1999',\n",
       "  'identifier': '2107743791',\n",
       "  'references': ['2147152072',\n",
       "   '2049633694',\n",
       "   '1956559956',\n",
       "   '1612003148',\n",
       "   '2567948266',\n",
       "   '2127314673',\n",
       "   '1718512272',\n",
       "   '2140842551',\n",
       "   '2143144851',\n",
       "   '2063089147'],\n",
       "  'title': 'Probabilistic latent semantic indexing'},\n",
       " {'abstract': 'The classification of large dimensional data sets arising from the merging of remote sensing data with more traditional forms of ancillary data causes a significant computational problem. Decision tree classification is a popular approach to the problem. This type of classifier is characterized by the property that samples are subjected to a sequence of decision rules before they are assigned to a unique class. If a decision tree classifier is well designed, the result in many cases is a classification scheme which is accurate, flexible, and computationally efficient. This correspondence provides an automated technique for effective decision tree design which relies only on a priori statistics. This procedure utilizes canonical transforms and Bayes table look-up decision rules. An optimal design at each node is derived based on the associated decision table. A procedure for computing the global probability of correct classification is also provided. An example is given in which class statistics obtained from an actual Landsat scene are used as input to the program. The resulting decision tree design has an associated probability of correct classification of 0.75 compared to the theoretically optimum 0.79 probability of correct classification associated with a full dimensional Bayes classifier. Recommendations for future research are included.',\n",
       "  'authors': ['Peter Argentiero 1', ' Roland Chin 2', ' Paul Beaudet 2'],\n",
       "  'date': '1982',\n",
       "  'identifier': '2022167939',\n",
       "  'references': ['2098057602',\n",
       "   '2000134108',\n",
       "   '2024941397',\n",
       "   '2060162865',\n",
       "   '2086294262',\n",
       "   '2114534674',\n",
       "   '2148877046',\n",
       "   '2095444925',\n",
       "   '1540630610'],\n",
       "  'title': 'An Automated Approach to the Design of Decision Tree Classifiers'},\n",
       " {'abstract': 'From the Publisher: This is the first comprehensive treatment of feed-forward neural networks from the perspective of statistical pattern recognition. After introducing the basic concepts, the book examines techniques for modelling probability density functions and the properties and merits of the multi-layer perceptron and radial basis function network models. Also covered are various forms of error functions, principal algorithms for error function minimalization, learning and generalization in neural networks, and Bayesian techniques and their applications. Designed as a text, with over 100 exercises, this fully up-to-date work will benefit anyone involved in the fields of neural computation and pattern recognition.',\n",
       "  'authors': ['Christopher M. Bishop'],\n",
       "  'date': '1995',\n",
       "  'identifier': '1554663460',\n",
       "  'references': [],\n",
       "  'title': 'Neural networks for pattern recognition'},\n",
       " {'abstract': 'Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that capture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1-regularized least squares problem and an L2-constrained least squares problem. We propose novel algorithms to solve both of these optimization problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons.',\n",
       "  'authors': ['Honglak Lee ',\n",
       "   ' Alexis Battle ',\n",
       "   ' Rajat Raina ',\n",
       "   ' Andrew Y. Ng'],\n",
       "  'date': '2006',\n",
       "  'identifier': '2113606819',\n",
       "  'references': ['2063978378',\n",
       "   '2078204800',\n",
       "   '2145889472',\n",
       "   '2105464873',\n",
       "   '2140499889',\n",
       "   '2004915807',\n",
       "   '2101933716',\n",
       "   '16591383',\n",
       "   '2074376560',\n",
       "   '2146672645'],\n",
       "  'title': 'Efficient sparse coding algorithms'},\n",
       " {'abstract': \"Similarity breeds connection. This principle—the homophily principle—structures network ties of every type, including marriage, friendship, work, advice, support, information transfer, exchange, comembership, and other types of relationship. The result is that people's personal networks are homogeneous with regard to many sociodemographic, behavioral, and intrapersonal characteristics. Homophily limits people's social worlds in a way that has powerful implications for the information they receive, the attitudes they form, and the interactions they experience. Homophily in race and ethnicity creates the strongest divides in our personal environments, with age, religion, education, occupation, and gender following in roughly that order. Geographic propinquity, families, organizations, and isomorphic positions in social systems all create contexts in which homophilous relations form. Ties between nonsimilar individuals also dissolve at a higher rate, which sets the stage for the formation of niches (localize...\",\n",
       "  'authors': ['Miller McPherson 1', ' Lynn Smith-Lovin 2', ' James M Cook 3'],\n",
       "  'date': '2001',\n",
       "  'identifier': '2130354913',\n",
       "  'references': ['2051142925',\n",
       "   '2143969326',\n",
       "   '2136048895',\n",
       "   '2065859528',\n",
       "   '2323236851',\n",
       "   '1997052091',\n",
       "   '2105275777',\n",
       "   '1967272740',\n",
       "   '1992496496',\n",
       "   '2046601115'],\n",
       "  'title': 'Birds of a Feather: Homophily in Social Networks'},\n",
       " {'abstract': 'The primary goal of pattern recognition is supervised or unsupervised classification. Among the various frameworks in which pattern recognition has been traditionally formulated, the statistical approach has been most intensively studied and used in practice. More recently, neural network techniques and methods imported from statistical learning theory have been receiving increasing attention. The design of a recognition system requires careful attention to the following issues: definition of pattern classes, sensing environment, pattern representation, feature extraction and selection, cluster analysis, classifier design and learning, selection of training and test samples, and performance evaluation. In spite of almost 50 years of research and development in this field, the general problem of recognizing complex patterns with arbitrary orientation, location, and scale remains unsolved. New and emerging applications, such as data mining, web searching, retrieval of multimedia data, face recognition, and cursive handwriting recognition, require robust and efficient pattern recognition techniques. The objective of this review paper is to summarize and compare some of the well-known methods used in various stages of a pattern recognition system and identify research topics and applications which are at the forefront of this exciting and challenging field.',\n",
       "  'authors': ['A.K. Jain 1', ' R.P.W. Duin 2', ' Jianchang Mao 3'],\n",
       "  'date': '2000',\n",
       "  'identifier': '2132549764',\n",
       "  'references': ['2156909104',\n",
       "   '2148603752',\n",
       "   '2124776405',\n",
       "   '1554663460',\n",
       "   '2139212933',\n",
       "   '1548802052',\n",
       "   '2912934387',\n",
       "   '2125838338',\n",
       "   '1679913846',\n",
       "   '2125055259'],\n",
       "  'title': 'Statistical pattern recognition: a review'},\n",
       " {'abstract': \"Subband coding has become quite popular for the source encoding of speech. This paper presents a simple yet efficient extension of this concept to the source coding of images. We specify the constraints for a set of two-dimensional quadrature mirror filters (QMF's) for a particular frequency-domain partition, and show that these constraints are satisfied by a separable combination of one-dimensional QMF's. Bits are then optimally allocated among the subbands to minimize the mean-squared error for DPCM coding of the subbands. Also, an adaptive technique is developed to allocate the bits within each subband by means of a local variance mask. Optimum quantization is employed with quantizers matched to the Laplacian distribution. Subband coded images are presented along with their signal-to-noise ratios (SNR's). The SNR performance of the subband coder is compared to that of the adaptive discrete cosine transform (DCT), vector quantization, and differential vector quantization for bit rates of 0.67, 1.0, and 2.0 bits per pixel for 256 × 256 monochrome images. The adaptive subband coder has the best SNR performance.\",\n",
       "  'authors': ['J. Woods ', \" S. O'Neil\"],\n",
       "  'date': '1986',\n",
       "  'identifier': '2139797453',\n",
       "  'references': ['2103504761',\n",
       "   '2186435531',\n",
       "   '2153639720',\n",
       "   '1985814307',\n",
       "   '1690240707',\n",
       "   '1627215994',\n",
       "   '2118274709',\n",
       "   '2315251844'],\n",
       "  'title': 'Subband coding of images'},\n",
       " {'abstract': 'We describe approximate digital implementations of two new mathematical transforms, namely, the ridgelet transform and the curvelet transform. Our implementations offer exact reconstruction, stability against perturbations, ease of implementation, and low computational complexity. A central tool is Fourier-domain computation of an approximate digital Radon transform. We introduce a very simple interpolation in the Fourier space which takes Cartesian samples and yields samples on a rectopolar grid, which is a pseudo-polar sampling set based on a concentric squares geometry. Despite the crudeness of our interpolation, the visual performance is surprisingly good. Our ridgelet transform applies to the Radon transform a special overcomplete wavelet pyramid whose wavelets have compact support in the frequency domain. Our curvelet transform uses our ridgelet transform as a component step, and implements curvelet subbands using a filter bank of a/spl grave/ trous wavelet filters. Our philosophy throughout is that transforms should be overcomplete, rather than critically sampled. We apply these digital transforms to the denoising of some standard images embedded in white noise. In the tests reported here, simple thresholding of the curvelet coefficients is very competitive with \"state of the art\" techniques based on wavelets, including thresholding of decimated or undecimated wavelet transforms and also including tree-based Bayesian posterior mean methods. Moreover, the curvelet reconstructions exhibit higher perceptual quality than wavelet-based reconstructions, offering visually sharper images and, in particular, higher quality recovery of edges and of faint linear and curvilinear features. Existing theory for curvelet and ridgelet transforms suggests that these new approaches can outperform wavelet methods in certain image reconstruction problems. The empirical results reported here are in encouraging agreement.',\n",
       "  'authors': ['Jean-Luc Starck 1', ' E.J. Candes 2', ' D.L. Donoho 1'],\n",
       "  'date': '2002',\n",
       "  'identifier': '2132680427',\n",
       "  'references': ['2134929491',\n",
       "   '1485280399',\n",
       "   '59771946',\n",
       "   '2109504624',\n",
       "   '2107790757',\n",
       "   '2066462711',\n",
       "   '2035737465',\n",
       "   '2033367330',\n",
       "   '2031299600',\n",
       "   '2158576618'],\n",
       "  'title': 'The curvelet transform for image denoising'},\n",
       " {'abstract': '',\n",
       "  'authors': ['A. A. Mullin ', ' Frank Rosenblatt'],\n",
       "  'date': '1962',\n",
       "  'identifier': '2322002063',\n",
       "  'references': ['2076063813',\n",
       "   '2119821739',\n",
       "   '1498436455',\n",
       "   '1570963478',\n",
       "   '2144499799',\n",
       "   '1873332500',\n",
       "   '2155653793',\n",
       "   '2148138104',\n",
       "   '2147169507'],\n",
       "  'title': 'Principles of neurodynamics'},\n",
       " {'abstract': 'Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, aaa, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.',\n",
       "  'authors': ['Leo Breiman'],\n",
       "  'date': '2001',\n",
       "  'identifier': '2911964244',\n",
       "  'references': ['2912934387',\n",
       "   '2112076978',\n",
       "   '1975846642',\n",
       "   '2152761983',\n",
       "   '2113242816',\n",
       "   '1605688901',\n",
       "   '2120240539',\n",
       "   '2099968818',\n",
       "   '2067885219',\n",
       "   '1580948147'],\n",
       "  'title': 'Random Forests'},\n",
       " {'abstract': 'Content analysis is a family of systematic, rule-guided techniques used to analyze the informational contents of textual data (Mayring, 2000). It is used frequently in nursing research, and is rapidly becoming more prominent in the medical and bioethics literature. There are several types of content analysis including quantitative and qualitative methods all sharing the central feature of systematically categorizing textual data in order to make sense of it (Miles & Huberman, 1994). They differ, however, in the ways they generate categories and apply them to the data, and how they analyze the resulting data. In this chapter, we describe a type of qualitative content analysis in which categories are largely derived from the data, applied to the data through close reading, and analyzed solely qualitatively. The generation and application of categories that we describe can also be used in studies that include quantitative analysis.',\n",
       "  'authors': ['Jane Forman ', ' Laura Damschroder'],\n",
       "  'date': '2007',\n",
       "  'identifier': '2498731006',\n",
       "  'references': ['2142225512',\n",
       "   '2159165123',\n",
       "   '2910572681',\n",
       "   '2100877102',\n",
       "   '2160375265',\n",
       "   '1996600674',\n",
       "   '2140472019',\n",
       "   '2049133117',\n",
       "   '2163451080',\n",
       "   '2165407725'],\n",
       "  'title': 'Qualitative Content Analysis'},\n",
       " {'abstract': 'Non-negative matrix factorization (NMF) has previously been shown to be a useful decomposition for multivariate data. Two different multiplicative algorithms for NMF are analyzed. They differ only slightly in the multiplicative factor used in the update rules. One algorithm can be shown to minimize the conventional least squares error while the other minimizes the generalized Kullback-Leibler divergence. The monotonic convergence of both algorithms can be proven using an auxiliary function analogous to that used for proving convergence of the Expectation-Maximization algorithm. The algorithms can also be interpreted as diagonally rescaled gradient descent, where the rescaling factor is optimally chosen to ensure convergence.',\n",
       "  'authors': ['Daniel D. Lee 1', ' H. Sebastian Seung 2'],\n",
       "  'date': '2000',\n",
       "  'identifier': '2135029798',\n",
       "  'references': ['2148694408',\n",
       "   '2138451337',\n",
       "   '2313307644',\n",
       "   '1902027874',\n",
       "   '2049633694',\n",
       "   '3004157836',\n",
       "   '1634005169',\n",
       "   '2180838288',\n",
       "   '2069317438',\n",
       "   '2033482494'],\n",
       "  'title': 'Algorithms for Non-negative Matrix Factorization'},\n",
       " {'abstract': 'The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries---stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB). Basis pursuit (BP) is a principle for decomposing a signal into an \"optimal\"\\' superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, abstract harmonic analysis, total variation denoising, and multiscale edge denoising. BP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear and quadratic programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver.',\n",
       "  'authors': ['Scott Shaobing Chen 1',\n",
       "   ' David L. Donoho 2',\n",
       "   ' Michael A. Saunders 2'],\n",
       "  'date': '2001',\n",
       "  'identifier': '2078204800',\n",
       "  'references': ['2062024414',\n",
       "   '2798909945',\n",
       "   '2146842127',\n",
       "   '2099641086',\n",
       "   '2151693816',\n",
       "   '2103559027',\n",
       "   '2152328854',\n",
       "   '2156447271',\n",
       "   '2611147814',\n",
       "   '2128659236'],\n",
       "  'title': 'Atomic Decomposition by Basis Pursuit'},\n",
       " {'abstract': 'Several extensions to evolutionary algorithms (EAs) and particle swarm optimization (PSO) have been suggested during the last decades offering improved performance on selected benchmark problems. Recently, another search heuristic termed differential evolution (DE) has shown superior performance in several real-world applications. In this paper, we evaluate the performance of DE, PSO, and EAs regarding their general applicability as numerical optimization techniques. The comparison is performed on a suite of 34 widely used benchmark problems. The results from our study show that DE generally outperforms the other algorithms. However, on two noisy functions, both DE and PSO were outperformed by the EA.',\n",
       "  'authors': ['J. Vesterstrom ', ' R. Thomsen'],\n",
       "  'date': '2004',\n",
       "  'identifier': '1708394971',\n",
       "  'references': ['2152195021',\n",
       "   '1497256448',\n",
       "   '1595159159',\n",
       "   '2165299997',\n",
       "   '1998674867',\n",
       "   '1560921017',\n",
       "   '177635913',\n",
       "   '2074916243',\n",
       "   '2165713450',\n",
       "   '1524363999'],\n",
       "  'title': 'A comparative study of differential evolution, particle swarm optimization, and evolutionary algorithms on numerical benchmark problems'},\n",
       " {'abstract': 'The Viterbi algorithm (VA) is a recursive optimal solution to the problem of estimating the state sequence of a discrete-time finite-state Markov process observed in memoryless noise. Many problems in areas such as digital communications can be cast in this form. This paper gives a tutorial exposition of the algorithm and of how it is implemented and analyzed. Applications to date are reviewed. Increasing use of the algorithm in a widening variety of areas is foreseen.',\n",
       "  'authors': ['Jr. G.D. Forney'],\n",
       "  'date': '1973',\n",
       "  'identifier': '2142384583',\n",
       "  'references': ['1562979145',\n",
       "   '2045407304',\n",
       "   '2131086249',\n",
       "   '1991133427',\n",
       "   '2122683098',\n",
       "   '2106185713',\n",
       "   '2153810958',\n",
       "   '2035227369',\n",
       "   '2161457263',\n",
       "   '2134360027'],\n",
       "  'title': 'The viterbi algorithm'},\n",
       " {'abstract': 'The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure.',\n",
       "  'authors': ['David H. Ackley 1',\n",
       "   ' Geoffrey E. Hinton 1',\n",
       "   ' Terrence J. Sejnowski 2'],\n",
       "  'date': '1988',\n",
       "  'identifier': '1507849272',\n",
       "  'references': ['2581275558',\n",
       "   '1997063559',\n",
       "   '2293063825',\n",
       "   '2112325651',\n",
       "   '2056760934',\n",
       "   '2157629899',\n",
       "   '2098205603',\n",
       "   '1597474747',\n",
       "   '2414854470',\n",
       "   '807785616'],\n",
       "  'title': 'A learning algorithm for Boltzmann machines'},\n",
       " {'abstract': 'This paper presents the top 10 data mining algorithms identified by the IEEE International Conference on Data Mining (ICDM) in December 2006: C4.5, k-Means, SVM, Apriori, EM, PageRank, AdaBoost, kNN, Naive Bayes, and CART. These top 10 algorithms are among the most influential data mining algorithms in the research community. With each algorithm, we provide a description of the algorithm, discuss the impact of the algorithm, and review current and further research on the algorithm. These 10 algorithms cover classification,',\n",
       "  'authors': ['Xindong Wu 1',\n",
       "   ' Vipin Kumar 2',\n",
       "   ' J. Ross Quinlan 3',\n",
       "   ' Joydeep Ghosh 4',\n",
       "   ' Qiang Yang 5',\n",
       "   ' Hiroshi Motoda 6',\n",
       "   ' Geoffrey J. McLachlan 7',\n",
       "   ' Angus Ng 8',\n",
       "   ' Bing Liu 9',\n",
       "   ' Philip S. Yu 10',\n",
       "   ' Zhi-Hua Zhou 11',\n",
       "   ' Michael Steinbach 2',\n",
       "   ' David J. Hand 12',\n",
       "   ' Dan Steinberg 13'],\n",
       "  'date': '2007',\n",
       "  'identifier': '2142827986',\n",
       "  'references': ['2156909104',\n",
       "   '2164598857',\n",
       "   '3013264884',\n",
       "   '1565377632',\n",
       "   '1484413656',\n",
       "   '1988790447',\n",
       "   '2138621811',\n",
       "   '1854214752',\n",
       "   '3023786531',\n",
       "   '2125055259'],\n",
       "  'title': 'Top 10 algorithms in data mining'},\n",
       " {'abstract': 'A metaheuristic is a high-level problem independent algorithmic framework that provides a set of guidelines or strategies to develop heuristic optimization algorithms. Metaheuristic algorithms attempt to find the best solution out of all possible solutions of an optimization problem. A very active area of research is the design of nature-inspired metaheuristics. Nature acts as a source of concepts, mechanisms and principles for designing of artificial computing systems to deal with complex computational problems. In this paper, a new metaheuristic algorithm, inspired by the behavior of emperor penguins which is called Emperor Penguins Colony (EPC), is proposed. This algorithm is controlled by the body heat radiation of the penguins and their spiral-like movement in their colony. The proposed algorithm is compared with eight developed metaheuristic algorithms. Ten benchmark test functions are applied to all algorithms. The results of the experiments to find the optimal result, show that the proposed algorithm is better than other metaheuristic algorithms.',\n",
       "  'authors': ['Sasan Harifi ',\n",
       "   ' Madjid Khalilian ',\n",
       "   ' Javad Mohammadzadeh ',\n",
       "   ' Sadoullah Ebrahimnejad'],\n",
       "  'date': '2019',\n",
       "  'identifier': '2915435825',\n",
       "  'references': ['2613176274',\n",
       "   '1595159159',\n",
       "   '2061438946',\n",
       "   '2581275558',\n",
       "   '2143560894',\n",
       "   '1976744965',\n",
       "   '2290883490',\n",
       "   '2963103847',\n",
       "   '1523741643',\n",
       "   '1993885071'],\n",
       "  'title': 'Emperor Penguins Colony: a new metaheuristic algorithm for optimization'},\n",
       " {'abstract': 'In the Fall of 2000, we collected a database of more than 40,000 facial images of 68 people. Using the Carnegie Mellon University 3D Room, we imaged each person across 13 different poses, under 43 different illumination conditions, and with four different expressions. We call this the CMU pose, illumination, and expression (PIE) database. We describe the imaging hardware, the collection procedure, the organization of the images, several possible uses, and how to obtain the database.',\n",
       "  'authors': ['T. Sim 1', ' S. Baker 2', ' M. Bsat 2'],\n",
       "  'date': '2003',\n",
       "  'identifier': '2006793117',\n",
       "  'references': ['2155759509',\n",
       "   '2118774738',\n",
       "   '2102760078',\n",
       "   '2120420721',\n",
       "   '2110822444',\n",
       "   '2121114545',\n",
       "   '2106143125',\n",
       "   '2141503314',\n",
       "   '2144855601'],\n",
       "  'title': 'The CMU pose, illumination, and expression database'},\n",
       " {'abstract': 'Automatic grouping and segmentation of images remains a challenging problem in computer vision. Recently, a number of authors have demonstrated good performance on this task using methods that are based on eigenvectors of the affinity matrix. These approaches are extremely attractive in that they are based on simple eigendecomposition algorithms whose stability is well understood. Nevertheless, the use of eigendecompositions in the context of segmentation is far from well understood. In this paper we give a unified treatment of these algorithms, and show the close connections between them while highlighting their distinguishing features. We then prove results on eigenvectors of block matrices that allow us to analyze the performance of these algorithms in simple grouping settings. Finally, we use our analysis to motivate a variation on the existing methods that combines aspects from different eigenvector segmentation algorithms. We illustrate our analysis with results on real and synthetic images.',\n",
       "  'authors': ['Y. Weiss'],\n",
       "  'date': '1999',\n",
       "  'identifier': '2160167256',\n",
       "  'references': ['2121947440',\n",
       "   '2049633694',\n",
       "   '1640070940',\n",
       "   '2136120429',\n",
       "   '2143920871',\n",
       "   '2083761303',\n",
       "   '1576344349',\n",
       "   '1731063510',\n",
       "   '2151869980',\n",
       "   '2127123593'],\n",
       "  'title': 'Segmentation using eigenvectors: a unifying view'},\n",
       " {'abstract': \"MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.\",\n",
       "  'authors': ['Jeffrey Dean ', ' Sanjay Ghemawat'],\n",
       "  'date': '2008',\n",
       "  'identifier': '2173213060',\n",
       "  'references': ['2173213060',\n",
       "   '2119565742',\n",
       "   '2148317584',\n",
       "   '2073965851',\n",
       "   '2109722477',\n",
       "   '2104644701',\n",
       "   '1510543252',\n",
       "   '2044534358',\n",
       "   '1988243929',\n",
       "   '2045271686'],\n",
       "  'title': 'MapReduce: simplified data processing on large clusters'},\n",
       " {'abstract': 'A great deal of research has focused on algorithms for learning features from unlabeled data. Indeed, much progress has been made on benchmark datasets like NORB and CIFAR by employing increasingly complex unsupervised learning algorithms and deep models. In this paper, however, we show that several simple factors, such as the number of hidden nodes in the model, may be more important to achieving high performance than the learning algorithm or the depth of the model. Specifically, we will apply several othe-shelf feature learning algorithms (sparse auto-encoders, sparse RBMs, K-means clustering, and Gaussian mixtures) to CIFAR, NORB, and STL datasets using only singlelayer networks. We then present a detailed analysis of the eect of changes in the model setup: the receptive field size, number of hidden nodes (features), the step-size (“stride”) between extracted features, and the eect of whitening. Our results show that large numbers of hidden nodes and dense feature extraction are critical to achieving high performance—so critical, in fact, that when these parameters are pushed to their limits, we achieve state-of-the-art performance on both CIFAR-10 and NORB using only a single layer of features. More surprisingly, our best performance is based on K-means clustering, which is extremely fast, has no hyperparameters to tune beyond the model structure itself, and is very easy to implement. Despite the simplicity of our system, we achieve accuracy beyond all previously published results on the CIFAR-10 and NORB datasets (79.6% and 97.2% respectively).',\n",
       "  'authors': ['Adam Coates 1', ' Andrew Y. Ng 2', ' Honglak Lee 1'],\n",
       "  'date': '2011',\n",
       "  'identifier': '2118858186',\n",
       "  'references': ['2136922672',\n",
       "   '3118608800',\n",
       "   '2162915993',\n",
       "   '2116064496',\n",
       "   '2546302380',\n",
       "   '2025768430',\n",
       "   '2130325614',\n",
       "   '2097018403',\n",
       "   '2107034620',\n",
       "   '1625255723'],\n",
       "  'title': 'An analysis of single-layer networks in unsupervised feature learning'},\n",
       " {'abstract': 'Maximum covariance (MAXCOV) is a method for determining whether a group of 3 or more indicators marks 1 continuous or 2 discrete latent distributions of individuals. Although the circumstances under which MAXCOV is effective in detecting latent taxa have been specified, its efficiency in classifying cases into groups has not been assessed, and few studies have compared its performance with that of cluster analysis. In the present Monte Carlo study, the classification efficiencies of MAXCOV and the k-means algorithm were compared across ranges of sample size, effect size, indicator number, taxon base rate, and within-groups covariance. When the impact of these parameters was minimized, k-means classified more data points correctly than MAXCOV. However, when the effects of all parameters were increased concurrently, MAXCOV outperformed k-means.',\n",
       "  'authors': ['Theodore P. Beauchaine 1', ' Robert J. Beauchaine 2'],\n",
       "  'date': '2002',\n",
       "  'identifier': '2116302707',\n",
       "  'references': ['2159011576',\n",
       "   '2296042795',\n",
       "   '3097169496',\n",
       "   '2037124948',\n",
       "   '2432517183',\n",
       "   '2162542540',\n",
       "   '2107031757',\n",
       "   '2796837256',\n",
       "   '2050006242',\n",
       "   '2913066018'],\n",
       "  'title': 'A comparison of maximum covariance and K-means cluster analysis in classifying cases into known taxon groups.'},\n",
       " {'abstract': 'Communication Systems and Information Theory. A Measure of Information. Coding for Discrete Sources. Discrete Memoryless Channels and Capacity. The Noisy-Channel Coding Theorem. Techniques for Coding and Decoding. Memoryless Channels with Discrete Time. Waveform Channels. Source Coding with a Fidelity Criterion. Index.',\n",
       "  'authors': ['Robert G. Gallager'],\n",
       "  'date': '1968',\n",
       "  'identifier': '2142901448',\n",
       "  'references': ['1667950888',\n",
       "   '2150498905',\n",
       "   '2129766733',\n",
       "   '2151795416',\n",
       "   '2106864314',\n",
       "   '2159080219',\n",
       "   '2133401839',\n",
       "   '2127490352',\n",
       "   '2098567664'],\n",
       "  'title': 'Information Theory and Reliable Communication'},\n",
       " {'abstract': 'When computer networks link people as well as machines, they become social networks. Such computer-supported social networks (CSSNs) are becoming important bases of virtual communities, computer-supported cooperative work, and telework. Computer-mediated communication such as electronic mail and computerized conferencing is usually text-based and asynchronous. It has limited social presence, and on-line communications are often more uninhibited, creative, and blunt than in-person communication. Nevertheless, CSSNs sustain strong, intermediate, and weak ties that provide information and social support in both specialized and broadly based relationships. CSSNs foster virtual communities that are usually partial and narrowly focused, although some do become encompassing and broadly based. CSSNs accomplish a wide variety of cooperative work, connecting workers within and between organizations who are often physically dispersed. CSSNs also link teleworkers from their homes or remote work centers to main organi...',\n",
       "  'authors': ['Barry Wellman ',\n",
       "   ' Janet Salaff ',\n",
       "   ' Dimitrina Dimitrova ',\n",
       "   ' Laura Garton ',\n",
       "   ' Milena Gulia ',\n",
       "   ' Caroline Haythornthwaite'],\n",
       "  'date': '1996',\n",
       "  'identifier': '2143969326',\n",
       "  'references': ['2576297379',\n",
       "   '2124990143',\n",
       "   '2108752510',\n",
       "   '2024372407',\n",
       "   '2083781593',\n",
       "   '2098572648',\n",
       "   '2127841983',\n",
       "   '2564226227',\n",
       "   '653349970',\n",
       "   '2075894438'],\n",
       "  'title': 'Computer Networks as Social Networks: Collaborative Work, Telework, and Virtual Community'},\n",
       " {'abstract': 'Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224 $\\\\times$ 224) input image. This requirement is “artificial” and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, “spatial pyramid pooling”, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102 $\\\\times$ faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.',\n",
       "  'authors': ['Kaiming He 1',\n",
       "   ' Xiangyu Zhang 2',\n",
       "   ' Shaoqing Ren 3',\n",
       "   ' Jian Sun 1'],\n",
       "  'date': '2015',\n",
       "  'identifier': '2109255472',\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2151103935',\n",
       "   '2097117768',\n",
       "   '2153635508',\n",
       "   '2102605133',\n",
       "   '2117539524',\n",
       "   '2161969291',\n",
       "   '2108598243',\n",
       "   '2168356304'],\n",
       "  'title': 'Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition'},\n",
       " {'abstract': 'This paper presents the basic concepts of a multistage classification strategy called the decision tree classifier. Two methods for designing decision trees are discussed and experimental results are reported. The relative advantages and disadvantages of each design method are considered. A spectrum of typical applications in remote sensing is noted.',\n",
       "  'authors': ['Philip H. Swain ', ' Hans Hauska'],\n",
       "  'date': '1977',\n",
       "  'identifier': '1970361289',\n",
       "  'references': ['2136251662',\n",
       "   '2036798369',\n",
       "   '2082874195',\n",
       "   '2035549557',\n",
       "   '2125283600',\n",
       "   '1780185704',\n",
       "   '2010364553',\n",
       "   '2148022840',\n",
       "   '2510496529',\n",
       "   '2172169906'],\n",
       "  'title': 'The decision tree classifier: Design and potential'},\n",
       " {'abstract': \"People use weak ties---relationships with acquaintances or strangers---to seek help unavailable from friends or colleagues. Yet in the absence of personal relationships or the expectation of direct reciprocity, help from weak ties might not be forthcoming or could be of low quality. We examined the practice of distant employees (strangers) exchanging technical advice through a large organizational computer network. A survey of advice seekers and those who replied was conducted to test hypotheses about the viability and usefulness of such electronic weak tie exchanges.Theories of organizational motivation suggest that positive regard for the larger organization can substitute for direct incentives or personal relationships in motivating people to help others. Theories of weak ties suggest that the usefulness of this help may depend on the number of ties, the diversity of ties, or the resources of help providers. We hypothesized that, in an organizational context, the firm-specific resources and organizational motivation of people who provide advice will predict the usefulness of advice.We investigated these theories in a study of employees of a global computer manufacturer. We collected survey and observational data on the relationships between information seekers and information providers; the number, diversity, resources, and motivations of information providers, and subjective ratings of the usefulness of the advice (from both parties in the exchange) and whether or not the advice solved information seekers' problems.We found that information providers gave useful advice and solved the problems of information seekers, despite their lack of a personal connection with the seekers. The data support the main hypotheses and provide some support for resource and diversity explanations of weak tie influence. We discuss how this organization's culture sustained useful intormation exchange through weak ties.\",\n",
       "  'authors': ['David Constant ', ' Lee Sproull ', ' Sara Kiesler'],\n",
       "  'date': '1996',\n",
       "  'identifier': '2075894438',\n",
       "  'references': ['2888190061',\n",
       "   '2015370254',\n",
       "   '2109469951',\n",
       "   '2108752510',\n",
       "   '1987235421',\n",
       "   '2026003563',\n",
       "   '1595891490',\n",
       "   '2059872550',\n",
       "   '1598208848',\n",
       "   '2105124919'],\n",
       "  'title': 'The kindness of strangers: The usefulness of electronic weak ties for technical advice.'},\n",
       " {'abstract': \"Suppose one is given a description of a system, together with an observation of the system's behaviour which conflicts with the way the system is meant to behave. The diagnostic problem is to determine those components of the system which, when assumed to be functioning abnormally, will explain the discrepancy between the observed and correct system behaviour. We propose a general theory for this problem. The theory requires only that the system be described in a suitable logic. Moreover, there are many such suitable logics, e.g. first-order, temporal, dynamic, etc. As a result, the theory accommodates diagnostic reasoning in a wide variety of practical settings, including digital and analogue circuits, medicine, and database updates. The theory leads to an algorithm for computing all diagnoses, and to various results concerning principles of measurement for discriminating among competing diagnoses. Finally, the theory reveals close connections between diagnostic reasoning and nonmonotonic reasoning.\",\n",
       "  'authors': ['Raymond Reiter'],\n",
       "  'date': '1987',\n",
       "  'identifier': '2108309071',\n",
       "  'references': ['2155322595',\n",
       "   '2144386448',\n",
       "   '1549872659',\n",
       "   '2140627345',\n",
       "   '1983382292',\n",
       "   '1998363560',\n",
       "   '1562964770',\n",
       "   '2032511848',\n",
       "   '2038569232',\n",
       "   '1520443109'],\n",
       "  'title': 'A theory of diagnosis from first principles'},\n",
       " {'abstract': 'This chapter contains sections titled: The Problem, The Generalized Delta Rule, Simulation Results, Some Further Generalizations, Conclusion',\n",
       "  'authors': ['D. E. Rumelhart ', ' G. E. Hinton ', ' R. J. Williams'],\n",
       "  'date': '1988',\n",
       "  'identifier': '2154642048',\n",
       "  'references': ['2154642048',\n",
       "   '1652505363',\n",
       "   '1535810436',\n",
       "   '1507849272',\n",
       "   '2101926813',\n",
       "   '2073257493',\n",
       "   '2021878536',\n",
       "   '1490454746',\n",
       "   '2115647291',\n",
       "   '1505136099'],\n",
       "  'title': 'Learning internal representations by error propagation'},\n",
       " {'abstract': 'The intent of this paper is to explore the application of information obtained from fully polarimetric data for land cover classiflcation. Various land cover classiflcation techniques are available in the literature, but still uncertainty exists in labeling various clusters to their own classes without using any a priori information. Therefore, the present work is focused on analyzing useful intrinsic information extracted from SAR observables obtained by various decomposition techniques. The eigenvalue decomposition and Pauli decomposition have been carried out to separate classes on the basis of their scattering mechanisms. The various classiflcation techniques (supervised: minimum distance, maximum likelihood, parallelepiped and unsupervised: Wishart) were applied in order to see possible difierences among SAR observables in terms of information that they contain and their usefulness in classifying particular land cover type. Another important issue is labeling the clusters, and this work is carried out by decision tree classiflcation that uses knowledge based approach. This classifler is implemented by scrupulous knowledge of data obtained by empirical evidence and their experimental validation. It has been demonstrated quantitatively that standard polarimetric parameters such as polarized backscatter coe-cients (linear, circular',\n",
       "  'authors': ['Pooja Mishra ', ' Dharmendra Singh ', ' Yoshio Yamaguchi'],\n",
       "  'date': '2011',\n",
       "  'identifier': '2172169906',\n",
       "  'references': ['2078985447',\n",
       "   '2133989913',\n",
       "   '2097272115',\n",
       "   '2141424348',\n",
       "   '2099534828',\n",
       "   '2312997001',\n",
       "   '2164296623',\n",
       "   '2008826820',\n",
       "   '2082874195',\n",
       "   '2162480849'],\n",
       "  'title': 'Land Cover Classification of Palsar Images by Knowledge Based Decision Tree Classifier and Supervised Classifiers Based on SAR Observables'},\n",
       " {'abstract': 'The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and ban@ass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete--i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli. © 1997 Elsevier Science Ltd',\n",
       "  'authors': ['Bruno A. Olshausen ', ' David J. Field'],\n",
       "  'date': '1997',\n",
       "  'identifier': '2105464873',\n",
       "  'references': ['2132984323',\n",
       "   '2099741732',\n",
       "   '3110653090',\n",
       "   '2145889472',\n",
       "   '1536929369',\n",
       "   '2133069808',\n",
       "   '2107790757',\n",
       "   '2167034998',\n",
       "   '3022628558',\n",
       "   '2145012779'],\n",
       "  'title': 'Sparse Coding with an Overcomplete Basis Set: A Strategy Employed by V1 ?'},\n",
       " {'abstract': 'We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.',\n",
       "  'authors': ['Eric Brochu ', ' Vlad M. Cora ', ' Nando de Freitas'],\n",
       "  'date': '2010',\n",
       "  'identifier': '2099201756',\n",
       "  'references': ['2100495367',\n",
       "   '2121863487',\n",
       "   '2903158431',\n",
       "   '2018044188',\n",
       "   '1510052597',\n",
       "   '1576452626',\n",
       "   '2951665052',\n",
       "   '2041946752',\n",
       "   '2130761473',\n",
       "   '2131824593'],\n",
       "  'title': 'A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning'},\n",
       " {'abstract': 'The naive Bayes classifier, currently experiencing a renaissance in machine learning, has long been a core technique in information retrieval. We review some of the variations of naive Bayes models used for text retrieval and classification, focusing on the distributional assumptions made about word occurrences in documents.',\n",
       "  'authors': ['David D. Lewis'],\n",
       "  'date': '1998',\n",
       "  'identifier': '1924689489',\n",
       "  'references': ['2149684865',\n",
       "   '1956559956',\n",
       "   '2140785063',\n",
       "   '3017143921',\n",
       "   '1997841190',\n",
       "   '2000672666',\n",
       "   '2085989833',\n",
       "   '2000569744',\n",
       "   '1969572066',\n",
       "   '2014415866'],\n",
       "  'title': 'Naive (Bayes) at forty: the independence assumption in information retrieval'},\n",
       " {'abstract': \"Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classification t, it is possible to find a new input x' that is similar to x but classified as t. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from 95% to 0.5%.In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with 100% probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.\",\n",
       "  'authors': ['Nicholas Carlini ', ' David Wagner'],\n",
       "  'date': '2017',\n",
       "  'identifier': '2963857521',\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '2117539524',\n",
       "   '2145339207',\n",
       "   '2108598243',\n",
       "   '2257979135',\n",
       "   '2310919327',\n",
       "   '2964153729',\n",
       "   '2160815625',\n",
       "   '2143612262'],\n",
       "  'title': 'Towards Evaluating the Robustness of Neural Networks'},\n",
       " {'abstract': 'We present a Monte Carlo algorithm to find approximate solutions of the traveling salesman problem. The algorithm generates randomly the permutations of the stations of the traveling salesman trip, with probability depending on the length of the corresponding route. Reasoning by analogy with statistical thermodynamics, we use the probability given by the Boltzmann-Gibbs distribution. Surprisingly enough, using this simple algorithm, one can get very close to the optimal solution of the problem or even find the true optimum. We demonstrate this on several examples. We conjecture that the analogy with thermodynamics can offer a new insight into optimization problems and can suggest efficient algorithms for solving them.',\n",
       "  'authors': ['V. Černý'],\n",
       "  'date': '1985',\n",
       "  'identifier': '2154061444',\n",
       "  'references': ['2056760934'],\n",
       "  'title': 'Thermodynamical approach to the traveling salesman problem: An efficient simulation algorithm'},\n",
       " {'abstract': 'Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these \"Stepped Sigmoid Units\" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.',\n",
       "  'authors': ['Vinod Nair ', ' Geoffrey E. Hinton'],\n",
       "  'date': '2010',\n",
       "  'identifier': '1665214252',\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2116064496',\n",
       "   '2546302380',\n",
       "   '1782590233',\n",
       "   '2134557905',\n",
       "   '2099866409',\n",
       "   '1994197834',\n",
       "   '2536626143',\n",
       "   '2157364932'],\n",
       "  'title': 'Rectified Linear Units Improve Restricted Boltzmann Machines'},\n",
       " {'abstract': 'The growth of Internet commerce has stimulated the use of collaborative filtering (CF) algorithms as recommender systems. Such systems leverage knowledge about the known preferences of multiple users to recommend items of interest to other users. CF methods have been harnessed to make recommendations about such items as web pages, movies, books, and toys. Researchers have proposed and evaluated many approaches for generating recommendations. We describe and evaluate a new method called personality diagnosis (PD). Given a user\\'s preferences for some items, we compute the probability that he or she is of the same \"personality type\" as other users, and, in turn, the probability that he or she will like new items. PD retains some of the advantages of traditional similarity-weighting techniques in that all data is brought to bear on each prediction and new data can be added easily and incrementally. Additionally, PD has a meaningful probabilistic interpretation, which may be leveraged to justify, explain, and augment results. We report empirical results on the EachMovie database of movie ratings, and on user profile data collected from the CiteSeer digital library of Computer Science research papers. The probabilistic framework naturally supports a variety of descriptive measurements--in particular, we consider the applicability of a value of information (VOI) computation.',\n",
       "  'authors': ['David M. Pennock 1',\n",
       "   ' Eric Horvitz 2',\n",
       "   ' Steve Lawrence 1',\n",
       "   ' C. Lee Giles 1'],\n",
       "  'date': '2000',\n",
       "  'identifier': '2109992782',\n",
       "  'references': ['2110325612',\n",
       "   '2155106456',\n",
       "   '2124591829',\n",
       "   '1966553486',\n",
       "   '2341865734',\n",
       "   '1999047234',\n",
       "   '2107890099',\n",
       "   '2154498027',\n",
       "   '1997136459',\n",
       "   '2124029832'],\n",
       "  'title': 'Collaborative filtering by personality diagnosis: a hybrid memory- and model-based approach'},\n",
       " {'abstract': 'Speech recognition is formulated as a problem of maximum likelihood decoding. This formulation requires statistical models of the speech production process. In this paper, we describe a number of statistical models for use in speech recognition. We give special attention to determining the parameters for such models from sparse data. We also describe two decoding methods, one appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks. To illustrate the usefulness of the methods described, we review a number of decoding results that have been obtained with them.',\n",
       "  'authors': ['Lalit R. Bahl ', ' Frederick Jelinek ', ' Robert L. Mercer'],\n",
       "  'date': '1983',\n",
       "  'identifier': '1966812932',\n",
       "  'references': ['2142384583',\n",
       "   '1597533204',\n",
       "   '1575431606',\n",
       "   '2341171179',\n",
       "   '2163929346',\n",
       "   '2157477135',\n",
       "   '2029491572',\n",
       "   '2035227369',\n",
       "   '2137095888',\n",
       "   '1989226853'],\n",
       "  'title': 'A Maximum Likelihood Approach to Continuous Speech Recognition'},\n",
       " {'abstract': 'Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. 1 Deep Neural Networks Deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features. They include Appearing in Proceedings of the 13 International Conference on Artificial Intelligence and Statistics (AISTATS) 2010, Chia Laguna Resort, Sardinia, Italy. Volume 9 of JMLR: WC Weston et al., 2008). Much attention has recently been devoted to them (see (Bengio, 2009) for a review), because of their theoretical appeal, inspiration from biology and human cognition, and because of empirical success in vision (Ranzato et al., 2007; Larochelle et al., 2007; Vincent et al., 2008) and natural language processing (NLP) (Collobert & Weston, 2008; Mnih & Hinton, 2009). Theoretical results reviewed and discussed by Bengio (2009), suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one may need deep architectures. Most of the recent experimental results with deep architecture are obtained with models that can be turned into deep supervised neural networks, but with initialization or training schemes different from the classical feedforward neural networks (Rumelhart et al., 1986). Why are these new algorithms working so much better than the standard random initialization and gradient-based optimization of a supervised training criterion? Part of the answer may be found in recent analyses of the effect of unsupervised pretraining (Erhan et al., 2009), showing that it acts as a regularizer that initializes the parameters in a “better” basin of attraction of the optimization procedure, corresponding to an apparent local minimum associated with better generalization. But earlier work (Bengio et al., 2007) had shown that even a purely supervised but greedy layer-wise procedure would give better results. So here instead of focusing on what unsupervised pre-training or semi-supervised criteria bring to deep architectures, we focus on analyzing what may be going wrong with good old (but deep) multilayer neural networks. Our analysis is driven by investigative experiments to monitor activations (watching for saturation of hidden units) and gradients, across layers and across training iterations. We also evaluate the effects on these of choices of activation function (with the idea that it might affect saturation) and initialization procedure (since unsupervised pretraining is a particular form of initialization and it has a drastic impact).',\n",
       "  'authors': ['Xavier Glorot ', ' Yoshua Bengio'],\n",
       "  'date': '2010',\n",
       "  'identifier': '1533861849',\n",
       "  'references': ['2136922672',\n",
       "   '3118608800',\n",
       "   '2310919327',\n",
       "   '2072128103',\n",
       "   '2117130368',\n",
       "   '2025768430',\n",
       "   '2110798204',\n",
       "   '1498436455',\n",
       "   '1994197834',\n",
       "   '2131462252'],\n",
       "  'title': 'Understanding the difficulty of training deep feedforward neural networks'},\n",
       " {'abstract': 'We have developed a near-real-time computer system that can locate and track a subject\\'s head, and then recognize the person by comparing characteristics of the face to those of known individuals. The computational approach taken in this system is motivated by both physiology and information theory, as well as by the practical requirements of near-real-time performance and accuracy. Our approach treats the face recognition problem as an intrinsically two-dimensional (2-D) recognition problem rather than requiring recovery of three-dimensional geometry, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. The system functions by projecting face images onto a feature space that spans the significant variations among known face images. The significant features are known as \"eigenfaces,\" because they are the eigenvectors (principal components) of the set of faces; they do not necessarily correspond to features such as eyes, ears, and noses. The projection operation characterizes an individual face by a weighted sum of the eigenface features, and so to recognize a particular face it is necessary only to compare these weights to those of known individuals. Some particular advantages of our approach are that it provides for the ability to learn and later recognize new faces in an unsupervised manner, and that it is easy to implement using a neural network architecture.',\n",
       "  'authors': ['Matthew Turk ', ' Alex Pentland'],\n",
       "  'date': '1991',\n",
       "  'identifier': '2138451337',\n",
       "  'references': ['2135463994',\n",
       "   '2130259898',\n",
       "   '2125848778',\n",
       "   '2055712799',\n",
       "   '2125999363',\n",
       "   '1509703770',\n",
       "   '1526492552',\n",
       "   '1507699566',\n",
       "   '2032361618',\n",
       "   '1986450498'],\n",
       "  'title': 'Eigenfaces for recognition'},\n",
       " {'abstract': 'To comprehend the multipartite organization of large-scale biological and social systems, we introduce an information theoretic approach that reveals community structure in weighted and directed networks. We use the probability flow of random walks on a network as a proxy for information flows in the real system and decompose the network into modules by compressing a description of the probability flow. The result is a map that both simplifies and highlights the regularities in the structure and their relationships. We illustrate the method by making a map of scientific communication as captured in the citation patterns of >6,000 journals. We discover a multicentric organization with fields that vary dramatically in size and degree of integration into the network of science. Along the backbone of the network—including physics, chemistry, molecular biology, and medicine—information flows bidirectionally, but the map reveals a directional pattern of citation from the applied fields to the basic sciences.',\n",
       "  'authors': ['Martin Rosvall ', ' Carl T. Bergstrom'],\n",
       "  'date': '2008',\n",
       "  'identifier': '2164998314',\n",
       "  'references': ['2148606196',\n",
       "   '2095293504',\n",
       "   '1971421925',\n",
       "   '2089458547',\n",
       "   '2164928285',\n",
       "   '2047940964',\n",
       "   '1599334980',\n",
       "   '2147824439',\n",
       "   '2017987256',\n",
       "   '1666636243'],\n",
       "  'title': 'Maps of random walks on complex networks reveal community structure'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Raymond J. Jessen'],\n",
       "  'date': '1975',\n",
       "  'identifier': '2330498442',\n",
       "  'references': ['2038669746',\n",
       "   '2325086744',\n",
       "   '2071088261',\n",
       "   '2160231391',\n",
       "   '99669289',\n",
       "   '1988858799',\n",
       "   '2294788433',\n",
       "   '2086973731',\n",
       "   '43340385',\n",
       "   '2333435989'],\n",
       "  'title': 'Square and Cubic Lattice Sampling'},\n",
       " {'abstract': 'The book Knowledge Discovery in Databases, edited by Piatetsky-Shapiro and Frawley [PSF91], is an early collection of research papers on knowledge discovery from data. The book Advances in Knowledge Discovery and Data Mining, edited by Fayyad, Piatetsky-Shapiro, Smyth, and Uthurusamy [FPSSe96], is a collection of later research results on knowledge discovery and data mining. There have been many data mining books published in recent years, including Predictive Data Mining by Weiss and Indurkhya [WI98], Data Mining Solutions: Methods and Tools for Solving Real-World Problems by Westphal and Blaxton [WB98], Mastering Data Mining: The Art and Science of Customer Relationship Management by Berry and Linofi [BL99], Building Data Mining Applications for CRM by Berson, Smith, and Thearling [BST99], Data Mining: Practical Machine Learning Tools and Techniques by Witten and Frank [WF05], Principles of Data Mining (Adaptive Computation and Machine Learning) by Hand, Mannila, and Smyth [HMS01], The Elements of Statistical Learning by Hastie, Tibshirani, and Friedman [HTF01], Data Mining: Introductory and Advanced Topics by Dunham, and Data Mining: Multimedia, Soft Computing, and Bioinformatics by Mitra and Acharya [MA03]. There are also books containing collections of papers on particular aspects of knowledge discovery, such as Machine Learning and Data Mining: Methods and Applications edited by Michalski, Brakto, and Kubat [MBK98], and Relational Data Mining edited by Dzeroski and Lavrac [De01], as well as many tutorial notes on data mining in major database, data mining and machine learning conferences.',\n",
       "  'authors': ['Jiawei Han ', ' Micheline Kamber'],\n",
       "  'date': '2006',\n",
       "  'identifier': '2186428165',\n",
       "  'references': ['2156909104',\n",
       "   '2912565176',\n",
       "   '1880262756',\n",
       "   '2148603752',\n",
       "   '2124776405',\n",
       "   '1639032689',\n",
       "   '2331432542',\n",
       "   '2008620264',\n",
       "   '1554944419',\n",
       "   '3029645440'],\n",
       "  'title': 'Data Mining: Concepts and Techniques (2nd edition)'},\n",
       " {'abstract': \"Computer vision is moving into a new era in which the aim is to develop visual skills for robots that allow them to interact with a dynamic, unconstrained environment. To achieve this aim, new kinds of vision algorithms need to be developed which run in real time and subserve the robot's goals. Two fundamental goals are determining the identity of an object with a known location, and determining the location of a known object. Color can be successfully used for both tasks. This dissertation demonstrates that color histograms of multicolored objects provide a robust, efficient cue for indexing into a large database of models. It shows that color histograms are stable object representations in the presence of occlusion and over change in view, and that they can differentiate among a large number of objects. For solving the identification problem, it introduces a technique called Histogram Intersection, which matches model and image histograms and a fast incremental version of Histogram Intersection which allows real-time indexing into a large database of stored models. It demonstrates techniques for dealing with crowded scenes and with models with similar color signatures. For solving the location problem it introduces an algorithm called Histogram Backprojection which performs this task efficiently in crowded scenes.\",\n",
       "  'authors': ['Michael James Swain ', ' Dana H. Ballard'],\n",
       "  'date': '1991',\n",
       "  'identifier': '2914885528',\n",
       "  'references': ['2740373864',\n",
       "   '2115738369',\n",
       "   '2913703059',\n",
       "   '3021212382',\n",
       "   '2296039540',\n",
       "   '2415527960',\n",
       "   '2125756925',\n",
       "   '2119204143',\n",
       "   '2172373809',\n",
       "   '2489504689'],\n",
       "  'title': 'Color indexing'},\n",
       " {'abstract': \"Description or discrimination of boundary curves (shapes) is an important problem in picture processing and pattern recognition Fourier descriptors (FD's) have interesting properties in this respect. First, a critical review is given of two kinds of FD's. Some properties of the FD's are given and a distance measure is proposed, in terms of FD's, that measures the difference between two boundarv curves. It is shown how FD's can be used for obtaining skeletons fobjects. Finally, experimental results are given in character recognition and machine parts recognition.\",\n",
       "  'authors': ['Eric Persoon ', ' King-Sun Fu'],\n",
       "  'date': '1986',\n",
       "  'identifier': '2008313333',\n",
       "  'references': ['2008313333',\n",
       "   '2122827492',\n",
       "   '1966591781',\n",
       "   '1993324373',\n",
       "   '2084280096',\n",
       "   '1979819178',\n",
       "   '2044247918',\n",
       "   '89157676',\n",
       "   '1967836552'],\n",
       "  'title': 'Shape Discrimination Using Fourier Descriptors'},\n",
       " {'abstract': 'Advances in scientific computing have made modelling and simulation an important part of the decision-making process in engineering, science, and public policy. This book provides a comprehensive and systematic development of the basic concepts, principles, and procedures for verification and validation of models and simulations. The emphasis is placed on models that are described by partial differential and integral equations and the simulations that result from their numerical solution. The methods described can be applied to a wide range of technical fields, from the physical sciences, engineering and technology and industry, through to environmental regulations and safety, product and plant safety, financial investing, and governmental regulations. This book will be genuinely welcomed by researchers, practitioners, and decision makers in a broad range of fields, who seek to improve the credibility and reliability of simulation results. It will also be appropriate either for university courses or for independent study.',\n",
       "  'authors': ['William L. Oberkampf ', ' Christopher J. Roy'],\n",
       "  'date': '2010',\n",
       "  'identifier': '1607663648',\n",
       "  'references': ['2331432542',\n",
       "   '2315214008',\n",
       "   '2798909945',\n",
       "   '1496357020',\n",
       "   '2045656233',\n",
       "   '1493688518',\n",
       "   '1506342804',\n",
       "   '3004157836',\n",
       "   '2038785086',\n",
       "   '2053289371'],\n",
       "  'title': 'Verification and Validation in Scientific Computing'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Leonard E. Baum ', ' J. A. Eagon'],\n",
       "  'date': '1967',\n",
       "  'identifier': '2077574412',\n",
       "  'references': ['1967618559', '2130450805'],\n",
       "  'title': 'An inequality with applications to statistical estimation for probabilistic functions of Markov processes and to a model for ecology'},\n",
       " {'abstract': 'A general method for model-based object recognition in occluded scenes is presented. It is based on geometric hashing. The method stands out for its efficiency. We describe the general framework of the method and illustrate its applications for various recogni- tion problems both in 3-D and 2-D. Special attention is given to the recognition of 3-D objects in occluded scenes from 2-D gray scale images. New experimental results are included for this important case.',\n",
       "  'authors': ['Y. Lamdan ', ' H.J. Wolfson'],\n",
       "  'date': '1988',\n",
       "  'identifier': '2154204736',\n",
       "  'references': ['22745672',\n",
       "   '1996773532',\n",
       "   '1532977286',\n",
       "   '2026311529',\n",
       "   '2337770697',\n",
       "   '2000048778',\n",
       "   '2163535310',\n",
       "   '2107977888',\n",
       "   '2134102912',\n",
       "   '2058389820'],\n",
       "  'title': 'Geometric Hashing: A General And Efficient Model-based Recognition Scheme'},\n",
       " {'abstract': 'Abstract : The use of active compliance enables robots to carry out tasks in the presence of significant sensing and control errors. Compliant motions are quite difficult for humans to specify, however. Furthermore, robot programs are quite sensitive to details of geometry and to error characteristics and must, therefore, be constructed anew for each task. These factors motivate the need for automatic synthesis tools for robot programming, especially for compliant motion. This paper describes formal approach to the synthesis of compliant motion strategies from geometric descriptions of assembly operations and explicit estimates of errors in sensing and control. A key aspect of the approach is that it provides correctness criteria for compliant motion strategies. (Author)',\n",
       "  'authors': ['Tomás Lozano-pérez ',\n",
       "   ' Matthew T. Mason ',\n",
       "   ' Russell H. Taylor'],\n",
       "  'date': '1991',\n",
       "  'identifier': '2337770697',\n",
       "  'references': ['1969160376',\n",
       "   '2088043683',\n",
       "   '2128082316',\n",
       "   '1999839266',\n",
       "   '2095981630',\n",
       "   '2171771720',\n",
       "   '2023509699',\n",
       "   '2141124675',\n",
       "   '2112535044',\n",
       "   '2116233534'],\n",
       "  'title': 'Automatic synthesis of fine-motion strategies for robots'},\n",
       " {'abstract': 'As a method specifically intended for the study of messages, content analysis is fundamental to mass communication research. Intercoder reliability, more specifically termed intercoder agreement, is a measure of the extent to which independent judges make the same coding decisions in evaluating the characteristics of messages, and is at the heart of this method. Yet there are few standard and accessible guidelines available regarding the appropriate procedures to use to assess and report intercoder reliability, or software tools to calculate it. As a result, it seems likely that there is little consistency in how this critical element of content analysis is assessed and reported in published mass communication studies. Following a review of relevant concepts, indices, and tools, a content analysis of 200 studies utilizing content analysis published in the communication literature between 1994 and 1998 is used to characterize practices in the field. The results demonstrate that mass communication researchers often fail to assess (or at least report) intercoder reliability and often rely on percent agreement, an overly liberal index. Based on the review and these results, concrete guidelines are offered regarding procedures for assessment and reporting of this important aspect of content analysis.',\n",
       "  'authors': ['Matthew Lombard 1',\n",
       "   ' Jennifer Snyder-Duch 2',\n",
       "   ' Cheryl Campanella Bracken 3'],\n",
       "  'date': '2002',\n",
       "  'identifier': '2160375265',\n",
       "  'references': ['1568955415',\n",
       "   '3021916629',\n",
       "   '2313339984',\n",
       "   '1563918824',\n",
       "   '1667831672',\n",
       "   '2166410494',\n",
       "   '2053154970',\n",
       "   '2076898331',\n",
       "   '1564901016',\n",
       "   '2037789405'],\n",
       "  'title': 'Content Analysis in Mass Communication: Assessment and Reporting of Intercoder Reliability'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Horace Barlow'],\n",
       "  'date': '1994',\n",
       "  'identifier': '2946776431',\n",
       "  'references': ['2133589685',\n",
       "   '2137234026',\n",
       "   '2101933716',\n",
       "   '2143421693',\n",
       "   '2124486835',\n",
       "   '145818128',\n",
       "   '2126747264',\n",
       "   '2143296986',\n",
       "   '1489504112',\n",
       "   '1754959797'],\n",
       "  'title': 'What is the computational goal of the neocortex'},\n",
       " {'abstract': \"Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index.\",\n",
       "  'authors': ['Thomas M. Cover 1', ' Joy A. Thomas 2'],\n",
       "  'date': '1991',\n",
       "  'identifier': '2099111195',\n",
       "  'references': ['2071707134',\n",
       "   '2106248279',\n",
       "   '2148963518',\n",
       "   '2032372805',\n",
       "   '2132103241',\n",
       "   '2165232124',\n",
       "   '2151795416',\n",
       "   '2790166049'],\n",
       "  'title': 'Elements of information theory'},\n",
       " {'abstract': 'A machine is designed, based on a pyramid architecture, that supports smart sensing and related highly efficient processing. Key elements of the design are (a) hierarchical data structures for image representation, (b) fine-to-coarse algorithms for the fast generation of image measures, (c) coarse-to-fine search strategies that rapidly locate objects or events within a scene, and (d) high-level control mechanisms that guide data gathering even as visual information is being interpreted. This system, known as the Pyramid Vision Machine, achieves high performance at modest cost. Design considerations and several applications are described. >',\n",
       "  'authors': ['P.J. Burt'],\n",
       "  'date': '1988',\n",
       "  'identifier': '2055712799',\n",
       "  'references': ['2103504761',\n",
       "   '2003370853',\n",
       "   '2978983090',\n",
       "   '2153709524',\n",
       "   '2067398582',\n",
       "   '2074163268',\n",
       "   '2033266778',\n",
       "   '1964415410',\n",
       "   '1834283795',\n",
       "   '2075554361'],\n",
       "  'title': 'Smart sensing within a pyramid vision machine'},\n",
       " {'abstract': 'Multiresolution representations are effective for analyzing the information content of images. The properties of the operator which approximates a signal at a given resolution were studied. It is shown that the difference of information between the approximation of a signal at the resolutions 2/sup j+1/ and 2/sup j/ (where j is an integer) can be extracted by decomposing this signal on a wavelet orthonormal basis of L/sup 2/(R/sup n/), the vector space of measurable, square-integrable n-dimensional functions. In L/sup 2/(R), a wavelet orthonormal basis is a family of functions which is built by dilating and translating a unique function psi (x). This decomposition defines an orthogonal multiresolution representation called a wavelet representation. It is computed with a pyramidal algorithm based on convolutions with quadrature mirror filters. Wavelet representation lies between the spatial and Fourier domains. For images, the wavelet representation differentiates several spatial orientations. The application of this representation to data compression in image coding, texture discrimination and fractal analysis is discussed. >',\n",
       "  'authors': ['S.G. Mallat'],\n",
       "  'date': '1989',\n",
       "  'identifier': '2132984323',\n",
       "  'references': [],\n",
       "  'title': 'A theory for multiresolution signal decomposition: the wavelet representation'},\n",
       " {'abstract': 'From the Publisher: \"Face Image Analysis by Unsupervised Learning explores adaptive approaches to image analysis. It draws upon principles of unsupervised learning and information theory to adapt processing to the immediate task environment. In contrast to more traditional approaches to image analysis in which relevant structure is determined in advance and extracted using hand-engineered techniques, Face Image Analysis by Unsupervised Learning explores methods that have roots in biological vision and/or learn about the image structure directly from the image ensemble. Particular attention is paid to unsupervised learning techniques for encoding the statistical dependencies in the image ensemble.\" \"Face Image Analysis by Unsupervised Learning is suitable as a secondary text for a graduate level course and as a reference for researchers and practitioners in industry.\"--BOOK JACKET.',\n",
       "  'authors': ['Marian Stewart Bartlett'],\n",
       "  'date': '2001',\n",
       "  'identifier': '1489504112',\n",
       "  'references': ['2124776405',\n",
       "   '2138451337',\n",
       "   '1902027874',\n",
       "   '2121647436',\n",
       "   '2099741732',\n",
       "   '3110653090',\n",
       "   '2339343773',\n",
       "   '2994841252',\n",
       "   '2293063825',\n",
       "   '2145889472'],\n",
       "  'title': 'Face image analysis by unsupervised learning'},\n",
       " {'abstract': \"A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by “learning without a teacher”, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname “neocognitron”. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of “S-cells”, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of “C-cells” similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any “teacher” during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.\",\n",
       "  'authors': ['Kunihiko Fukushima'],\n",
       "  'date': '1980',\n",
       "  'identifier': '2101926813',\n",
       "  'references': ['2116360511',\n",
       "   '2322002063',\n",
       "   '2053120614',\n",
       "   '1588340522',\n",
       "   '1594551768',\n",
       "   '2010315761',\n",
       "   '2272360941',\n",
       "   '22889343',\n",
       "   '2324189819',\n",
       "   '2091546412'],\n",
       "  'title': 'Neocognitron: A Self Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position'},\n",
       " {'abstract': 'Properties of the receptive fields of simple cells in macaque cortex were compared with properties of independent component filters generated by independent component analysis (ICA) on a large set of natural images. Histograms of spatial frequency bandwidth, orientation tuning bandwidth, aspect ratio and length of the receptive fields match well. This indicates that simple cells are well tuned to the expected statistics of natural stimuli. There is no match, however, in calculated and measured distributions for the peak of the spatial frequency response: the filters produced by ICA do not vary their spatial scale as much as simple cells do, but are fixed to scales close to the finest ones allowed by the sampling lattice. Possible ways to resolve this discrepancy are discussed.',\n",
       "  'authors': ['J H van Hateren 1', ' A van der Schaaf 2'],\n",
       "  'date': '1998',\n",
       "  'identifier': '2101933716',\n",
       "  'references': ['2145889472',\n",
       "   '2019502123',\n",
       "   '2105464873',\n",
       "   '2137234026',\n",
       "   '2180838288',\n",
       "   '2022735534',\n",
       "   '2167034998',\n",
       "   '2120838001',\n",
       "   '2170319235',\n",
       "   '2042755403'],\n",
       "  'title': 'Independent component filters of natural images compared with simple cells in primary visual cortex'},\n",
       " {'abstract': 'Multimodal fusion increases the performance of emotion recognition because of the complementarity of different modalities. Compared with decision level and feature level fusion, model level fusion makes better use of the advantages of deep neural networks. In this work, we utilize the Transformer model to fuse audio-visual modalities on the model level. Specifically, the multi-head attention produces multimodal emotional intermediate representations from common semantic feature space after encoding audio and visual modalities. Meanwhile, it also can learn long-term temporal dependencies with self-attention mechanism effectively. The experiments, on the AVEC 2017 database, shows the superiority of model level fusion than other fusion strategies. Moreover, we combine the Transformer model and LSTM to further improve the performance, which achieves better results than other methods.',\n",
       "  'authors': ['Jian Huang ',\n",
       "   ' Jianhua Tao ',\n",
       "   ' Bin Liu ',\n",
       "   ' Zheng Lian ',\n",
       "   ' Mingyue Niu'],\n",
       "  'date': '2020',\n",
       "  'identifier': '3016138882',\n",
       "  'references': ['2963403868',\n",
       "   '2085662862',\n",
       "   '2173629880',\n",
       "   '2239141610',\n",
       "   '2313339984',\n",
       "   '2346454595',\n",
       "   '2143350951',\n",
       "   '2153822685',\n",
       "   '2009059481',\n",
       "   '2751214333'],\n",
       "  'title': 'Multimodal Transformer Fusion for Continuous Emotion Recognition'},\n",
       " {'abstract': 'Abstract A Hebbian adaptation rule with winner-take-all like competition is introduced. It is shown that this competitive Hebbian rule forms so-called Delaunay triangulations, which play an important role in computational geometry for efficiently solving proximity problems. Given a set of neural units i, i = 1,…, N, the synaptic weights of which can be interpreted as pointers wi, i = 1,…, N in RD, the competitive Hebbian rule leads to a connectivity structure between the units i that corresponds to the Delaunay triangulation of the set of pointers wi. Such competitive Hebbian rule develops connections (Cij > 0) between neural units i, j with neighboring receptive fields (Voronoi polygons) Vi, Vj, whereas between all other units i, j no connections evolve (Cij = 0). Combined with a procedure that distributes the pointers wi over a given feature manifold M, for example, a submanifold M ⊂ RD, the competitive Hebbian rule provides a novel approach to the problem of constructing topology preserving feature maps and representing intricately structured manifolds. The competitive Hebbian rule connects only neural units, the receptive fields (Voronoi polygons) Vi, Vj of which are adjacent on the given manifold M. This leads to a connectivity structure that defines a perfectly topology preserving map and forms a discrete, path preserving representation of M, also in cases where M has an intricate topology. This makes this novel approach particularly useful in all applications where neighborhood relations have to be exploited or the shape and topology of submanifolds have to be take into account.',\n",
       "  'authors': ['Thomas Martinetz 1', ' 2', ' Klaus Schulten 1'],\n",
       "  'date': '1994',\n",
       "  'identifier': '2047870719',\n",
       "  'references': ['2046079134',\n",
       "   '1991848143',\n",
       "   '3017143921',\n",
       "   '65738273',\n",
       "   '22297218',\n",
       "   '2913399920',\n",
       "   '2005314985',\n",
       "   '2166322089',\n",
       "   '2002182716',\n",
       "   '2098929365'],\n",
       "  'title': 'Topology representing networks'},\n",
       " {'abstract': 'Is perception of the whole based on perception of its parts? There is psychological1 and physiological2,3 evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations4,5. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.',\n",
       "  'authors': ['Daniel D. Lee 1', ' H. Sebastian Seung 1', ' 2'],\n",
       "  'date': '1999',\n",
       "  'identifier': '1902027874',\n",
       "  'references': ['2138451337',\n",
       "   '3110653090',\n",
       "   '2049633694',\n",
       "   '1956559956',\n",
       "   '2145889472',\n",
       "   '1983578042',\n",
       "   '1996355918',\n",
       "   '1993845689',\n",
       "   '2156406284',\n",
       "   '2180838288'],\n",
       "  'title': 'Learning the parts of objects by non-negative matrix factorization'},\n",
       " {'abstract': 'In recent years, a variety of nonlinear dimensionality reduction techniques have been proposed that aim to address the limitations of traditional techniques such as PCA and classical scaling. The paper presents a review and systematic comparison of these techniques. The performances of the nonlinear techniques are investigated on artificial and natural tasks. The results of the experiments reveal that nonlinear techniques perform well on selected artificial tasks, but that this strong performance does not necessarily extend to real-world tasks. The paper explains these results by identifying weaknesses of current nonlinear techniques, and suggests how the performance of nonlinear dimensionality reduction techniques may be improved.',\n",
       "  'authors': ['Laurens van der Maaten ',\n",
       "   ' Eric Postma 1',\n",
       "   ' Jaap van den Herik 2'],\n",
       "  'date': '2009',\n",
       "  'identifier': '2137570937',\n",
       "  'references': ['2296319761',\n",
       "   '1880262756',\n",
       "   '2136922672',\n",
       "   '2100495367',\n",
       "   '1497256448',\n",
       "   '2187089797',\n",
       "   '2072128103',\n",
       "   '2053186076',\n",
       "   '2121947440',\n",
       "   '2001141328'],\n",
       "  'title': 'Dimensionality Reduction: A Comparative Review'},\n",
       " {'abstract': 'Two recently implemented machine-learning algorithms, RIPPER and sleeping-experts for phrases , are evaluated on a number of large text categorization problems. These algorithms both construct classifiers that allow the “context” of a word w to affect how (or even whether) the presence or absence of w will contribute to a classification. However, RIPPER and sleeping-experts differ radically in many other respects: differences include different notions as to what constitutes a context, different ways of combining contexts to construct a classifier, different methods to search for a combination of contexts, and different criteria as to what contexts should be included in such a combination. In spite of these differences, both RIPPER and sleeping-experts perform extremely well across a wide variety of categorization problems, generally outperforming previously applied learning methods. We view this result as a confirmation of the usefulness of classifiers that represent contextual information.',\n",
       "  'authors': ['William W. Cohen ', ' Yoram Singer'],\n",
       "  'date': '1999',\n",
       "  'identifier': '1969572066',\n",
       "  'references': ['1988790447',\n",
       "   '1670263352',\n",
       "   '1619226191',\n",
       "   '2093825590',\n",
       "   '2085989833',\n",
       "   '1999138184',\n",
       "   '2060216474',\n",
       "   '2129113961',\n",
       "   '40914139',\n",
       "   '2094934653'],\n",
       "  'title': 'Context-sensitive learning methods for text categorization'},\n",
       " {'abstract': 'Deep Neural Networks (DNNs) have recently shown outstanding performance on image classification tasks [14]. In this paper we go one step further and address the problem of object detection using DNNs, that is not only classifying but also precisely localizing objects of various classes. We present a simple and yet powerful formulation of object detection as a regression problem to object bounding box masks. We define a multi-scale inference procedure which is able to produce high-resolution object detections at a low cost by a few network applications. State-of-the-art performance of the approach is shown on Pascal VOC.',\n",
       "  'authors': ['Christian Szegedy ', ' Alexander Toshev ', ' Dumitru Erhan'],\n",
       "  'date': '2013',\n",
       "  'identifier': '2130306094',\n",
       "  'references': ['2618530766',\n",
       "   '2161969291',\n",
       "   '2108598243',\n",
       "   '2168356304',\n",
       "   '2146502635',\n",
       "   '2100495367',\n",
       "   '2031489346',\n",
       "   '2072128103',\n",
       "   '2022508996',\n",
       "   '2167510172'],\n",
       "  'title': 'Deep Neural Networks for Object Detection'},\n",
       " {'abstract': 'Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyperparameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.',\n",
       "  'authors': ['Yoshua Bengio'],\n",
       "  'date': '2012',\n",
       "  'identifier': '1806891645',\n",
       "  'references': ['2136922672',\n",
       "   '2146502635',\n",
       "   '2310919327',\n",
       "   '2158899491',\n",
       "   '2187089797',\n",
       "   '1665214252',\n",
       "   '2072128103',\n",
       "   '1533861849',\n",
       "   '2912934387',\n",
       "   '2001141328'],\n",
       "  'title': 'Practical recommendations for gradient-based training of deep architectures'},\n",
       " {'abstract': 'Anomaly detection is an important problem that has been researched within diverse research areas and application domains. Many anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key assumptions, which are used by the techniques to differentiate between normal and anomalous behavior. When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and more succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category. We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with.',\n",
       "  'authors': ['Varun Chandola ', ' Arindam Banerjee ', ' Vipin Kumar'],\n",
       "  'date': '2009',\n",
       "  'identifier': '2122646361',\n",
       "  'references': ['2156909104',\n",
       "   '2148694408',\n",
       "   '1565377632',\n",
       "   '2147880316',\n",
       "   '1679913846',\n",
       "   '1673310716',\n",
       "   '2162800060',\n",
       "   '2158454296',\n",
       "   '2132870739',\n",
       "   '1971784203'],\n",
       "  'title': 'Anomaly detection: A survey'},\n",
       " {'abstract': 'The Elias source coding scheme is modified to permit a source sequence of practically unlimited length to be coded as a single codeword using arithmetic of only limited precision. The result is shown to be a nonblock arithmetic code of the first in, first out (FIFO) type-- source symbols are decoded in the same order as they were encoded. Codeword lengths which are near optimum for the specified statistical properties of the source can be achieved. Explicit encoding and decoding algorithms are Provided which effectively implement the coding scheme. Applications to data compression and cryptography are suggested.',\n",
       "  'authors': ['C. Jones'],\n",
       "  'date': '1981',\n",
       "  'identifier': '2006384477',\n",
       "  'references': ['2156186849',\n",
       "   '1995875735',\n",
       "   '2171545223',\n",
       "   '2911940095',\n",
       "   '2046419776',\n",
       "   '1992371956',\n",
       "   '158805393',\n",
       "   '2103206811',\n",
       "   '2126711987',\n",
       "   '2013094118'],\n",
       "  'title': 'An efficient coding system for long source sequences'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Dana H. Ballard'],\n",
       "  'date': '1991',\n",
       "  'identifier': '2913703059',\n",
       "  'references': ['2914885528',\n",
       "   '2528268528',\n",
       "   '2225341423',\n",
       "   '2415527960',\n",
       "   '2125756925',\n",
       "   '168365783',\n",
       "   '2119204143',\n",
       "   '1481882669',\n",
       "   '2172373809',\n",
       "   '1996348120'],\n",
       "  'title': 'Animate vision'},\n",
       " {'abstract': 'A performance measure is derived for a multiclass hierarchical classifier under the assumption that a maximum likelihood rule is used at each node and the features at different nodes of the tree are class-conditionally statistically independent. The mean accuracy of an estimated hierarchical classifier is then defined as its performance averaged across all classification problems, when an estimated decision rule is used at every node. For a balanced binary decision tree, it is shown that there exists an optimum number of quantization levels for the features which maximizes the mean accuracy. The optimum quantization level increases with the number of training samples per class available to estimate the node decisions and is a nondecreasing function of the depth of the tree.',\n",
       "  'authors': ['Kulkarni'],\n",
       "  'date': '1978',\n",
       "  'identifier': '2086294262',\n",
       "  'references': ['2098057602',\n",
       "   '1970361289',\n",
       "   '2065283057',\n",
       "   '1579570236',\n",
       "   '1970854614',\n",
       "   '795398596',\n",
       "   '2000367563',\n",
       "   '2114446222'],\n",
       "  'title': 'On the Mean Accuracy of Hierarchical Classifiers'},\n",
       " {'abstract': \"The perceptual recognition of objects is conceptualized to be a process in which the image of the input is segmented at regions of deep concavity into an arrangement of simple geometric components, such as blocks, cylinders, wedges, and cones. The fundamental assumption of the proposed theory, recognition-by-components (RBC), is that a modest set of generalized-cone components, called geons (N £ 36), can be derived from contrasts of five readily detectable properties of edges in a two-dimensiona l image: curvature, collinearity, symmetry, parallelism, and cotermination. The detection of these properties is generally invariant over viewing position an$ image quality and consequently allows robust object perception when the image is projected from a novel viewpoint or is degraded. RBC thus provides a principled account of the heretofore undecided relation between the classic principles of perceptual organization and pattern recognition: The constraints toward regularization (Pragnanz) characterize not the complete object but the object's components. Representational power derives from an allowance of free combinations of the geons. A Principle of Componential Recovery can account for the major phenomena of object recognition: If an arrangement of two or three geons can be recovered from the input, objects can be quickly recognized even when they are occluded, novel, rotated in depth, or extensively degraded. The results from experiments on the perception of briefly presented pictures by human observers provide empirical support for the theory. Any single object can project an infinity of image configurations to the retina. The orientation of the object to the viewer can vary continuously, each giving rise to a different two-dimensional projection. The object can be occluded by other objects or texture fields, as when viewed behind foliage. The object need not be presented as a full-colored textured image but instead can be a simplified line drawing. Moreover, the object can even be missing some of its parts or be a novel exemplar of its particular category. But it is only with rare exceptions that an image fails to be rapidly and readily classified, either as an instance of a familiar object category or as an instance that cannot be so classified (itself a form of classification).\",\n",
       "  'authors': ['Irving Biederman'],\n",
       "  'date': '1987',\n",
       "  'identifier': '2156406284',\n",
       "  'references': ['2740373864',\n",
       "   '2149095485',\n",
       "   '2059975159',\n",
       "   '2073257493',\n",
       "   '1513966746',\n",
       "   '2032533296',\n",
       "   '2059799772',\n",
       "   '2125756925',\n",
       "   '1501418839',\n",
       "   '2037732452'],\n",
       "  'title': 'Recognition-by-Components: A Theory of Human Image Understanding.'},\n",
       " {'abstract': 'Real-world learning tasks may involve high-dimensional data sets with arbitrary patterns of missing data. In this paper we present a framework based on maximum likelihood density estimation for learning from such data set.s. We use mixture models for the density estimates and make two distinct appeals to the Expectation-Maximization (EM) principle (Dempster et al., 1977) in deriving a learning algorithm--EM is used both for the estimation of mixture components and for coping with missing data. The resulting algorithm is applicable to a wide range of supervised as well as unsupervised learning problems. Results from a classification benchmark--the iris data set--are presented.',\n",
       "  'authors': ['Zoubin Ghahramani ', ' Michael I. Jordan'],\n",
       "  'date': '1993',\n",
       "  'identifier': '2128221272',\n",
       "  'references': ['3085162807',\n",
       "   '2049633694',\n",
       "   '2044758663',\n",
       "   '1594031697',\n",
       "   '2102201073',\n",
       "   '3017143921',\n",
       "   '2150884987',\n",
       "   '2025653905',\n",
       "   '2149723649',\n",
       "   '1992402718'],\n",
       "  'title': 'Supervised learning from incomplete data via an EM approach'},\n",
       " {'abstract': \"General Introduction Introduction History of Mixture Models Background to the General Classification Problem Mixture Likelihood Approach to Clustering Identifiability Likelihood Estimation for Mixture Models via EM Algorithm Start Values for EMm Algorithm Properties of Likelihood Estimators for Mixture Models Information Matrix for Mixture Models Tests for the Number of Components in a Mixture Partial Classification of the Data Classification Likelihood Approach to Clustering Mixture Models with Normal Components Likelihood Estimation for a Mixture of Normal Distribution Normal Homoscedastic Components Asymptotic Relative Efficiency of the Mixture Likelihood Approach Expected and Observed Information Matrices Assessment of Normality for Component Distributions: Partially Classified Data Assessment of Typicality: Partially Classified Data Assessment of Normality and Typicality: Unclassified Data Robust Estimation for Mixture Models Applications of Mixture Models to Two-Way Data Sets Introduction Clustering of Hemophilia Data Outliers in Darwin's Data Clustering of Rare Events Latent Classes of Teaching Styles Estimation of Mixing Proportions Introduction Likelihood Estimation Discriminant Analysis Estimator Asymptotic Relative Efficiency of Discriminant Analysis Estimator Moment Estimators Minimum Distance Estimators Case Study Homogeneity of Mixing Proportions Assessing the Performance of the Mixture Likelihood Approach to Clustering Introduction Estimators of the Allocation Rates Bias Correction of the Estimated Allocation Rates Estimated Allocation Rates of Hemophilia Data Estimated Allocation Rates for Simulated Data Other Methods of Bias Corrections Bias Correction for Estimated Posterior Probabilities Partitioning of Treatment Means in ANOVA Introduction Clustering of Treatment Means by the Mixture Likelihood Approach Fitting of a Normal Mixture Model to a RCBD with Random Block Effects Some Other Methods of Partitioning Treatment Means Example 1 Example 2 Example 3 Example 4 Mixture Likelihood Approach to the Clustering of Three-Way Data Introduction Fitting a Normal Mixture Model to Three-Way Data Clustering of Soybean Data Multidimensional Scaling Approach to the Analysis of Soybean Data References Appendix\",\n",
       "  'authors': ['Geoffrey J. McLachlan ', ' Kaye E. Basford'],\n",
       "  'date': '1988',\n",
       "  'identifier': '1992402718',\n",
       "  'references': ['2140190241',\n",
       "   '2011430131',\n",
       "   '1501500081',\n",
       "   '2011832962',\n",
       "   '3110717804',\n",
       "   '2063532964',\n",
       "   '2150884987',\n",
       "   '2110802877',\n",
       "   '2115627867'],\n",
       "  'title': 'Mixture models : inference and applications to clustering'},\n",
       " {'abstract': 'We design low-density parity-check (LDPC) codes that perform at rates extremely close to the Shannon capacity. The codes are built from highly irregular bipartite graphs with carefully chosen degree patterns on both sides. Our theoretical analysis of the codes is based on the work of Richardson and Urbanke (see ibid., vol.47, no.2, p.599-618, 2000). Assuming that the underlying communication channel is symmetric, we prove that the probability densities at the message nodes of the graph possess a certain symmetry. Using this symmetry property we then show that, under the assumption of no cycles, the message densities always converge as the number of iterations tends to infinity. Furthermore, we prove a stability condition which implies an upper bound on the fraction of errors that a belief-propagation decoder can correct when applied to a code induced from a bipartite graph with a given degree distribution. Our codes are found by optimizing the degree structure of the underlying graphs. We develop several strategies to perform this optimization. We also present some simulation results for the codes found which show that the performance of the codes is very close to the asymptotic theoretical bounds.',\n",
       "  'authors': ['T.J. Richardson 1', ' M.A. Shokrollahi 2', ' R.L. Urbanke 1'],\n",
       "  'date': '2001',\n",
       "  'identifier': '2127490352',\n",
       "  'references': ['1595159159',\n",
       "   '2121606987',\n",
       "   '2169732368',\n",
       "   '2135764410',\n",
       "   '2029712200',\n",
       "   '2140655888',\n",
       "   '2126259959',\n",
       "   '2087040279',\n",
       "   '2029268759',\n",
       "   '2158669549'],\n",
       "  'title': 'Design of capacity-approaching irregular low-density parity-check codes'},\n",
       " {'abstract': 'We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text. Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited-domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction). In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric. By being fictional, the answer typically can be found only in the story itself. The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task. We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions. By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but at one tenth the editing cost. By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text. 1 Reading Comprehension A major goal for NLP is for machines to be able to understand text as well as people. Several research disciplines are focused on this problem: for example, information extraction, relation extraction, semantic role labeling, and recognizing textual entailment. Yet these techniques are necessarily evaluated individually, rather than by how much they advance us towards the end goal. On the other hand, the goal of semantic parsing is the machine comprehension of text (MCT), yet its evaluation requires adherence to a specific knowledge representation, and it is currently unclear what the best representation is, for open-domain text. We believe that it is useful to directly tackle the top-level task of MCT. For this, we need a way to measure progress. One common method for evaluating someone’s understanding of text is by giving them a multiple-choice reading comprehension test. This has the advantage that it is objectively gradable (vs. essays) yet may test a range of abilities such as causal or counterfactual reasoning, inference among relations, or just basic understanding of the world in which the passage is set. Therefore, we propose a multiple-choice reading comprehension task as a way to evaluate progress on MCT. We have built a reading comprehension dataset containing 500 fictional stories, with 4 multiple choice questions per story. It was built using methods which can easily scale to at least 5000 stories, since the stories were created, and the curation was done, using crowd sourcing almost entirely, at a total of $4.00 per story. We plan to periodically update the dataset to ensure that methods are not overfitting to the existing data. The dataset is open-domain, yet restricted to concepts and words that a 7 year old is expected to understand. This task is still beyond the capability of today’s computers and algorithms.',\n",
       "  'authors': ['Matthew Richardson ',\n",
       "   ' Christopher J.C. Burges ',\n",
       "   ' Erin Renshaw'],\n",
       "  'date': '2013',\n",
       "  'identifier': '2125436846',\n",
       "  'references': ['2106568252',\n",
       "   '2525127255',\n",
       "   '2126631960',\n",
       "   '2167090521',\n",
       "   '2103001613',\n",
       "   '1979532929',\n",
       "   '2989499211',\n",
       "   '2096979215',\n",
       "   '2121300346',\n",
       "   '2142898321'],\n",
       "  'title': 'MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text'},\n",
       " {'abstract': 'The network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. We develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their effectiveness in a variety of context on the World Wide Web. The central issue we address within our framework is the distillation of broad search topics, through the discovery of “authorative” information sources on such topics. We propose and test an algorithmic formulation of the notion of authority, based on the relationship between a set of relevant authoritative pages and the set of “hub pages” that join them together in the link structure. Our formulation has connections to the eigenvectors of certain matrices associated with the link graph; these connections in turn motivate additional heuristrics for link-based analysis.',\n",
       "  'authors': ['Jon M. Kleinberg'],\n",
       "  'date': '1999',\n",
       "  'identifier': '2138621811',\n",
       "  'references': ['2148694408',\n",
       "   '3013264884',\n",
       "   '2798909945',\n",
       "   '2147152072',\n",
       "   '1578099820',\n",
       "   '2006119904',\n",
       "   '2089192108',\n",
       "   '1568713441',\n",
       "   '2079672501',\n",
       "   '1996764654'],\n",
       "  'title': 'Authoritative sources in a hyperlinked environment'},\n",
       " {'abstract': 'For single databases, primary hindrances for end-user access are the volume of data that is becoming available, the lack of abstraction, and the need to understand the representation of the data. When information is combined from multiple databases, the major concern is the mismatch encountered in information representation and structure. Intelligent and active use of information requires a class of software modules that mediate between the workstation applications and the databases. It is shown that mediation simplifies, abstracts, reduces, merges, and explains data. A mediator is a software module that exploits encoded knowledge about certain sets or subsets of data to create information for a higher layer of applications. A model of information processing and information system components is described. The mediator architecture, including mediator interfaces, sharing of mediator modules, distribution of mediators, and triggers for knowledge maintenance, are discussed. >',\n",
       "  'authors': ['G. Wiederhold'],\n",
       "  'date': '1992',\n",
       "  'identifier': '2141824507',\n",
       "  'references': ['144096106',\n",
       "   '2005649826',\n",
       "   '2293540547',\n",
       "   '3110717095',\n",
       "   '2092849686',\n",
       "   '2043351574',\n",
       "   '2131062488',\n",
       "   '2119285119',\n",
       "   '1602357614',\n",
       "   '1558580417'],\n",
       "  'title': 'Mediators in the architecture of future information systems'},\n",
       " {'abstract': 'We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application.',\n",
       "  'authors': ['Yann LeCun ', ' John S. Denker ', ' Sara A. Solla'],\n",
       "  'date': '1989',\n",
       "  'identifier': '2114766824',\n",
       "  'references': ['2147800946',\n",
       "   '2165758113',\n",
       "   '2154579312',\n",
       "   '1533169541',\n",
       "   '169539560',\n",
       "   '19621276',\n",
       "   '56903235',\n",
       "   '2134273960',\n",
       "   '2169163929',\n",
       "   '2120972216'],\n",
       "  'title': 'Optimal Brain Damage'},\n",
       " {'abstract': '',\n",
       "  'authors': ['DE Rumelhart ', ' GE Hinton ', ' RJ William'],\n",
       "  'date': '1986',\n",
       "  'identifier': '2504871398',\n",
       "  'references': ['2119821739',\n",
       "   '1787224781',\n",
       "   '2109722477',\n",
       "   '2914746235',\n",
       "   '2160958420',\n",
       "   '2218318129',\n",
       "   '2105728138',\n",
       "   '2124537004',\n",
       "   '1613249581',\n",
       "   '2803163155'],\n",
       "  'title': 'Learning representations by back-propagation errors, nature'},\n",
       " {'abstract': 'Several variations are given on the construction of orthonormal bases of wavelets with compact support. They have, respectively, more symmetry, more regularity, or more vanishing moments for the scaling function than the examples constructed in Daubechies [Comm. Pure Appl. Math., 41 (1988), pp. 909–996].',\n",
       "  'authors': ['Ingrid Daubechies'],\n",
       "  'date': '1993',\n",
       "  'identifier': '2140832824',\n",
       "  'references': ['2062024414',\n",
       "   '2098914003',\n",
       "   '2094585768',\n",
       "   '2018016896',\n",
       "   '2026891750',\n",
       "   '2091859624',\n",
       "   '2039921213'],\n",
       "  'title': 'Orthonormal bases of compactly supported wavelets II: variations on a theme'},\n",
       " {'abstract': 'This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.',\n",
       "  'authors': ['Ross Girshick'],\n",
       "  'date': '2015',\n",
       "  'identifier': '1536680647',\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2102605133',\n",
       "   '2108598243',\n",
       "   '2155893237',\n",
       "   '2168356304',\n",
       "   '2164598857',\n",
       "   '1861492603',\n",
       "   '2963542991',\n",
       "   '2109255472'],\n",
       "  'title': 'Fast R-CNN'},\n",
       " {'abstract': 'We construct orthonormal bases of compactly supported wavelets, with arbitrarily high regularity. The order of regularity increases linearly with the support width. We start by reviewing the concept of multiresolution analysis as well as several algorithms in vision decomposition and reconstruction. The construction then follows from a synthesis of these different approaches.',\n",
       "  'authors': ['Ingrid Daubechies'],\n",
       "  'date': '1988',\n",
       "  'identifier': '2098914003',\n",
       "  'references': ['2103504761',\n",
       "   '1980149518',\n",
       "   '2096684483',\n",
       "   '2043583598',\n",
       "   '2087377426',\n",
       "   '2013987111',\n",
       "   '1975474302',\n",
       "   '2039942287',\n",
       "   '1989491465',\n",
       "   '2033966461'],\n",
       "  'title': 'Orthonormal bases of compactly supported wavelets'},\n",
       " {'abstract': 'Correlation is a statistical method used to assess a possible linear association between two continuous variables. It is simple both to calculate and to interpret. However, misuse of correlation is so common among researchers that some statisticians have wished that the method had never been devised at all. The aim of this article is to provide a guide to appropriate use of correlation in medical research and to highlight some misuse. Examples of the applications of the correlation coefficient have been provided using data from statistical simulations as well as real data. Rule of thumb for interpreting size of a correlation coefficient has been provided.',\n",
       "  'authors': ['Mavuto Mukaka'],\n",
       "  'date': '2012',\n",
       "  'identifier': '1531237901',\n",
       "  'references': ['2118202495',\n",
       "   '2133750711',\n",
       "   '2002664886',\n",
       "   '2040432867',\n",
       "   '2045228286'],\n",
       "  'title': 'Statistics corner: A guide to appropriate use of correlation coefficient in medical research.'},\n",
       " {'abstract': 'During the past few years several monochromeimage transform-coding systems have been developed. In these systems, a quantized and coded version of a spatial unitary transform of an image is transmitted over a channel, rather than an image itself. In this paper the transform-coding concept has been applied to the coding of color images represented by three primary color planes of data. The principles of spatial transform coding are reviewed and the merits of various methods of color-image representation are discussed. A performance analysis is presented for the color-image transform-coding system. Results of a computer simulation of the coding system are also given. It is shown that, by transform coding, the chrominance content of a color image can be coded with an average of 1.0 bits per element or less without serious degradation. If luminance coding is also employed, the average rate reduces to about 2.0 bits per element or less.',\n",
       "  'authors': ['W. Pratt'],\n",
       "  'date': '1971',\n",
       "  'identifier': '2161816980',\n",
       "  'references': ['2134809980',\n",
       "   '2160050411',\n",
       "   '2172074673',\n",
       "   '2150515765',\n",
       "   '2046682096',\n",
       "   '2061674564',\n",
       "   '2001251887',\n",
       "   '2003577166',\n",
       "   '2044913388',\n",
       "   '1982841155'],\n",
       "  'title': 'Spatial Transform Coding of Color Images'},\n",
       " {'abstract': 'Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.',\n",
       "  'authors': ['Jeffrey Pennington 1',\n",
       "   ' Richard Socher 2',\n",
       "   ' Christopher Manning 1'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2250539671',\n",
       "  'references': ['2153579005',\n",
       "   '1614298861',\n",
       "   '2146502635',\n",
       "   '2158899491',\n",
       "   '2072128103',\n",
       "   '2141599568',\n",
       "   '2117130368',\n",
       "   '2118020653',\n",
       "   '2132339004',\n",
       "   '2158139315'],\n",
       "  'title': 'Glove: Global Vectors for Word Representation'},\n",
       " {'abstract': 'The CoNLL-2012 shared task involved predicting coreference in three languages -- English, Chinese and Arabic -- using OntoNotes data. It was a follow-on to the English-only task organized in 2011. Until the creation of the OntoNotes corpus, resources in this subfield of language processing have tended to be limited to noun phrase coreference, often on a restricted set of entities, such as ACE entities. OntoNotes provides a large-scale corpus of general anaphoric coreference not restricted to noun phrases or to a specified set of entity types and covering multiple languages. OntoNotes also provides additional layers of integrated annotation, capturing additional shallow semantic structure. This paper briefly describes the OntoNotes annotation (coreference and other layers) and then describes the parameters of the shared task including the format, pre-processing information, evaluation criteria, and presents and discusses the results achieved by the participating systems. Being a task that has a complex evaluation history, and multiple evalation conditions, it has, in the past, been difficult to judge the improvement in new algorithms over previously reported results. Having a standard test set and evaluation parameters, all based on a resource that provides multiple integrated annotation layers (parses, semantic roles, word senses, named entities and coreference) that could support joint models, should help to energize ongoing research in the task of entity and event coreference.',\n",
       "  'authors': ['Sameer Pradhan 1',\n",
       "   ' Alessandro Moschitti 2',\n",
       "   ' Nianwen Xue 3',\n",
       "   ' Olga Uryupina 2',\n",
       "   ' Yuchen Zhang 3'],\n",
       "  'date': '2012',\n",
       "  'identifier': '2155069789',\n",
       "  'references': ['1632114991',\n",
       "   '2158847908',\n",
       "   '2088911157',\n",
       "   '2164455818',\n",
       "   '2125712079',\n",
       "   '2129657639',\n",
       "   '108437174',\n",
       "   '1970849810',\n",
       "   '2017875634',\n",
       "   '2407338347'],\n",
       "  'title': 'CoNLL-2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes'},\n",
       " {'abstract': 'Many optimization problems in various fields have been solved using diverse optimization al gorithms. Traditional optimization techniques such as linear programming (LP), non-linear programming (NL...',\n",
       "  'authors': ['Zong Woo Geem 1', ' Joong Hoon Kim 2', ' G.V. Loganathan 3'],\n",
       "  'date': '2001',\n",
       "  'identifier': '1993885071',\n",
       "  'references': ['2581275558',\n",
       "   '1997621454',\n",
       "   '2093505886',\n",
       "   '1983966343',\n",
       "   '2056760934',\n",
       "   '2011703887',\n",
       "   '2011944551',\n",
       "   '2164855953',\n",
       "   '2047115136',\n",
       "   '1997319433'],\n",
       "  'title': 'A New Heuristic Optimization Algorithm: Harmony Search'},\n",
       " {'abstract': '',\n",
       "  'authors': ['G. M. Raab ', ' A. J. Miller'],\n",
       "  'date': '1991',\n",
       "  'identifier': '2797502950',\n",
       "  'references': ['2158940042',\n",
       "   '2017337590',\n",
       "   '2135695572',\n",
       "   '1495061682',\n",
       "   '1603903339',\n",
       "   '1689445748',\n",
       "   '2127300249',\n",
       "   '1506069954',\n",
       "   '2137226992',\n",
       "   '2007069447'],\n",
       "  'title': 'Subset Selection in Regression.'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Schouhamer Immink ', ' A Kees'],\n",
       "  'date': '2004',\n",
       "  'identifier': '1579764623',\n",
       "  'references': ['2798333393',\n",
       "   '1669285020',\n",
       "   '1480210121',\n",
       "   '1560089794',\n",
       "   '2122007740',\n",
       "   '1992987784',\n",
       "   '2056021806',\n",
       "   '1995875735',\n",
       "   '2075379212',\n",
       "   '2259948355'],\n",
       "  'title': 'Codes for mass data storage systems'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Richard E. Blahut'],\n",
       "  'date': '1987',\n",
       "  'identifier': '1560089794',\n",
       "  'references': ['1660562555',\n",
       "   '2099741732',\n",
       "   '2106749358',\n",
       "   '2124035649',\n",
       "   '2111616148',\n",
       "   '2129244720',\n",
       "   '2044535354',\n",
       "   '2106833918',\n",
       "   '2161439181'],\n",
       "  'title': 'Principles and practice of information theory'},\n",
       " {'abstract': \"Several theoretical, computational, and experimental studies suggest that neurons encode sensory information using a small number of active neurons at any given point in time. This strategy, referred to as 'sparse coding', could possibly confer several advantages. First, it allows for increased storage capacity in associative memories; second, it makes the structure in natural signals explicit; third, it represents complex data in a way that is easier to read out at subsequent levels of processing; and fourth, it saves energy. Recent physiological recordings from sensory neurons have indicated that sparse coding could be a ubiquitous strategy employed in several different modalities across different organisms.\",\n",
       "  'authors': ['Bruno A Olshausen 1', ' David J Field 2'],\n",
       "  'date': '2004',\n",
       "  'identifier': '2074376560',\n",
       "  'references': ['2053186076',\n",
       "   '2145889472',\n",
       "   '1536929369',\n",
       "   '2105464873',\n",
       "   '2137234026',\n",
       "   '2140499889',\n",
       "   '2107790757',\n",
       "   '2180838288',\n",
       "   '2151035455',\n",
       "   '2042422091'],\n",
       "  'title': 'Sparse coding of sensory inputs.'},\n",
       " {'abstract': 'In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line.',\n",
       "  'authors': ['Yoav Freund ', ' Robert E Schapire'],\n",
       "  'date': '1997',\n",
       "  'identifier': '1988790447',\n",
       "  'references': ['2112076978',\n",
       "   '1966280301',\n",
       "   '1676820704',\n",
       "   '1530699444',\n",
       "   '2165758113',\n",
       "   '2093717447',\n",
       "   '2093825590',\n",
       "   '2070534370',\n",
       "   '1520252399',\n",
       "   '2104364170'],\n",
       "  'title': 'A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting'},\n",
       " {'abstract': 'We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.',\n",
       "  'authors': ['John Duchi 1', ' Elad Hazan 2', ' Yoram Singer 3'],\n",
       "  'date': '2011',\n",
       "  'identifier': '2146502635',\n",
       "  'references': ['2296319761',\n",
       "   '2108598243',\n",
       "   '3120740533',\n",
       "   '2798766386',\n",
       "   '2610857016',\n",
       "   '2167732364',\n",
       "   '2150102617',\n",
       "   '2124541940',\n",
       "   '1992208280',\n",
       "   '2160218441'],\n",
       "  'title': 'Adaptive Subgradient Methods for Online Learning and Stochastic Optimization'},\n",
       " {'abstract': 'Recommendation systems make suggestions about artifacts to a user. For instance, they may predict whether a user would be interested in seeing a particular movie. Social recomendation methods collect ratings of artifacts from many individuals, and use nearest-neighbor techniques to make recommendations to a user concerning new artifacts. However, these methods do not use the significant amount of other information that is often available about the nature of each artifact - such as cast lists o r movie reviews, for example. This paper presents an inductive learning approach to recommendation that is able to use both ratings information and other forms of information about each artifact in predicting user preferences. We show that our method outperforms an existing social-filtering method in the domain of movie recommendations on a dataset of more than 45,000 movie ratings collected from a community of over 250 users.',\n",
       "  'authors': ['Chumki Basu 1', ' Haym Hirsh 2', ' William Cohen 3'],\n",
       "  'date': '1998',\n",
       "  'identifier': '2124029832',\n",
       "  'references': ['2124591829',\n",
       "   '2043403353',\n",
       "   '1670263352',\n",
       "   '2030144199',\n",
       "   '1493526108',\n",
       "   '1587718046',\n",
       "   '2919963289',\n",
       "   '1517178556',\n",
       "   '2165695298',\n",
       "   '2028962062'],\n",
       "  'title': 'Recommendation as classification: using social and content-based information in recommendation'},\n",
       " {'abstract': \"Breiman's bagging and Freund and Schapire's boosting are recent methods for improving the predictive power of classifier learning systems. Both form a set of classifiers that are combined by voting, bagging by generating replicated bootstrap samples of the data, and boosting by adjusting the weights of training instances. This paper reports results of applying both techniques to a system that learns decision trees and testing on a representative collection of datasets. While both approaches substantially improve predictive accuracy, boosting shows the greater benefit. On the other hand, boosting also produces severe degradation on some datasets. A small change to the way that boosting combines the votes of learned classifiers reduces this downside and also leads to slightly better results on most of the datasets considered.\",\n",
       "  'authors': ['J. R. Quinlan'],\n",
       "  'date': '1996',\n",
       "  'identifier': '1966280301',\n",
       "  'references': ['1988790447',\n",
       "   '2912934387',\n",
       "   '3085162807',\n",
       "   '2112076978',\n",
       "   '1676820704',\n",
       "   '1530699444',\n",
       "   '2134696506',\n",
       "   '1567276288',\n",
       "   '1539741229',\n",
       "   '1510806966'],\n",
       "  'title': 'Bagging, boosting, and C4.S'},\n",
       " {'abstract': 'A model for aspects of visual attention based on the concept of selective tuning is presented. It provides for a solution to the problems of selection in an image, information routing through the visual processing hierarchy and task-specific attentional bias. The central thesis is that attention acts to optimize the search procedure inherent in a solution to vision. It does so by selectively tuning the visual processing network which is accomplished by a top-down hierarchy of winner-take-all processes embedded within the visual processing pyramid. Comparisons to other major computational models of attention and to the relevant neurobiology are included in detail throughout the paper. The model has been implemented; several examples of its performance are shown. This model is a hypothesis for primate visual attention, but it also outperforms existing computational solutions for attention in machine vision and is highly appropriate to solving the problem in a robot vision system.',\n",
       "  'authors': ['John K. Tsotsos ',\n",
       "   ' Sean M. Culhane ',\n",
       "   ' Winky Yan Kei Wai ',\n",
       "   ' Yuzhong Lai ',\n",
       "   ' Neal Davis ',\n",
       "   ' Fernando Nuflo'],\n",
       "  'date': '1995',\n",
       "  'identifier': '2089597841',\n",
       "  'references': ['2914885528',\n",
       "   '1497599070',\n",
       "   '2131605659',\n",
       "   '2115441154',\n",
       "   '2032774941',\n",
       "   '2112325651',\n",
       "   '2012521891',\n",
       "   '2121487911',\n",
       "   '2033118251',\n",
       "   '1853938811'],\n",
       "  'title': 'Modeling visual attention via selective tuning'},\n",
       " {'abstract': 'We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes ...',\n",
       "  'authors': ['Yee Whye Teh ',\n",
       "   ' Michael I. Jordan ',\n",
       "   ' Matthew J. Beal ',\n",
       "   ' David M. Blei'],\n",
       "  'date': '2006',\n",
       "  'identifier': '2158266063',\n",
       "  'references': ['1880262756',\n",
       "   '2098126593',\n",
       "   '2125838338',\n",
       "   '2152664025',\n",
       "   '2110755408',\n",
       "   '2009570821',\n",
       "   '1956559956',\n",
       "   '2011832962',\n",
       "   '2890040444',\n",
       "   '2115979064'],\n",
       "  'title': 'Hierarchical Dirichlet Processes'},\n",
       " {'abstract': \"A tutorial on the design and development of automatic speaker-recognition systems is presented. Automatic speaker recognition is the use of a machine to recognize a person from a spoken phrase. These systems can operate in two modes: to identify a particular person or to verify a person's claimed identity. Speech processing and the basic components of automatic speaker-recognition systems are shown and design tradeoffs are discussed. Then, a new automatic speaker-recognition system is given. This recognizer performs with 98.9% correct decalcification. Last, the performances of various systems are compared.\",\n",
       "  'authors': ['Jr. J.P. Campbell'],\n",
       "  'date': '1997',\n",
       "  'identifier': '2129244720',\n",
       "  'references': ['2125838338',\n",
       "   '1560013842',\n",
       "   '1766888123',\n",
       "   '3017143921',\n",
       "   '2165880886',\n",
       "   '2105594594',\n",
       "   '2069883713',\n",
       "   '2111460811',\n",
       "   '2135346934',\n",
       "   '2069501481'],\n",
       "  'title': 'Speaker recognition: a tutorial'},\n",
       " {'abstract': 'In dimensions two and higher, wavelets can efficiently represent only a small range of the full diversity of interesting behaviour. In effect, wavelets are well adapted for pointlike phenomena, whe...',\n",
       "  'authors': ['Emmanuel J. Candès ', ' David L. Donoho'],\n",
       "  'date': '1999',\n",
       "  'identifier': '2066462711',\n",
       "  'references': ['2098914003',\n",
       "   '2103496339',\n",
       "   '2166116275',\n",
       "   '2033367330',\n",
       "   '2050880896',\n",
       "   '2091886411',\n",
       "   '2013987111',\n",
       "   '1527329021',\n",
       "   '2031299600',\n",
       "   '2044828368'],\n",
       "  'title': 'Ridgelets: a key to higher-dimensional intermittency?'},\n",
       " {'abstract': 'This paper introduces new tight frames of curvelets to address the problem of finding optimally sparse representations of objects with discontinuities along piecewise C 2 edges. Conceptually, the curvelet transform is a multiscale pyramid with many directions and positions at each length scale, and needle-shaped elements at fine scales. These elements have many useful geometric multiscale features that set them apart from classical multiscale representations such as wavelets. For instance, curvelets obey a parabolic scaling relation which says that at scale 2 -j , each element has an envelope that is aligned along a ridge of length 2 -j/2 and width 2 -j . We prove that curvelets provide an essentially optimal representation of typical objects f that are C 2 except for discontinuities along piecewise C 2 curves. Such representations are nearly as sparse as if f were not singular and turn out to be far more sparse than the wavelet decomposition of the object. For instance, the n-term partial reconstruction f C n obtained by selecting the n largest terms in the curvelet series obeys ∥f - f C n ∥ 2 L2 ≤ C . n -2 . (log n) 3 , n → ∞. This rate of convergence holds uniformly over a class of functions that are C 2 except for discontinuities along piecewise C 2 curves and is essentially optimal. In comparison, the squared error of n-term wavelet approximations only converges as n -1 as n → ∞, which is considerably worse than the optimal behavior.',\n",
       "  'authors': ['Emmanuel J. Candès 1', ' David L. Donoho 2'],\n",
       "  'date': '2004',\n",
       "  'identifier': '2069912449',\n",
       "  'references': ['2146842127',\n",
       "   '2132573196',\n",
       "   '2132680427',\n",
       "   '2109504624',\n",
       "   '2107790757',\n",
       "   '2066462711',\n",
       "   '1971103086',\n",
       "   '654435104',\n",
       "   '2050880896',\n",
       "   '2087377426'],\n",
       "  'title': 'New tight frames of curvelets and optimal representations of objects with piecewise C2 singularities'},\n",
       " {'abstract': 'We present and discuss several novel applications of deep learning for the physical layer. By interpreting a communications system as an autoencoder, we develop a fundamental new way to think about communications system design as an end-to-end reconstruction task that seeks to jointly optimize transmitter and receiver components in a single process. We show how this idea can be extended to networks of multiple transmitters and receivers and present the concept of radio transformer networks as a means to incorporate expert domain knowledge in the machine learning model. Lastly, we demonstrate the application of convolutional neural networks on raw IQ samples for modulation classification which achieves competitive accuracy with respect to traditional schemes relying on expert features. This paper is concluded with a discussion of open challenges and areas for future investigation.',\n",
       "  'authors': [\"Timothy O'Shea 1\", ' Jakob Hoydis 2'],\n",
       "  'date': '2017',\n",
       "  'identifier': '2734408173',\n",
       "  'references': ['2962835968',\n",
       "   '2964121744',\n",
       "   '2099471712',\n",
       "   '2095705004',\n",
       "   '2101234009',\n",
       "   '2155893237',\n",
       "   '1677182931',\n",
       "   '2136922672',\n",
       "   '2145417574',\n",
       "   '2187089797'],\n",
       "  'title': 'An Introduction to Deep Learning for the Physical Layer'},\n",
       " {'abstract': 'The authors show that a multiple-input, single-output, single-hidden-layer feedforward network with (known) hardwired connections from input to hidden layer, monotone squashing at the hidden layer and no squashing at the output embeds as a special case a so-called Fourier network, which yields a Fourier series approximation properties of Fourier series representations. In particular, approximation to any desired accuracy of any square integrable function can be achieved by such a network, using sufficiently many hidden units. In this sense, such networks do not make avoidable mistakes. >',\n",
       "  'authors': ['Gallant 1', ' White 2'],\n",
       "  'date': '1988',\n",
       "  'identifier': '1654142532',\n",
       "  'references': ['18965947', '2095301394'],\n",
       "  'title': 'There exists a neural network that does not make avoidable mistakes'},\n",
       " {'abstract': 'Recent work has shown that combining multiple versions of unstable classifiers such as trees or neural nets results in reduced test set error. One of the more effective is bagging. Here, modified training sets are formed by resampling from the original training set, classifiers constructed using these training sets and then combined by voting. Freund and Schapire propose an algorithm the basis of which is to adaptively resample and combine (hence the acronym arcing) so that the weights in the resampling are increased for those cases most often misclassified and the combining is done by weighted voting. Arcing is more successful than bagging in test set error reduction. We explore two arcing algorithms, compare them to each other and to bagging, and try to understand how arcing works. We introduce the definitions of bias and variance for a classifier as components of the test set error. Unstable classifiers can have low bias on a large range of data sets. Their problem is high variance. Combining multiple versions either through bagging or arcing reduces variance significantly.',\n",
       "  'authors': ['Leo Breiman'],\n",
       "  'date': '1998',\n",
       "  'identifier': '2067885219',\n",
       "  'references': ['2911964244',\n",
       "   '2053463056',\n",
       "   '1966701961',\n",
       "   '2075647286',\n",
       "   '3104887532',\n",
       "   '2167917621',\n",
       "   '2032210760',\n",
       "   '1540007258',\n",
       "   '2155806188',\n",
       "   '2168020168'],\n",
       "  'title': 'Arcing classifier (with discussion and a rejoinder by the author)'},\n",
       " {'abstract': 'We present a neural network-based face detection system. A retinally connected neural network examines small windows of an image, and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We use a bootstrap algorithm for training, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting non-face training examples, which must be chosen to span the entire space of non-face images. Comparisons with another state-of-the-art face detection system are presented; our system has better performance in terms of detection and false-positive rates.',\n",
       "  'authors': ['Henry A. Rowley ', ' Shumeet Baluja ', ' Takeo Kanade'],\n",
       "  'date': '1995',\n",
       "  'identifier': '2125713050',\n",
       "  'references': ['2138451337',\n",
       "   '2046079134',\n",
       "   '2159686933',\n",
       "   '2147800946',\n",
       "   '2098947662',\n",
       "   '2173629880',\n",
       "   '1575739010',\n",
       "   '2084844503',\n",
       "   '2056695679',\n",
       "   '1479911746'],\n",
       "  'title': 'Human Face Detection in Visual Scenes'},\n",
       " {'abstract': 'We describe a recursive algorithm to compute representations of functions with respect to nonorthogonal and possibly overcomplete dictionaries of elementary building blocks e.g. affine (wavelet) frames. We propose a modification to the matching pursuit algorithm of Mallat and Zhang (1992) that maintains full backward orthogonality of the residual (error) at every step and thereby leads to improved convergence. We refer to this modified algorithm as orthogonal matching pursuit (OMP). It is shown that all additional computation required for the OMP algorithm may be performed recursively. >',\n",
       "  'authors': ['Y.C. Pati ', ' R. Rezaiifar ', ' P.S. Krishnaprasad'],\n",
       "  'date': '1993',\n",
       "  'identifier': '2128659236',\n",
       "  'references': ['2151693816', '2084439265'],\n",
       "  'title': 'Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition'},\n",
       " {'abstract': 'We study automatic question generation for sentences from text passages in reading comprehension. We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentence- vs. paragraph-level information. In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead trainable end-to-end via sequence-to-sequence learning. Automatic evaluation results show that our system significantly outperforms the state-of-the-art rule-based system. In human evaluations, questions generated by our system are also rated as being more natural (i.e.,, grammaticality, fluency) and as more difficult to answer (in terms of syntactic and lexical divergence from the original text and reasoning needed to answer).',\n",
       "  'authors': ['Xinya Du ', ' Junru Shao ', ' Claire Cardie'],\n",
       "  'date': '2017',\n",
       "  'identifier': '2962717047',\n",
       "  'references': ['2964308564',\n",
       "   '2250539671',\n",
       "   '2130942839',\n",
       "   '2157331557',\n",
       "   '2064675550',\n",
       "   '2101105183',\n",
       "   '2963748441',\n",
       "   '2123442489',\n",
       "   '1902237438',\n",
       "   '1514535095'],\n",
       "  'title': 'Learning to Ask: Neural Question Generation for Reading Comprehension'},\n",
       " {'abstract': 'More than twelve years have elapsed since the first public release of WEKA. In that time, the software has been rewritten entirely from scratch, evolved substantially and now accompanies a text on data mining [35]. These days, WEKA enjoys widespread acceptance in both academia and business, has an active community, and has been downloaded more than 1.4 million times since being placed on Source-Forge in April 2000. This paper provides an introduction to the WEKA workbench, reviews the history of the project, and, in light of the recent 3.6 stable release, briefly discusses what has been added since the last stable version (Weka 3.4) released in 2003.',\n",
       "  'authors': ['Mark Hall 1',\n",
       "   ' Eibe Frank 2',\n",
       "   ' Geoffrey Holmes 2',\n",
       "   ' Bernhard Pfahringer 2',\n",
       "   ' Peter Reutemann 2',\n",
       "   ' Ian H. Witten 2'],\n",
       "  'date': '2009',\n",
       "  'identifier': '2133990480',\n",
       "  'references': ['2153635508',\n",
       "   '2966207845',\n",
       "   '1570448133',\n",
       "   '2118585731',\n",
       "   '3085162807',\n",
       "   '2123504579',\n",
       "   '1594031697',\n",
       "   '2110119381',\n",
       "   '2150757437',\n",
       "   '2135514714'],\n",
       "  'title': 'The WEKA data mining software: an update'},\n",
       " {'abstract': 'The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps us understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a Cp estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates.',\n",
       "  'authors': ['Bradley Efron 1',\n",
       "   ' Trevor Hastie 1',\n",
       "   ' Iain Johnstone 1',\n",
       "   ' Robert Tibshirani 1',\n",
       "   ' Hemant Ishwaran 2',\n",
       "   ' Keith Knight 3',\n",
       "   ' Jean Michel Loubes 4',\n",
       "   ' 5',\n",
       "   ' Pascal Massart 4',\n",
       "   ' 6',\n",
       "   ' David Madigan 7',\n",
       "   ' Greg Ridgeway 7',\n",
       "   ' 8',\n",
       "   ' Saharon Rosset 1',\n",
       "   ' 9',\n",
       "   ' J. I. Zhu 10',\n",
       "   ' Robert A. Stine 11',\n",
       "   ' Berwin A. Turlach 12',\n",
       "   ' Sanford Weisberg 13'],\n",
       "  'date': '2004',\n",
       "  'identifier': '2063978378',\n",
       "  'references': ['1554944419',\n",
       "   '2110065044',\n",
       "   '2135046866',\n",
       "   '1988790447',\n",
       "   '2912934387',\n",
       "   '1678356000',\n",
       "   '2798909945',\n",
       "   '3085162807',\n",
       "   '2158940042',\n",
       "   '1594031697'],\n",
       "  'title': 'Least angle regression'},\n",
       " {'abstract': 'We propose a novel approach for optical flow estimation, targeted at large displacements with significant occlusions. It consists of two steps: i) dense matching by edge-preserving interpolation from a sparse set of matches; ii) variational energy minimization initialized with the dense matches. The sparse-to-dense interpolation relies on an appropriate choice of the distance, namely an edge-aware geodesic distance. This distance is tailored to handle occlusions and motion boundaries - two common and difficult issues for optical flow computation. We also propose an approximation scheme for the geodesic distance to allow fast computation without loss of performance. Subsequent to the dense interpolation step, standard one-level variational energy minimization is carried out on the dense matches to obtain the final flow estimation. The proposed approach, called Edge-Preserving Interpolation of Correspondences (EpicFlow) is fast and robust to large displacements. It significantly outperforms the state of the art on MPI-Sintel and performs on par on Kitti and Middlebury.',\n",
       "  'authors': ['Jerome Revaud ',\n",
       "   ' Philippe Weinzaepfel ',\n",
       "   ' Zaid Harchaoui ',\n",
       "   ' Cordelia Schmid'],\n",
       "  'date': '2015',\n",
       "  'identifier': '1951289974',\n",
       "  'references': ['2033819227',\n",
       "   '2118246710',\n",
       "   '2110158442',\n",
       "   '2115579991',\n",
       "   '1496357020',\n",
       "   '2145023731',\n",
       "   '2147253850',\n",
       "   '1867429401',\n",
       "   '2131747574',\n",
       "   '2129587342'],\n",
       "  'title': 'EpicFlow: Edge-preserving interpolation of correspondences for optical flow'},\n",
       " {'abstract': 'The performance of a state-of-the-art neural network classifier for hand-written digits is compared to that of a k-nearest-neighbor classifier and to human performance. The neural network has a clear advantage over the k-nearest-neighbor method, but at the same time does not yet reach human performance. Two methods for combining neural-network ideas and the k-nearest-neighbor algorithm are proposed. Numerical experiments for these methods show an improvement in performance.',\n",
       "  'authors': ['J. Bromley ', ' E. Sackinger'],\n",
       "  'date': '1991',\n",
       "  'identifier': '1568787085',\n",
       "  'references': ['2119821739',\n",
       "   '2159737176',\n",
       "   '1548139318',\n",
       "   '2138882494',\n",
       "   '2277406607',\n",
       "   '1595098149',\n",
       "   '2010332406',\n",
       "   '2125145210',\n",
       "   '2415856871',\n",
       "   '1973224984'],\n",
       "  'title': 'Neural-Network and k-Nearest-neighbor Classifiers'},\n",
       " {'abstract': 'Wind farms can be analyzed using state estimation methods, which can be used to obtain its running state, including several aspects that cannot be easily obtained using other methods (e.g., capacitor bank aging) Using these methods on these types of networks is strongly affected by decoupling between active and reactive power and by a radial configuration, which is typical. For example, this decoupling affects its observability and robustness as well as the technical feasibility of the results. To overcome these drawbacks, an extended state estimation method is proposed in which the models for the different wind turbine technologies have been incorporated. These models have been mainly generated from measurement data using neural networks and polynomial fitting; these models do not require parameter values, which are rarely available from manufacturers. Furthermore, the resulting equations for modeling wind turbines are easily integrated into the state estimator due to their simplicity and derivatives.Thus, a method that guarantees feasible results, at least for wind turbines, was generated with increased observability robustness.',\n",
       "  'authors': ['Blanca Nieves Miranda-Blanco ',\n",
       "   ' Eloy Díaz-Dorado ',\n",
       "   ' Camilo Carrillo ',\n",
       "   ' J. Cidrás'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2019258010',\n",
       "  'references': ['2154642048',\n",
       "   '2103496339',\n",
       "   '2090483754',\n",
       "   '2093512919',\n",
       "   '1573503290',\n",
       "   '1879630343',\n",
       "   '2042546342',\n",
       "   '2256679588',\n",
       "   '2085491819',\n",
       "   '2153146514'],\n",
       "  'title': 'State estimation for wind farms including the wind turbine generator models'},\n",
       " {'abstract': 'The human visual system exhibits substantially different properties between foveal and peripheral vision. Peripheral vision is special in that it has to compress data onto fewer units by reduced visual acuity and larger receptive fields, yielding greatly reduced performance on many tasks such as object recognition. However, here we show that the pooling operations implemented by peripheral vision provide exactly the invariance properties required by a self-localization task. We test the effect of different pooling sizes, as well as acuity reduction, on localization, object recognition, and scene categorization tasks. We find that peripheral pooling, but not reduced acuity, affects localization performance positively, whereas it is detrimental to object recognition performance.',\n",
       "  'authors': ['Sven Eberhardt ', ' Christoph Zetzsche ', ' Kerstin Schill'],\n",
       "  'date': '2016',\n",
       "  'identifier': '2558231299',\n",
       "  'references': ['2618530766',\n",
       "   '2161969291',\n",
       "   '2162915993',\n",
       "   '2124386111',\n",
       "   '1566135517',\n",
       "   '2166049352',\n",
       "   '2026942141',\n",
       "   '2170505850',\n",
       "   '1484228140',\n",
       "   '2149194912'],\n",
       "  'title': 'Peripheral pooling is tuned to the localization task.'},\n",
       " {'abstract': 'Influence maximization, defined by Kempe et al. (SIGKDD 2003), is the problem of finding a small set of seed nodes in a social network that maximizes the spread of influence under certain influence cascade models. The scalability of influence maximization is a key factor for enabling prevalent viral marketing in large-scale online social networks. Prior solutions, such as the greedy algorithm of Kempe et al. (SIGKDD 2003) and its improvements are slow and not scalable, while other heuristic algorithms do not provide consistently good performance on influence spreads. In this article, we design a new heuristic algorithm that is easily scalable to millions of nodes and edges in our experiments. Our algorithm has a simple tunable parameter for users to control the balance between the running time and the influence spread of the algorithm. Our results from extensive simulations on several real-world and synthetic networks demonstrate that our algorithm is currently the best scalable solution to the influence maximization problem: (a) our algorithm scales beyond million-sized graphs where the greedy algorithm becomes infeasible, and (b) in all size ranges, our algorithm performs consistently well in influence spread—it is always among the best algorithms, and in most cases it significantly outperforms all other scalable heuristics to as much as 100–260% increase in influence spread.',\n",
       "  'authors': ['Chi Wang 1', ' Wei Chen 2', ' Yajun Wang 2'],\n",
       "  'date': '2012',\n",
       "  'identifier': '2144629587',\n",
       "  'references': ['3013264884',\n",
       "   '2402962589',\n",
       "   '2141403143',\n",
       "   '2108858998',\n",
       "   '2042123098',\n",
       "   '1984069252',\n",
       "   '2611804663',\n",
       "   '2021314079',\n",
       "   '2073926352',\n",
       "   '2143996311'],\n",
       "  'title': 'Scalable influence maximization for independent cascade model in large-scale social networks'},\n",
       " {'abstract': 'Linear algebra and matrix theory are fundamental tools in mathematical and physical science, as well as fertile fields for research. This new edition of the acclaimed text presents results of both classic and recent matrix analyses using canonical forms as a unifying theme, and demonstrates their importance in a variety of applications. The authors have thoroughly revised, updated, and expanded on the first edition. The book opens with an extended summary of useful concepts and facts and includes numerous new topics and features, such as: - New sections on the singular value and CS decompositions - New applications of the Jordan canonical form - A new section on the Weyr canonical form - Expanded treatments of inverse problems and of block matrices - A central role for the Von Neumann trace theorem - A new appendix with a modern list of canonical forms for a pair of Hermitian matrices and for a symmetric-skew symmetric pair - Expanded index with more than 3,500 entries for easy reference - More than 1,100 problems and exercises, many with hints, to reinforce understanding and develop auxiliary themes such as finite-dimensional quantum systems, the compound and adjugate matrices, and the Loewner ellipsoid - A new appendix provides a collection of problem-solving hints.',\n",
       "  'authors': ['Roger A. Horn 1', ' Charles R. Johnson 2'],\n",
       "  'date': '1985',\n",
       "  'identifier': '2610857016',\n",
       "  'references': ['2146502635',\n",
       "   '3029645440',\n",
       "   '2107396783',\n",
       "   '2118040894',\n",
       "   '2139212933',\n",
       "   '2053186076',\n",
       "   '2160643434',\n",
       "   '2165744313',\n",
       "   '2151795416'],\n",
       "  'title': 'Matrix Analysis'},\n",
       " {'abstract': 'We develop a new version of prospect theory that employs cumulative rather than separable decision weights and extends the theory in several respects. This version, called cumulative prospect theory, applies to uncertain as well as to risky prospects with any number of outcomes, and it allows different weighting functions for gains and for losses. Two principles, diminishing sensitivity and loss aversion, are invoked to explain the characteristic curvature of the value function and the weighting functions. A review of the experimental evidence and the results of a new experiment confirm a distinctive fourfold pattern of risk: risk aversion for gains and risk seeking for losses of high probability; risk seeking for gains and risk aversion for losses of low probability. Copyright 1992 by Kluwer Academic Publishers',\n",
       "  'authors': ['Amos Tversky 1', ' Daniel Kahneman 2'],\n",
       "  'date': '1992',\n",
       "  'identifier': '2041946752',\n",
       "  'references': ['2061592058',\n",
       "   '2133469585',\n",
       "   '3011865677',\n",
       "   '158727920',\n",
       "   '2030874315',\n",
       "   '2328418989',\n",
       "   '1986646649',\n",
       "   '2120733867',\n",
       "   '2016794508',\n",
       "   '2056819186'],\n",
       "  'title': 'Advances in prospect theory: cumulative representation of uncertainty'},\n",
       " {'abstract': \"Software development projects can be fun, productive, and even daring. Yet they can consistently deliver value to a business and remain under control.Extreme Programming (XP) was conceived and developed to address the specific needs of software development conducted by small teams in the face of vague and changing requirements. This new lightweight methodology challenges many conventional tenets, including the long-held assumption that the cost of changing a piece of software necessarily rises dramatically over the course of time. XP recognizes that projects have to work to achieve this reduction in cost and exploit the savings once they have been earned.Fundamentals of XP include: Distinguishing between the decisions to be made by business interests and those to be made by project stakeholders. Writing unit tests before programming and keeping all of the tests running at all times. Integrating and testing the whole system--several times a day. Producing all software in pairs, two programmers at one screen. Starting projects with a simple design that constantly evolves to add needed flexibility and remove unneeded complexity. Putting a minimal system into production quickly and growing it in whatever directions prove most valuable.Why is XP so controversial? Some sacred cows don't make the cut in XP: Don't force team members to specialize and become analysts, architects, programmers, testers, and integrators--every XP programmer participates in all of these critical activities every day. Don't conduct complete up-front analysis and design--an XP project starts with a quick analysis of the entire system, and XP programmers continue to make analysis and design decisions throughout development. Develop infrastructure and frameworks as you develop your application, not up-front--delivering business value is the heartbeat that drives XP projects. Don't write and maintain implementation documentation--communication in XP projects occurs face-to-face, or through efficient tests and carefully written code.You may love XP, or you may hate it, but Extreme Programming Explained will force you to take a fresh look at how you develop software. 0201616416B04062001\",\n",
       "  'authors': ['Kent Beck'],\n",
       "  'date': '1999',\n",
       "  'identifier': '1493688518',\n",
       "  'references': ['2153887189',\n",
       "   '2184024640',\n",
       "   '2168811232',\n",
       "   '285857824',\n",
       "   '2061659496',\n",
       "   '387168707',\n",
       "   '1607663648',\n",
       "   '2171383742',\n",
       "   '2896849024'],\n",
       "  'title': 'Extreme Programming Explained: Embrace Change'},\n",
       " {'abstract': 'High-resolution seismic methods are needed especially in oil and gas field development. They involve the use of backscattered energy rather than that of reflected signals, and make it interesting to look for representations of seismic traces in the time-frequency domain. One such representation was introduced by D. Gabor in 1946 into signal analysis; it is based on the consideration of a family of “elementary wavelets” that can be obtained from one “basic wavelet” by shifts in time and in frequency. We present here a different representation, in which frequency shifts are replaced by dilations. The resulting “voice transform” and “cycle-octave transform” are briefly described from the mathematical point of view and illustrated by numerical examples.',\n",
       "  'authors': ['P. Goupillaud 1',\n",
       "   ' 2',\n",
       "   ' A. Grossmann 1',\n",
       "   ' 2',\n",
       "   ' J. Morlet 1',\n",
       "   ' 2'],\n",
       "  'date': '1984',\n",
       "  'identifier': '1989491465',\n",
       "  'references': ['2096684483',\n",
       "   '2129451521',\n",
       "   '1203142710',\n",
       "   '1603491459',\n",
       "   '2154455356',\n",
       "   '1985720831',\n",
       "   '2001301726'],\n",
       "  'title': 'Cycle-octave and related transforms in seismic signal analysis'},\n",
       " {'abstract': 'Institutional openness is becoming increasingly popular in practice and academia: open innovation, open R&D and open business models. Our special issue builds on the concepts, underlying assumptions and implications discussed in two previous R&D Management special issues (2006, 2009). This overview indicates nine perspectives needed to develop an open innovation theory more fully. It also assesses some of the recent evidence that has come to light about open innovation, in theory and in practice.',\n",
       "  'authors': ['Oliver Gassmann 1', ' Ellen Enkel 2', ' Henry Chesbrough 3'],\n",
       "  'date': '2010',\n",
       "  'identifier': '2896849024',\n",
       "  'references': ['1561725974',\n",
       "   '1493688518',\n",
       "   '2108795964',\n",
       "   '1511351087',\n",
       "   '3022734214',\n",
       "   '2087712586',\n",
       "   '1542405687',\n",
       "   '2402811117',\n",
       "   '1550832676',\n",
       "   '2118243939'],\n",
       "  'title': 'The future of open innovation'},\n",
       " {'abstract': 'Algorithms for encoding and decoding finite strings over a finite alphabet are described. The coding operations are arithmetic involving rational numbers li as parameters such that Σi2-l i≤2-∈. This coding technique requires no blocking, and the per-symbol length of the encoded string approaches the associated entropy within ∈. The coding speed is comparable to that of conventional coding methods.',\n",
       "  'authors': ['J. J. Rissanen'],\n",
       "  'date': '1976',\n",
       "  'identifier': '2046419776',\n",
       "  'references': ['2128777897',\n",
       "   '2034323860',\n",
       "   '1992371956',\n",
       "   '158805393',\n",
       "   '2103206811'],\n",
       "  'title': 'Generalized kraft inequality and arithmetic coding'},\n",
       " {'abstract': 'Many neural network learning procedures compute gradients of the errors on the output layer of units after they have settled to their final values. We describe a procedure for finding E/wij, where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and wij are the weights of that network. Computing these quantities allows one to perform gradient descent in the weights to minimize E. Simulations in which networks are taught to move through limit cycles are shown. This type of recurrent network seems particularly suited for temporally continuous domains, such as signal processing, control, and speech.',\n",
       "  'authors': ['Barak A. Pearlmutter'],\n",
       "  'date': '1989',\n",
       "  'identifier': '2143503258',\n",
       "  'references': ['2016589492',\n",
       "   '2007431958',\n",
       "   '1959983357',\n",
       "   '1971129545',\n",
       "   '2028629011',\n",
       "   '1996647346'],\n",
       "  'title': 'Learning state space trajectories in recurrent neural networks'},\n",
       " {'abstract': \"The problem of automatically learning object models for recognition and pose estimation is addressed. In contrast to the traditional approach, the recognition problem is formulated as one of matching appearance rather than shape. The appearance of an object in a two-dimensional image depends on its shape, reflectance properties, pose in the scene, and the illumination conditions. While shape and reflectance are intrinsic properties and constant for a rigid object, pose and illumination vary from scene to scene. A compact representation of object appearance is proposed that is parametrized by pose and illumination. For each object of interest, a large set of images is obtained by automatically varying pose and illumination. This image set is compressed to obtain a low-dimensional subspace, called the eigenspace, in which the object is represented as a manifold. Given an unknown input image, the recognition system projects the image to eigenspace. The object is recognized based on the manifold it lies on. The exact position of the projection on the manifold determines the object's pose in the image. A variety of experiments are conducted using objects with complex appearance characteristics. The performance of the recognition and pose estimation algorithms is studied using over a thousand input images of sample objects. Sensitivity of recognition to the number of eigenspace dimensions and the number of learning samples is analyzed. For the objects used, appearance representation in eigenspaces with less than 20 dimensions produces accurate recognition results with an average pose estimation error of about 1.0 degree. A near real-time recognition system with 20 complex objects in the database has been developed. The paper is concluded with a discussion on various issues related to the proposed learning and recognition methodology.\",\n",
       "  'authors': ['Hiroshi Murase 1', ' Shree K. Nayar 2'],\n",
       "  'date': '1995',\n",
       "  'identifier': '2123977795',\n",
       "  'references': ['2170120409',\n",
       "   '2098693229',\n",
       "   '2143956139',\n",
       "   '2130259898',\n",
       "   '2135346934',\n",
       "   '3110825305',\n",
       "   '1996773532',\n",
       "   '2086479969',\n",
       "   '2026311529',\n",
       "   '2033554200'],\n",
       "  'title': 'Visual learning and recognition of 3-D objects from appearance'},\n",
       " {'abstract': 'In this paper, we present an audio chord recognition system based on a recurrent neural network. The audio features are obtained from a deep neural network optimized with a combination of chromagram targets and chord information, and aggregated over different time scales. Contrarily to other existing approaches, our system incorporates acoustic and musicological models under a single training objective. We devise an efficient algorithm to search for the global mode of the output distribution while taking long-term dependencies into account. The resulting method is competitive with state-of-the-art approaches on the MIREX dataset in the major/minor prediction task.',\n",
       "  'authors': ['Nicolas Boulanger-Lewandowski ',\n",
       "   ' Yoshua Bengio ',\n",
       "   ' Pascal Vincent'],\n",
       "  'date': '2013',\n",
       "  'identifier': '2395935897',\n",
       "  'references': ['2136922672',\n",
       "   '2072128103',\n",
       "   '2147768505',\n",
       "   '2154642048',\n",
       "   '2184045248',\n",
       "   '2107878631',\n",
       "   '2962968839',\n",
       "   '1828163288',\n",
       "   '2108563286',\n",
       "   '2162911105'],\n",
       "  'title': 'Audio Chord Recognition with Recurrent Neural Networks.'},\n",
       " {'abstract': 'Classical coreference systems encode various syntactic, discourse, and semantic phenomena explicitly, using heterogenous features computed from hand-crafted heuristics. In contrast, we present a state-of-the-art coreference system that captures such phenomena implicitly, with a small number of homogeneous feature templates examining shallow properties of mentions. Surprisingly, our features are actually more effective than the corresponding hand-engineered ones at modeling these key linguistic phenomena, allowing us to win “easy victories” without crafted heuristics. These features are successful on syntax and discourse; however, they do not model semantic compatibility well, nor do we see gains from experiments with shallow semantic features from the literature, suggesting that this approach to semantics is an “uphill battle.” Nonetheless, our final system 1 outperforms the Stanford system (Lee et al. (2011), the winner of the CoNLL 2011 shared task) by 3.5% absolute on the CoNLL metric and outperforms the IMS system (Bj¨ orkelund and Farkas (2012), the best publicly available English coreference system) by 1.9% absolute.',\n",
       "  'authors': ['Greg Durrett ', ' Dan Klein'],\n",
       "  'date': '2013',\n",
       "  'identifier': '2251035762',\n",
       "  'references': ['2146502635',\n",
       "   '2088911157',\n",
       "   '2164455818',\n",
       "   '2129657639',\n",
       "   '2155069789',\n",
       "   '2124741472',\n",
       "   '2101268022',\n",
       "   '2050273484',\n",
       "   '2098345921',\n",
       "   '2139354869'],\n",
       "  'title': 'Easy Victories and Uphill Battles in Coreference Resolution'},\n",
       " {'abstract': 'This paper reporis the results of our studies with an unsupervised learning paradigm which we have called “Competitive Learning.” We have examined competitive learning using both computer simulation and formal analysis and hove found that when it is applied to parallel networks of neuron-like elements, many potentially useful learning tasks can be accomplished. We were attracted to competitive learning because it seems to provide o way to discover the salient, general features which can be used to classify o set of patterns. We show how o very simply competitive mechanism con discover a set of feature detectors which capture important aspects of the set of stimulus input patterns. We 0150 show how these feature detectors con form the basis of o multilayer system that con serve to learn categorizations of stimulus sets which ore not linearly separable. We show how the use of correlated stimuli con serve IX o kind of “teaching” input to the system to allow the development of feature detectors which would not develop otherwise. Although we find the competitive learning mechanism o very interesting and powerful learning principle, we do not, of course, imagine thot it is the only learning principle. Competitive learning is cm essentially nonassociative stotisticol learning scheme. We certainly imagine that other kinds of learning mechanisms will be involved in the building of associations among patterns of activation in o more complete neural network. We offer this analysis of these competitive learning mechanisms to further our understanding of how simple adaptive networks can discover features importont in the description of the stimulus environment in which the system finds itself.',\n",
       "  'authors': ['David E. Rumelhart ', ' David Zipser'],\n",
       "  'date': '1988',\n",
       "  'identifier': '1490454746',\n",
       "  'references': ['2073257493',\n",
       "   '1514711945',\n",
       "   '2113653296',\n",
       "   '2010315761',\n",
       "   '2103170504'],\n",
       "  'title': 'Feature discovery by competitive learning'},\n",
       " {'abstract': '',\n",
       "  'authors': ['H FriedmanJerome ', ' BentleyJon Louis ', ' FinkelRaphael Ari'],\n",
       "  'date': '1977',\n",
       "  'identifier': '2998097659',\n",
       "  'references': ['2151103935',\n",
       "   '2124509324',\n",
       "   '2142827986',\n",
       "   '2147717514',\n",
       "   '1502916507',\n",
       "   '2086504823',\n",
       "   '2151854612',\n",
       "   '2427881153',\n",
       "   '2063532964',\n",
       "   '2135581534'],\n",
       "  'title': 'An Algorithm for Finding Best Matches in Logarithmic Expected Time'},\n",
       " {'abstract': 'Matplotlib is a 2D graphics package used for Python for application development, interactive scripting,and publication-quality image generation across user interfaces and operating systems',\n",
       "  'authors': ['J.D. Hunter'],\n",
       "  'date': '2007',\n",
       "  'identifier': '2011301426',\n",
       "  'references': ['3003257820',\n",
       "   '2244729984',\n",
       "   '1606347560',\n",
       "   '2015159529',\n",
       "   '2132022337',\n",
       "   '3039901154',\n",
       "   '2432815617',\n",
       "   '2110417468',\n",
       "   '2417863416',\n",
       "   '2800392236'],\n",
       "  'title': 'Matplotlib: A 2D Graphics Environment'},\n",
       " {'abstract': 'This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.',\n",
       "  'authors': ['David G. Lowe'],\n",
       "  'date': '2004',\n",
       "  'identifier': '2151103935',\n",
       "  'references': ['2033819227',\n",
       "   '2124386111',\n",
       "   '2154422044',\n",
       "   '2012778485',\n",
       "   '2124404372',\n",
       "   '1676552347',\n",
       "   '2124087378',\n",
       "   '2111308925',\n",
       "   '2165497495',\n",
       "   '1949116567'],\n",
       "  'title': 'Distinctive Image Features from Scale-Invariant Keypoints'},\n",
       " {'abstract': 'When natural scenes are viewed, a multitude of objects that are stable in their environments are brought in and out of view by eye movements. The posterior parietal cortex is crucial for the analysis of space, visual attention and movement. Neurons in one of its subdivisions, the lateral intraparietal area (LIP), have visual responses to stimuli appearing abruptly at particular retinal locations (their receptive fields). We have tested the responses of LIP neurons to stimuli that entered their receptive field by saccades. Neurons had little or no response to stimuli brought into their receptive field by saccades, unless the stimuli were behaviourally significant. We established behavioural significance in two ways: either by making a stable stimulus task-relevant, or by taking advantage of the attentional attraction of an abruptly appearing stimulus. Our results show that under ordinary circumstances the entire visual world is only weakly represented in LIP. The visual representation in LIP is sparse, with only the most salient or behaviourally relevant objects being strongly represented.',\n",
       "  'authors': ['Jacqueline P. Gottlieb 1',\n",
       "   ' Makoto Kusunoki 1',\n",
       "   ' Michael E. Goldberg 1',\n",
       "   ' 2'],\n",
       "  'date': '1998',\n",
       "  'identifier': '2160903697',\n",
       "  'references': ['2118615399',\n",
       "   '2093353037',\n",
       "   '1497599070',\n",
       "   '2016527110',\n",
       "   '2074854285',\n",
       "   '2049236887',\n",
       "   '2181613865',\n",
       "   '1569395231',\n",
       "   '2143325267',\n",
       "   '1621551752'],\n",
       "  'title': 'The representation of visual salience in monkey parietal cortex'},\n",
       " {'abstract': 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).',\n",
       "  'authors': ['Jacob Devlin ',\n",
       "   ' Ming-Wei Chang ',\n",
       "   ' Kenton Lee ',\n",
       "   ' Kristina N. Toutanova'],\n",
       "  'date': '2018',\n",
       "  'identifier': '2963341956',\n",
       "  'references': ['2963403868',\n",
       "   '2153579005',\n",
       "   '2250539671',\n",
       "   '2108598243',\n",
       "   '2962739339',\n",
       "   '2131744502',\n",
       "   '2251939518',\n",
       "   '2963748441',\n",
       "   '2117130368',\n",
       "   '2025768430'],\n",
       "  'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'},\n",
       " {'abstract': 'For senior/graduate-level courses in Discrete-Time Signal Processing. THE definitive, authoritative text on DSP -- ideal for those with an introductory-level knowledge of signals and systems. Written by prominent, DSP pioneers, it provides thorough treatment of the fundamental theorems and properties of discrete-time linear systems, filtering, sampling, and discrete-time Fourier Analysis. By focusing on the general and universal concepts in discrete-time signal processing, it remains vital and relevant to the new challenges arising in the field --without limiting itself to specific technologies with relatively short life spans.',\n",
       "  'authors': ['Alan V. Oppenheim ', ' Ronald W. Schafer'],\n",
       "  'date': '1989',\n",
       "  'identifier': '1766888123',\n",
       "  'references': ['1525535255',\n",
       "   '2133865602',\n",
       "   '2143267104',\n",
       "   '1991252559',\n",
       "   '2124890704',\n",
       "   '210359992',\n",
       "   '2165949425',\n",
       "   '2141116650',\n",
       "   '2154278880',\n",
       "   '2114004090'],\n",
       "  'title': 'Discrete-Time Signal Processing'},\n",
       " {'abstract': 'Abstract In the feature subset selection problem, a learning algorithm is faced with the problem of selecting a relevant subset of features upon which to focus its attention, while ignoring the rest. To achieve the best possible performance with a particular learning algorithm on a particular training set, a feature subset selection method should consider how the algorithm and the training set interact. We explore the relation between optimal feature subset selection and relevance. Our wrapper method searches for an optimal feature subset tailored to a particular algorithm and a domain. We study the strengths and weaknesses of the wrapper approach and show a series of improved designs. We compare the wrapper approach to induction without feature subset selection and to Relief, a filter approach to feature subset selection. Significant improvement in accuracy is achieved for some datasets for the two families of induction algorithms used: decision trees and Naive-Bayes.',\n",
       "  'authors': ['Ron Kohavi 1', ' George H. John 2'],\n",
       "  'date': '1997',\n",
       "  'identifier': '2017337590',\n",
       "  'references': ['1639032689',\n",
       "   '1988790447',\n",
       "   '2912934387',\n",
       "   '2122410182',\n",
       "   '2084812512',\n",
       "   '2125055259',\n",
       "   '2340020088',\n",
       "   '3085162807',\n",
       "   '2149706766',\n",
       "   '1680392829'],\n",
       "  'title': 'Wrappers for feature subset selection'},\n",
       " {'abstract': 'n-gram (n = 1 to 5) statistics and other properties of the English language were derived for applications in natural language understanding and text processing. They were computed from a well-known corpus composed of 1 million word samples. Similar properties were also derived from the most frequent 1000 words of three other corpuses. The positional distributions of n-grams obtained in the present study are discussed. Statistical studies on word length and trends of n-gram frequencies versus vocabulary are presented. In addition to a survey of n-gram statistics found in the literature, a collection of n-gram statistics obtained by other researchers is reviewed and compared.',\n",
       "  'authors': ['Ching Y. Suen'],\n",
       "  'date': '1979',\n",
       "  'identifier': '2058929792',\n",
       "  'references': ['2142384583',\n",
       "   '1970026646',\n",
       "   '1647671624',\n",
       "   '2008819433',\n",
       "   '2157477135',\n",
       "   '2066792529',\n",
       "   '2042008249',\n",
       "   '2079145130',\n",
       "   '2027085566',\n",
       "   '2071663716'],\n",
       "  'title': 'n-Gram Statistics for Natural Language Understanding and Text Processing'},\n",
       " {'abstract': 'We present an example-based learning approach for locating vertical frontal views of human faces in complex scenes. The technique models the distribution of human face patterns by means of a few view-based \"face\" and \"nonface\" model clusters. At each image location, a difference feature vector is computed between the local image pattern and the distribution-based model. A trained classifier determines, based on the difference feature vector measurements, whether or not a human face exists at the current image location. We show empirically that the distance metric we adopt for computing difference feature vectors, and the \"nonface\" clusters we include in our distribution-based model, are both critical for the success of our system.',\n",
       "  'authors': ['K.-K. Sung 1', ' T. Poggio 2'],\n",
       "  'date': '1998',\n",
       "  'identifier': '2159686933',\n",
       "  'references': [],\n",
       "  'title': 'Example-based learning for view-based human face detection'},\n",
       " {'abstract': 'Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant.',\n",
       "  'authors': ['Mitchell P. Marcus 1',\n",
       "   ' Mary Ann Marcinkiewicz 1',\n",
       "   ' Beatrice Santorini 2'],\n",
       "  'date': '1993',\n",
       "  'identifier': '1632114991',\n",
       "  'references': ['2099247782',\n",
       "   '1483126227',\n",
       "   '2439178139',\n",
       "   '2334801970',\n",
       "   '900993354',\n",
       "   '2110190189',\n",
       "   '2012837062',\n",
       "   '2121407024',\n",
       "   '2076526090',\n",
       "   '2034693287'],\n",
       "  'title': 'Building a large annotated corpus of English: the penn treebank'},\n",
       " {'abstract': 'Multiple antennas can be used for increasing the amount of diversity or the number of degrees of freedom in wireless communication systems. We propose the point of view that both types of gains can be simultaneously obtained for a given multiple-antenna channel, but there is a fundamental tradeoff between how much of each any coding scheme can get. For the richly scattered Rayleigh-fading channel, we give a simple characterization of the optimal tradeoff curve and use it to evaluate the performance of existing multiple antenna schemes.',\n",
       "  'authors': ['Lizhong Zheng ', ' D.N.C. Tse'],\n",
       "  'date': '2003',\n",
       "  'identifier': '2129766733',\n",
       "  'references': ['2107080958',\n",
       "   '2130509920',\n",
       "   '2118040894',\n",
       "   '2110659753',\n",
       "   '2798333393',\n",
       "   '2133475491',\n",
       "   '2106929598',\n",
       "   '2117359246',\n",
       "   '2168571551',\n",
       "   '2096765600'],\n",
       "  'title': 'Diversity and multiplexing: a fundamental tradeoff in multiple-antenna channels'},\n",
       " {'abstract': 'Probablistic models are becoming increasingly important in analyzing the huge amount of data being produced by large-scale DNA-sequencing efforts such as the Human Genome Project. For example, hidden Markov models are used for analyzing biological sequences, linguistic-grammar-based probabilistic models for identifying RNA secondary structure, and probabilistic evolutionary models for inferring phylogenies of sequences from different organisms. This book gives a unified, up-to-date and self-contained account, with a Bayesian slant, of such methods, and more generally to probabilistic methods of sequence analysis. Written by an interdisciplinary team of authors, it is accessible to molecular biologists, computer scientists, and mathematicians with no formal knowledge of the other fields, and at the same time presents the state of the art in this new and important field.',\n",
       "  'authors': ['Richard Durbin 1',\n",
       "   ' Sean R. Eddy 2',\n",
       "   ' Anders Krogh 3',\n",
       "   ' Graeme J. Mitchison'],\n",
       "  'date': '1998',\n",
       "  'identifier': '2009570821',\n",
       "  'references': ['2140190241',\n",
       "   '2147880316',\n",
       "   '2168133698',\n",
       "   '2158266063',\n",
       "   '2153233077',\n",
       "   '2045843097',\n",
       "   '2138122982',\n",
       "   '2145191876',\n",
       "   '2110575115',\n",
       "   '2162315106'],\n",
       "  'title': 'Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Peter J. Denning'],\n",
       "  'date': '1982',\n",
       "  'identifier': '2053772216',\n",
       "  'references': ['1966553486',\n",
       "   '1982428762',\n",
       "   '2118079529',\n",
       "   '2165990684',\n",
       "   '3111893291',\n",
       "   '2115186087',\n",
       "   '2545577367',\n",
       "   '2147261473',\n",
       "   '2025172185',\n",
       "   '1977281680'],\n",
       "  'title': \"ACM president's letter: electronic junk\"},\n",
       " {'abstract': 'We propose a network architecture which uses a single internal layer of locally-tuned processing units to learn both classification tasks and real-valued function approximations (Moody and Darken 1988). We consider training such networks in a completely supervised manner, but abandon this approach in favor of a more computationally efficient hybrid learning method which combines self-organized and supervised learning. Our networks learn faster than backpropagation for two reasons: the local representations ensure that only a few units respond to any given input, thus reducing computational overhead, and the hybrid learning rules are linear rather than nonlinear, thus leading to faster convergence. Unlike many existing methods for data analysis, our network architecture and learning rules are truly adaptive and are thus appropriate for real-time use.',\n",
       "  'authors': ['John Moody ', ' Christian J. Darken'],\n",
       "  'date': '1989',\n",
       "  'identifier': '2171277043',\n",
       "  'references': ['2042264548',\n",
       "   '2143956139',\n",
       "   '2150593711',\n",
       "   '23758216',\n",
       "   '2084544490',\n",
       "   '1524100745',\n",
       "   '2034099719',\n",
       "   '2127218421',\n",
       "   '2072773743',\n",
       "   '2007700211'],\n",
       "  'title': 'Fast learning in networks of locally-tuned processing units'},\n",
       " {'abstract': 'Suppose x is an unknown vector in Ropfm (a digital image or signal); we plan to measure n general linear functionals of x and then reconstruct. If x is known to be compressible by transform coding with a known transform, and we reconstruct via the nonlinear procedure defined here, the number of measurements n can be dramatically smaller than the size m. Thus, certain natural classes of images with m pixels need only n=O(m1/4log5/2(m)) nonadaptive nonpixel samples for faithful recovery, as opposed to the usual m pixel samples. More specifically, suppose x has a sparse representation in some orthonormal basis (e.g., wavelet, Fourier) or tight frame (e.g., curvelet, Gabor)-so the coefficients belong to an lscrp ball for 0<ples1. The N most important coefficients in that expansion allow reconstruction with lscr2 error O(N1/2-1p/). It is possible to design n=O(Nlog(m)) nonadaptive measurements allowing reconstruction with accuracy comparable to that attainable with direct knowledge of the N most important coefficients. Moreover, a good approximation to those N important coefficients is extracted from the n measurements by solving a linear program-Basis Pursuit in signal processing. The nonadaptive measurements have the character of \"random\" linear combinations of basis/frame elements. Our results use the notions of optimal recovery, of n-widths, and information-based complexity. We estimate the Gel\\'fand n-widths of lscrp balls in high-dimensional Euclidean space in the case 0<ples1, and give a criterion identifying near- optimal subspaces for Gel\\'fand n-widths. We show that \"most\" subspaces are near-optimal, and show that convex optimization (Basis Pursuit) is a near-optimal way to extract information derived from these near-optimal subspaces',\n",
       "  'authors': ['D.L. Donoho'],\n",
       "  'date': '2004',\n",
       "  'identifier': '2296616510',\n",
       "  'references': ['2296319761',\n",
       "   '2145096794',\n",
       "   '2115755118',\n",
       "   '2129638195',\n",
       "   '2129131372',\n",
       "   '2135046866',\n",
       "   '2062024414',\n",
       "   '2078204800',\n",
       "   '2798909945',\n",
       "   '2138019504'],\n",
       "  'title': 'Compressed sensing'},\n",
       " {'abstract': 'Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered. >',\n",
       "  'authors': ['Y. Bengio 1', ' P. Simard 2', ' P. Frasconi 3'],\n",
       "  'date': '1994',\n",
       "  'identifier': '2107878631',\n",
       "  'references': ['2581275558',\n",
       "   '2154642048',\n",
       "   '2016589492',\n",
       "   '2128499899',\n",
       "   '19621276',\n",
       "   '2088978850',\n",
       "   '2148099973',\n",
       "   '1996741810',\n",
       "   '2125329357',\n",
       "   '1527772862'],\n",
       "  'title': 'Learning long-term dependencies with gradient descent is difficult'},\n",
       " {'abstract': 'Diagnostic tasks require determining the difierences between a model of an artifact and the artifact itself. The difierences between the manifested behavior of the artifact and the predicted behavior of the model guide the search for the difierences between the artifact and its model. The diagnostic procedure presented in this paper is model-based, inferring the behavior of the composite device from knowledge of the structure and function of the individual components comprising the device. The system (GDE | General Diagnostic Engine) has been implemented and tested on many examples in the domain of troubleshooting digital circuits. This research makes several novel contributions: First, the system diagnoses failures due to multiple faults. Second, failure candidates are represented and manipulated in terms of minimal sets of violated assumptions, resulting in an e‐cient diagnostic procedure. Third, the diagnostic procedure is incremental, exploiting the iterative nature of diagnosis. Fourth, a clear separation is drawn between diagnosis and behavior prediction, resulting in a domain (and inference procedure) independent diagnostic procedure. Fifth, GDE combines modelbased prediction with sequential diagnosis to propose measurements to localize the faults. The normally required conditional probabilities are computed from the structure of the device and models of its components. This capability results from a novel way of incorporating probabilities and information theory into the context mechanism provided by AssumptionBased Truth Maintenance.',\n",
       "  'authors': ['J de Kleer 1', ' B C Williams 2'],\n",
       "  'date': '1987',\n",
       "  'identifier': '2144386448',\n",
       "  'references': ['2108309071',\n",
       "   '2147096558',\n",
       "   '2159047538',\n",
       "   '2478175895',\n",
       "   '2140627345',\n",
       "   '1983382292',\n",
       "   '1995875735',\n",
       "   '1998363560',\n",
       "   '2033755422',\n",
       "   '2531504812'],\n",
       "  'title': 'Diagnosing multiple faults'},\n",
       " {'abstract': 'The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions. These waveforms are chosen in order to best match the signal structures. Matching pursuits are general procedures to compute adaptive signal representations. With a dictionary of Gabor functions a matching pursuit defines an adaptive time-frequency transform. They derive a signal energy distribution in the time-frequency plane, which does not include interference terms, unlike Wigner and Cohen class distributions. A matching pursuit isolates the signal structures that are coherent with respect to a given dictionary. An application to pattern extraction from noisy signals is described. They compare a matching pursuit decomposition with a signal expansion over an optimized wavepacket orthonormal basis, selected with the algorithm of Coifman and Wickerhauser see (IEEE Trans. Informat. Theory, vol. 38, Mar. 1992). >',\n",
       "  'authors': ['S.G. Mallat ', ' Zhifeng Zhang'],\n",
       "  'date': '1993',\n",
       "  'identifier': '2151693816',\n",
       "  'references': ['2062024414',\n",
       "   '2798909945',\n",
       "   '1996021349',\n",
       "   '2156447271',\n",
       "   '1970352604',\n",
       "   '2165878107',\n",
       "   '2913399920',\n",
       "   '2091886411',\n",
       "   '2080563952',\n",
       "   '2033519565'],\n",
       "  'title': 'Matching pursuits with time-frequency dictionaries'},\n",
       " {'abstract': 'The mathematical characterization of singularities with Lipschitz exponents is reviewed. Theorems that estimate local Lipschitz exponents of functions from the evolution across scales of their wavelet transform are reviewed. It is then proven that the local maxima of the wavelet transform modulus detect the locations of irregular structures and provide numerical procedures to compute their Lipschitz exponents. The wavelet transform of singularities with fast oscillations has a particular behavior that is studied separately. The local frequency of such oscillations is measured from the wavelet transform modulus maxima. It has been shown numerically that one- and two-dimensional signals can be reconstructed, with a good approximation, from the local maxima of their wavelet transform modulus. As an application, an algorithm is developed that removes white noises from signals by analyzing the evolution of the wavelet transform maxima across scales. In two dimensions, the wavelet transform maxima indicate the location of edges in images. >',\n",
       "  'authors': ['S. Mallat ', ' W.L. Hwang'],\n",
       "  'date': '1992',\n",
       "  'identifier': '2152328854',\n",
       "  'references': ['2145023731',\n",
       "   '2078206416',\n",
       "   '1996021349',\n",
       "   '2004217976',\n",
       "   '2109863423',\n",
       "   '2003370853',\n",
       "   '2096684483',\n",
       "   '1667165204',\n",
       "   '2133155955',\n",
       "   '1968245656'],\n",
       "  'title': 'Singularity detection and processing with wavelets'},\n",
       " {'abstract': 'To look at or reach for what we see, spatial information from the visual system must be transformed into a motor plan. The posterior parietal cortex (PPC) is well placed to perform this function, because it lies between visual areas, which encode spatial information, and motor cortical areas. The PPC contains several subdivisions, which are generally conceived as high-order sensory areas. Neurons in area 7a and the lateral intraparietal area fire before and during visually guided saccades. Other neurons in areas 7a and 5 are active before and during visually guided arm movements. These areas are also active during memory tasks in which the animal remembers the location of a target for hundreds of milliseconds before making an eye or arm movement. Such activity could reflect either visual attention or the intention to make movements. This question is difficult to resolve, because even if the animal maintains fixation while directing attention to a peripheral location, the observed neuronal activity could reflect movements that are planned but not executed. To address this, we recorded from the PPC while monkeys planned either reaches or saccades to a single remembered location. We now report that, for most neurons, activity before the movement depended on the type of movement being planned. We conclude that PPC contains signals related to what the animal intends to do.',\n",
       "  'authors': ['L. H. Snyder ', ' A. P. Batista ', ' R. A. Andersen'],\n",
       "  'date': '1997',\n",
       "  'identifier': '2016527110',\n",
       "  'references': ['2082627290',\n",
       "   '2074854285',\n",
       "   '1980871601',\n",
       "   '576881103',\n",
       "   '1569395231',\n",
       "   '2177960617',\n",
       "   '2188405233',\n",
       "   '2138956231',\n",
       "   '2022876055',\n",
       "   '1576240289'],\n",
       "  'title': 'Coding of intention in the posterior parietal cortex'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Wen-Hsiang Tsai'],\n",
       "  'date': '1995',\n",
       "  'identifier': '49742075',\n",
       "  'references': ['2133003941',\n",
       "   '2138957397',\n",
       "   '2524625303',\n",
       "   '2058668884',\n",
       "   '2010398978',\n",
       "   '2064782300',\n",
       "   '139350513',\n",
       "   '51975515',\n",
       "   '2474134051',\n",
       "   '2056661748'],\n",
       "  'title': 'Moment-preserving thresholding: a new approach'},\n",
       " {'abstract': 'We discuss orthogonal transformation as a method to compress frequency bandwidth for TV signals. We indicate that the slant orthogonal transformation derived from Hadamard matrix is most advantageous for compression of TV signal bandwidth. Results of simulation show that unweighted Sp-p/Nq rms of approximately 32 dB was obtained with 2.5 bits per sample on picture showing human face.',\n",
       "  'authors': ['Hajime Enomoto 1', ' Kosho Shibata 2'],\n",
       "  'date': '1971',\n",
       "  'identifier': '2046682096',\n",
       "  'references': ['2134809980'],\n",
       "  'title': 'Orthogonal Transform Coding System for Television Signals'},\n",
       " {'abstract': 'In a database to which data is continually added, users may wish to issue a permanent query and be notified whenever data matches the query. If such continuous queries examine only single records, this can be implemented by examining each record as it arrives. This is very efficient because only the incoming record needs to be scanned. This simple approach does not work for queries involving joins or time. The Tapestry system allows users to issue such queries over a database of mail and bulletin board messages. The user issues a static query, such as “show me all messages that have been replied to by Jones,” as though the database were fixed and unchanging. Tapestry converts the query into an incremental query that efficiently finds new matches to the original query as new messages are added to the database. This paper describes the techniques used in Tapestry, which do not depend on triggers and thus be implemented on any commercial database that supports SQL. Although Tapestry is designed for filtering mail and news messages, its techniques are applicable to any append-only database.',\n",
       "  'authors': ['Douglas Terry ',\n",
       "   ' David Goldberg ',\n",
       "   ' David Nichols ',\n",
       "   ' Brian Oki'],\n",
       "  'date': '1992',\n",
       "  'identifier': '2002110561',\n",
       "  'references': ['2037717074',\n",
       "   '2120052337',\n",
       "   '2019580127',\n",
       "   '138816814',\n",
       "   '113037826',\n",
       "   '2006299591',\n",
       "   '2066636971',\n",
       "   '2062851028',\n",
       "   '3003496299',\n",
       "   '1524011117'],\n",
       "  'title': 'Continuous queries over append-only databases'},\n",
       " {'abstract': 'We describe an information seeking assistant for the world wide web. This agent, called WebWatcher, interactively helps users locate desired information by employing learned knowledge about which hyperlinks are likely to lead to the target information. Our primary focus to date has been on two issues: (1) organizing WebWatcher to provide interactive advice to Mosaic users while logging their successful and unsuccessful searches as training data, and (2) incorporating machine learning methods to automatically acquire knowledge for selecting an appropriate hyperlink given the current web page viewed by the user and the user’s information goal. We describe the initial design of WebWatcher, and the results of our preliminary learning experiments.',\n",
       "  'authors': ['Robert Armstrong ',\n",
       "   ' Dayne Freitag ',\n",
       "   ' Thorsten Joachims ',\n",
       "   ' Tom Mitchell'],\n",
       "  'date': '1995',\n",
       "  'identifier': '40914139',\n",
       "  'references': ['2129113961', '2118383892', '1608168306'],\n",
       "  'title': 'WebWatcher : A Learning Apprentice for the World Wide Web'},\n",
       " {'abstract': 'One of the central problems in machine learning and pattern recognition is to develop appropriate representations for complex data. We consider the problem of constructing a representation for data lying on a low-dimensional manifold embedded in a high-dimensional space. Drawing on the correspondence between the graph Laplacian, the Laplace Beltrami operator on the manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for representing the high-dimensional data. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality-preserving properties and a natural connection to clustering. Some potential applications and illustrative examples are discussed.',\n",
       "  'authors': ['Mikhail Belkin ', ' Partha Niyogi'],\n",
       "  'date': '2003',\n",
       "  'identifier': '2097308346',\n",
       "  'references': ['2124776405',\n",
       "   '2053186076',\n",
       "   '2121947440',\n",
       "   '2001141328',\n",
       "   '2165874743',\n",
       "   '2140095548',\n",
       "   '1578099820',\n",
       "   '2156718197',\n",
       "   '2285257517',\n",
       "   '2077776048'],\n",
       "  'title': 'Laplacian Eigenmaps for dimensionality reduction and data representation'},\n",
       " {'abstract': 'A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more.',\n",
       "  'authors': ['Vladimir Naumovich Vapnik'],\n",
       "  'date': '1998',\n",
       "  'identifier': '2148603752',\n",
       "  'references': ['2156909104'],\n",
       "  'title': 'Statistical learning theory'},\n",
       " {'abstract': 'Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.',\n",
       "  'authors': ['Rupesh Kumar Srivastava ',\n",
       "   ' Klaus Greff ',\n",
       "   ' Jürgen Schmidhuber'],\n",
       "  'date': '2015',\n",
       "  'identifier': '1026270304',\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '2155893237',\n",
       "   '1677182931',\n",
       "   '2136922672',\n",
       "   '2064675550',\n",
       "   '1533861849',\n",
       "   '2294059674',\n",
       "   '104184427'],\n",
       "  'title': 'Training very deep networks'},\n",
       " {'abstract': \"We present a tree-structured architecture for supervised learning. The statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models (GLIM's). Learning is treated as a maximum likelihood problem; in particular, we present an Expectation-Maximization (EM) algorithm for adjusting the parameters of the architecture. We also develop an on-line learning algorithm in which the parameters are updated incrementally. Comparative simulation results are presented in the robot dynamics domain.\",\n",
       "  'authors': ['Michael I. Jordan 1', ' Robert A. Jacobs 2'],\n",
       "  'date': '1994',\n",
       "  'identifier': '2025653905',\n",
       "  'references': ['1492221128',\n",
       "   '2137983211',\n",
       "   '2149706766',\n",
       "   '2049633694',\n",
       "   '2044758663',\n",
       "   '1528905581',\n",
       "   '2102201073',\n",
       "   '3017143921',\n",
       "   '2150884987',\n",
       "   '2076118331'],\n",
       "  'title': 'Hierarchical mixtures of experts and the EM algorithm'},\n",
       " {'abstract': 'Inspired by empirical studies of networked systems such as the Internet, social networks, and biological networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. Here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks.',\n",
       "  'authors': ['Mark E. J. Newman'],\n",
       "  'date': '2003',\n",
       "  'identifier': '2148606196',\n",
       "  'references': ['2112090702',\n",
       "   '2008620264',\n",
       "   '2124637492',\n",
       "   '3013264884',\n",
       "   '2138621811',\n",
       "   '1971421925',\n",
       "   '1854214752',\n",
       "   '1992419399',\n",
       "   '2097571405',\n",
       "   '2164727176'],\n",
       "  'title': 'The Structure and Function of Complex Networks'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Michael Collins'],\n",
       "  'date': '2000',\n",
       "  'identifier': '3021452258',\n",
       "  'references': [],\n",
       "  'title': 'Discriminative Reranking for Natural Language Parsing'},\n",
       " {'abstract': 'This paper investigates the maximal channel coding rate achievable at a given blocklength and error probability. For general classes of channels new achievability and converse bounds are given, which are tighter than existing bounds for wide ranges of parameters of interest, and lead to tight approximations of the maximal achievable rate for blocklengths n as short as 100. It is also shown analytically that the maximal rate achievable with error probability ? isclosely approximated by C - ?(V/n) Q-1(?) where C is the capacity, V is a characteristic of the channel referred to as channel dispersion , and Q is the complementary Gaussian cumulative distribution function.',\n",
       "  'authors': ['Yury Polyanskiy ', ' H Vincent Poor ', ' Sergio Verdu'],\n",
       "  'date': '2010',\n",
       "  'identifier': '2106864314',\n",
       "  'references': ['2095882513',\n",
       "   '2801179766',\n",
       "   '2142901448',\n",
       "   '1590772317',\n",
       "   '2020347709',\n",
       "   '2090841176',\n",
       "   '2109035510',\n",
       "   '2753908209',\n",
       "   '2104340231',\n",
       "   '2751862591'],\n",
       "  'title': 'Channel Coding Rate in the Finite Blocklength Regime'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Benjamin Recht ', ' Maryam Fazel ', ' Pablo A. Parrilo'],\n",
       "  'date': '2010',\n",
       "  'identifier': '2202343345',\n",
       "  'references': ['2011359124',\n",
       "   '131378802',\n",
       "   '2156739854',\n",
       "   '3104684837',\n",
       "   '2041101642',\n",
       "   '2146616964',\n",
       "   '2906621894',\n",
       "   '648260396',\n",
       "   '1982059935',\n",
       "   '2100875869'],\n",
       "  'title': 'Guaranteed Minimum-Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization'},\n",
       " {'abstract': \"Data Analysis Using Regression and Multilevel/Hierarchical Models is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors' own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout.\",\n",
       "  'authors': ['Andrew Gelman ', ' Yu-Sung Su'],\n",
       "  'date': '2006',\n",
       "  'identifier': '1981457167',\n",
       "  'references': ['1951724000',\n",
       "   '2577537660',\n",
       "   '2019655958',\n",
       "   '2141845152',\n",
       "   '2166851633',\n",
       "   '2155988679',\n",
       "   '1998025025',\n",
       "   '2741346289',\n",
       "   '3032971139'],\n",
       "  'title': 'Data Analysis Using Regression and Multilevel/Hierarchical Models'},\n",
       " {'abstract': \"We investigate the use of information from all second order derivatives of the error function to perform network pruning (i.e., removing unimportant weights from a trained network) in order to improve generalization, simplify networks, reduce hardware or storage requirements, increase the speed of further training, and in some cases enable rule extraction. Our method, Optimal Brain Surgeon (OBS), is Significantly better than magnitude-based methods and Optimal Brain Damage [Le Cun, Denker and Solla, 1990], which often remove the wrong weights. OBS permits the pruning of more weights than other methods (for the same error on the training set), and thus yields better generalization on test data. Crucial to OBS is a recursion relation for calculating the inverse Hessian matrix H-1 from training data and structural information of the net. OBS permits a 90%, a 76%, and a 62% reduction in weights over backpropagation with weight decay on three benchmark MONK's problems [Thrun et al., 1991]. Of OBS, Optimal Brain Damage, and magnitude-based methods, only OBS deletes the correct weights from a trained XOR network in every case. Finally, whereas Sejnowski and Rosenberg [1987] used 18,000 weights in their NETtalk network, we used OBS to prune a network to just 1560 weights, yielding better generalization.\",\n",
       "  'authors': ['Babak Hassibi 1', ' David G. Stork 2'],\n",
       "  'date': '1992',\n",
       "  'identifier': '2125389748',\n",
       "  'references': ['2154642048',\n",
       "   '2133671888',\n",
       "   '3036751298',\n",
       "   '2114766824',\n",
       "   '2054658115',\n",
       "   '2156150815',\n",
       "   '2169273134',\n",
       "   '2128163155'],\n",
       "  'title': 'Second order derivatives for network pruning: Optimal Brain Surgeon'},\n",
       " {'abstract': 'Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.',\n",
       "  'authors': ['Rui Xu ', ' D. Wunsch'],\n",
       "  'date': '2005',\n",
       "  'identifier': '2153233077',\n",
       "  'references': ['2158714788',\n",
       "   '2912565176',\n",
       "   '2148603752',\n",
       "   '2124776405',\n",
       "   '2055043387',\n",
       "   '1497256448',\n",
       "   '1554663460',\n",
       "   '2168909179',\n",
       "   '2139212933',\n",
       "   '2150926065'],\n",
       "  'title': 'Survey of clustering algorithms'},\n",
       " {'abstract': \"Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.\",\n",
       "  'authors': ['Ilya Sutskever ', ' Oriol Vinyals ', ' Quoc V. Le'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2130942839',\n",
       "  'references': ['2618530766',\n",
       "   '2964308564',\n",
       "   '2157331557',\n",
       "   '2310919327',\n",
       "   '2064675550',\n",
       "   '2101105183',\n",
       "   '179875071',\n",
       "   '2147768505',\n",
       "   '2132339004',\n",
       "   '1753482797'],\n",
       "  'title': 'Sequence to Sequence Learning with Neural Networks'},\n",
       " {'abstract': 'We exhibit an orthogonal set of product states of two three-state particles that nevertheless cannot be reliably distinguished by a pair of separated observers ignorant of which of the states has been presented to them, even if the observers are allowed any sequence of local operations and classical communication between the separate observers. It is proved that there is a finite gap between the mutual information obtainable by a joint measurement on these states and a measurement in which only local actions are permitted. This result implies the existence of separable superoperators that cannot be implemented locally. A set of states are found involving three two-state particles that also appear to be nonmeasurable locally. These and other multipartite states are classified according to the entropy and entanglement costs of preparing and measuring them by local operations.',\n",
       "  'authors': ['Charles H. Bennett 1',\n",
       "   ' David P. DiVincenzo 1',\n",
       "   ' Christopher A. Fuchs 2',\n",
       "   ' Tal Mor 3',\n",
       "   ' Eric Rains 4',\n",
       "   ' Peter W. Shor 4',\n",
       "   ' John A. Smolin 1',\n",
       "   ' William K. Wootters 5'],\n",
       "  'date': '1999',\n",
       "  'identifier': '1974293884',\n",
       "  'references': ['1978553093',\n",
       "   '2137147061',\n",
       "   '2068967633',\n",
       "   '2060887031',\n",
       "   '2002372750',\n",
       "   '2051051926',\n",
       "   '2987687226',\n",
       "   '2154564336',\n",
       "   '2003989474',\n",
       "   '2001619599'],\n",
       "  'title': 'Quantum nonlocality without entanglement'},\n",
       " {'abstract': 'Describes a sequential universal data compression procedure for binary tree sources that performs the \"double mixture.\" Using a context tree, this method weights in an efficient recursive way the coding distributions corresponding to all bounded memory tree sources, and achieves a desirable coding distribution for tree sources with an unknown model and unknown parameters. Computational and storage complexity of the proposed procedure are both linear in the source sequence length. The authors derive a natural upper bound on the cumulative redundancy of the method for individual sequences. The three terms in this bound can be identified as coding, parameter, and model redundancy, The bound holds for all source sequence lengths, not only for asymptotically large lengths. The analysis that leads to this bound is based on standard techniques and turns out to be extremely simple. The upper bound on the redundancy shows that the proposed context-tree weighting procedure is optimal in the sense that it achieves the Rissanen (1984) lower bound. >',\n",
       "  'authors': ['F.M.J. Willems ', ' Y.M. Shtarkov ', ' T.J. Tjalkens'],\n",
       "  'date': '1995',\n",
       "  'identifier': '2163294786',\n",
       "  'references': ['1995875735',\n",
       "   '2132119275',\n",
       "   '2095963120',\n",
       "   '2119047110',\n",
       "   '2082967074',\n",
       "   '1999120268',\n",
       "   '2046419776',\n",
       "   '2148844225',\n",
       "   '2116190532',\n",
       "   '2119984390'],\n",
       "  'title': 'The context-tree weighting method: basic properties'},\n",
       " {'abstract': 'We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.',\n",
       "  'authors': ['Alex Graves ', ' Greg Wayne ', ' Ivo Danihelka'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2167839676',\n",
       "  'references': ['2964308564',\n",
       "   '2130942839',\n",
       "   '2064675550',\n",
       "   '2143612262',\n",
       "   '1503398984',\n",
       "   '1810943226',\n",
       "   '2102113734',\n",
       "   '1889268436',\n",
       "   '2293063825',\n",
       "   '2029949252'],\n",
       "  'title': 'Neural Turing Machines'},\n",
       " {'abstract': 'An edited volume containing data structures and algorithms for information retrieved including a disk with examples written in C. For programmers and students interested in parsing text, automated indexing, its the first collection in book form of the basic data structures and algorithms that are critical to the storage and retrieval of documents.',\n",
       "  'authors': ['William B. Frakes 1', ' Ricardo Baeza-Yates 2'],\n",
       "  'date': '1992',\n",
       "  'identifier': '1997841190',\n",
       "  'references': ['2098162425',\n",
       "   '1956559956',\n",
       "   '1491178396',\n",
       "   '2987803397',\n",
       "   '1655990431',\n",
       "   '2752853835',\n",
       "   '2002089154',\n",
       "   '2000672666',\n",
       "   '2029195137',\n",
       "   '2158365276'],\n",
       "  'title': 'Information Retrieval: Data Structures and Algorithms'},\n",
       " {'abstract': 'Notations and definitions necessary to identify the concepts and relationships that are important in modelling information retrieval objects and processes in the context of vector spaces are presented. Earlier work on the use of vector model is evaluated in terms of the concepts introduced and certain problems and inconsistencies are identified. More importantly, this investigation should lead to a clear understanding of the issues and problems in using the vector space model in information retrieval. © 1986 John Wiley & Sons, Inc.',\n",
       "  'authors': ['Vijay V. Raghavan ', ' S. K. M. Wong'],\n",
       "  'date': '1986',\n",
       "  'identifier': '1965061793',\n",
       "  'references': ['1956559956',\n",
       "   '1974406477',\n",
       "   '2024472735',\n",
       "   '2049577316',\n",
       "   '2004913349',\n",
       "   '2052842312',\n",
       "   '2074876593',\n",
       "   '1985414707'],\n",
       "  'title': 'A critical analysis of vector space model for information retrieval'},\n",
       " {'abstract': 'Recent work has exhibited the surprising cross-lingual abilities of multilingual BERT (M-BERT) -- surprising since it is trained without any cross-lingual objective and with no aligned data. In this work, we provide a comprehensive study of the contribution of different components in M-BERT to its cross-lingual ability. We study the impact of linguistic properties of the languages, the architecture of the model, and of the learning objectives. The experimental study is done in the context of three typologically different languages -- Spanish, Hindi, and Russian -- and using two conceptually different NLP tasks, textual entailment and named entity recognition. Among our key conclusions is the fact that lexical overlap between languages plays a negligible role in the cross-lingual success, while the depth of the network is an important part of it',\n",
       "  'authors': ['Karthikeyan K 1',\n",
       "   ' Zihan Wang 2',\n",
       "   ' Stephen Mayhew 3',\n",
       "   ' Dan Roth 3'],\n",
       "  'date': '2020',\n",
       "  'identifier': '2995015695',\n",
       "  'references': ['2963341956',\n",
       "   '2965373594',\n",
       "   '2972324944',\n",
       "   '3011411500',\n",
       "   '2914120296',\n",
       "   '2891555348',\n",
       "   '2952638691',\n",
       "   '2962795068',\n",
       "   '2154474435',\n",
       "   '2573062194'],\n",
       "  'title': 'Cross-Lingual Ability of Multilingual BERT: An Empirical Study'},\n",
       " {'abstract': 'In this paper, second-order coding rate of channel coding is discussed for general sequence of channels. The optimum second-order transmission rate with a constant error constraint epsiv is obtained by using the information spectrum method. We apply this result to the discrete memoryless case, the discrete memoryless case with a cost constraint, the additive Markovian case, and the Gaussian channel case with an energy constraint. We also clarify that the Gallager bound does not give the optimum evaluation in the second-order coding rate.',\n",
       "  'authors': ['M. Hayashi'],\n",
       "  'date': '2009',\n",
       "  'identifier': '2104340231',\n",
       "  'references': ['1549664537',\n",
       "   '2142901448',\n",
       "   '79265410',\n",
       "   '2020347709',\n",
       "   '1653619892',\n",
       "   '1995875735',\n",
       "   '2162635854',\n",
       "   '2167830490',\n",
       "   '2081715952',\n",
       "   '1990309618'],\n",
       "  'title': 'Information Spectrum Approach to Second-Order Coding Rate in Channel Coding'},\n",
       " {'abstract': 'A survey of computer algorithms and philosophies applied to problems of feature extraction and pattern recognition in conjunction with image analysis is presented. The main emphasis is on usable techniques applicable to practical image processing systems. The various methods are discussed under the broad headings of microanalysis and macroanalysis.',\n",
       "  'authors': ['M.D. Levine'],\n",
       "  'date': '1969',\n",
       "  'identifier': '2150642297',\n",
       "  'references': ['1978818813',\n",
       "   '2116360511',\n",
       "   '2159498975',\n",
       "   '2158240273',\n",
       "   '2108729336',\n",
       "   '2161376274',\n",
       "   '2170633497',\n",
       "   '2067958620',\n",
       "   '1996992876',\n",
       "   '2125027853'],\n",
       "  'title': 'Feature extraction: A survey'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Anil K. Jain ', ' Richard C. Dubes'],\n",
       "  'date': '1988',\n",
       "  'identifier': '1971784203',\n",
       "  'references': ['2913066018',\n",
       "   '1975152892',\n",
       "   '2118587067',\n",
       "   '1572134371',\n",
       "   '1576534100',\n",
       "   '1533790012',\n",
       "   '2018388286',\n",
       "   '2086943813',\n",
       "   '1978616828',\n",
       "   '2120636855'],\n",
       "  'title': 'Algorithms for clustering data'},\n",
       " {'abstract': 'Abstract Using quadratic programming and utilizing the concept of ‘nearest proportional to size sampling design’, we propose a method for two-dimensional optimal controlled selection, which ensures zero probability to non-preferred samples. An alternative strategy for estimation of population total and variance is also suggested. The utility of the proposed procedure is demonstrated with the help of examples.',\n",
       "  'authors': ['Neeraj Tiwari 1', ' A.K. Nigam 2'],\n",
       "  'date': '2010',\n",
       "  'identifier': '1988858799',\n",
       "  'references': ['2117161260',\n",
       "   '2056444916',\n",
       "   '2325142117',\n",
       "   '2001947543',\n",
       "   '2330659229',\n",
       "   '2080301823',\n",
       "   '2330498442',\n",
       "   '1983482630',\n",
       "   '2123277030',\n",
       "   '1030489121'],\n",
       "  'title': 'On two-dimensional optimal controlled nearest proportional to size sampling designs'},\n",
       " {'abstract': 'We describe a trainable system for analyzing videos of developing C. elegans embryos. The system automatically detects, segments, and locates cells and nuclei in microscopic images. The system was designed as the central component of a fully automated phenotyping system. The system contains three modules 1) a convolutional network trained to classify each pixel into five categories: cell wall, cytoplasm, nucleus membrane, nucleus, outside medium; 2) an energy-based model, which cleans up the output of the convolutional network by learning local consistency constraints that must be satisfied by label images; 3) a set of elastic models of the embryo at various stages of development that are matched to the label images.',\n",
       "  'authors': ['Feng Ning 1',\n",
       "   ' D. Delhomme 2',\n",
       "   ' Y. LeCun 3',\n",
       "   ' F. Piano 1',\n",
       "   ' L. Bottou 4',\n",
       "   ' P.E. Barbano 3'],\n",
       "  'date': '2005',\n",
       "  'identifier': '2158778629',\n",
       "  'references': ['2164598857',\n",
       "   '2310919327',\n",
       "   '2147880316',\n",
       "   '2104095591',\n",
       "   '1647075334',\n",
       "   '2134557905',\n",
       "   '2121927366',\n",
       "   '2119823327',\n",
       "   '1991848143',\n",
       "   '2157364932'],\n",
       "  'title': 'Toward automatic phenotyping of developing embryos from videos'},\n",
       " {'abstract': 'This paper gives an overview of automatic speaker recognition technology, with an emphasis on text-independent recognition. Speaker recognition has been studied actively for several decades. We give an overview of both the classical and the state-of-the-art methods. We start with the fundamentals of automatic speaker recognition, concerning feature extraction and speaker modeling. We elaborate advanced computational techniques to address robustness and session variability. The recent progress from vectors towards supervectors opens up a new area of exploration and represents a technology trend. We also provide an overview of this recent development and discuss the evaluation methodology of speaker recognition systems. We conclude the paper with discussion on future directions.',\n",
       "  'authors': ['Tomi Kinnunen 1', ' Haizhou Li 2'],\n",
       "  'date': '2010',\n",
       "  'identifier': '2154278880',\n",
       "  'references': ['1663973292',\n",
       "   '1875231349',\n",
       "   '1560013842',\n",
       "   '1766888123',\n",
       "   '2132549764',\n",
       "   '2041823554',\n",
       "   '1634005169',\n",
       "   '3111754282',\n",
       "   '2108995755',\n",
       "   '2165880886'],\n",
       "  'title': 'An overview of text-independent speaker recognition: From features to supervectors'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Richard O. Duda ', ' Peter E. Hart ', ' David G. Stork'],\n",
       "  'date': '1999',\n",
       "  'identifier': '1506281249',\n",
       "  'references': ['2752885492',\n",
       "   '2798909945',\n",
       "   '1655990431',\n",
       "   '2752853835',\n",
       "   '1496462336',\n",
       "   '1669104078',\n",
       "   '2796291475',\n",
       "   '2751862591',\n",
       "   '2577556340',\n",
       "   '2044437392'],\n",
       "  'title': 'Pattern Classification (2nd ed.)'},\n",
       " {'abstract': \"Many problems of recent interest in statistics and machine learning can be posed in the framework of convex optimization. Due to the explosion in size and complexity of modern datasets, it is increasingly important to be able to solve problems with a very large number of features or training examples. As a result, both the decentralized collection or storage of these datasets as well as accompanying distributed solution methods are either necessary or at least highly desirable. In this review, we argue that the alternating direction method of multipliers is well suited to distributed convex optimization, and in particular to large-scale problems arising in statistics, machine learning, and related areas. The method was developed in the 1970s, with roots in the 1950s, and is equivalent or closely related to many other algorithms, such as dual decomposition, the method of multipliers, Douglas–Rachford splitting, Spingarn's method of partial inverses, Dykstra's alternating projections, Bregman iterative algorithms for l1 problems, proximal methods, and others. After briefly surveying the theory and history of the algorithm, we discuss applications to a wide variety of statistical and machine learning problems of recent interest, including the lasso, sparse logistic regression, basis pursuit, covariance selection, support vector machines, and many others. We also discuss general distributed optimization, extensions to the nonconvex setting, and efficient implementation, including some details on distributed MPI and Hadoop MapReduce implementations.\",\n",
       "  'authors': ['Stephen Boyd 1',\n",
       "   ' Neal Parikh 1',\n",
       "   ' Eric Chu 1',\n",
       "   ' Borja Peleato 1',\n",
       "   ' Jonathan Eckstein 2'],\n",
       "  'date': '2011',\n",
       "  'identifier': '2164278908',\n",
       "  'references': ['2296319761',\n",
       "   '2173213060',\n",
       "   '2156909104',\n",
       "   '2296616510',\n",
       "   '2145096794',\n",
       "   '1554944419',\n",
       "   '3029645440',\n",
       "   '2129638195',\n",
       "   '2100556411',\n",
       "   '1981420413'],\n",
       "  'title': 'Distributed Optimization and Statistical Learning Via the Alternating Direction Method of Multipliers'},\n",
       " {'abstract': \"How should ecologists and evolutionary biologists analyze nonnormal data that involve random effects? Nonnormal data such as counts or proportions often defy classical statistical procedures. Generalized linear mixed models (GLMMs) provide a more flexible approach for analyzing nonnormal data when random effects are present. The explosion of research on GLMMs in the last decade has generated considerable uncertainty for practitioners in ecology and evolution. Despite the availability of accurate techniques for estimating GLMM parameters in simple cases, complex GLMMs are challenging to fit and statistical inference such as hypothesis testing remains difficult. We review the use (and misuse) of GLMMs in ecology and evolution, discuss estimation and inference and summarize 'best-practice' data analysis procedures for scientists facing this challenge.\",\n",
       "  'authors': ['Benjamin M. Bolker 1',\n",
       "   ' Mollie E. Brooks 1',\n",
       "   ' Connie J. Clark 1',\n",
       "   ' Shane W. Geange 2',\n",
       "   ' John R. Poulsen 1',\n",
       "   ' M. Henry H. Stevens 3',\n",
       "   ' Jada-Simone S. White 1'],\n",
       "  'date': '2009',\n",
       "  'identifier': '2155988679',\n",
       "  'references': ['2009435671',\n",
       "   '1513618424',\n",
       "   '2115709314',\n",
       "   '1981457167',\n",
       "   '2076983043',\n",
       "   '648151759',\n",
       "   '1587094587',\n",
       "   '2130416410',\n",
       "   '2125001590',\n",
       "   '3015463134'],\n",
       "  'title': 'Generalized linear mixed models: a practical guide for ecology and evolution'},\n",
       " {'abstract': 'There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.',\n",
       "  'authors': ['S. Kirkpatrick 1', ' C. D. Gelatt 1', ' M. P. Vecchi 2'],\n",
       "  'date': '1987',\n",
       "  'identifier': '2581275558',\n",
       "  'references': ['2042986967',\n",
       "   '2022820481',\n",
       "   '2114552889',\n",
       "   '2022494241',\n",
       "   '2143037347',\n",
       "   '2056760934',\n",
       "   '2148673189',\n",
       "   '86906884',\n",
       "   '2014952973',\n",
       "   '2014068360'],\n",
       "  'title': 'Optimization by simulated annealing'},\n",
       " {'abstract': '',\n",
       "  'authors': ['B Schölkopf ', ' A Smola'],\n",
       "  'date': '2001',\n",
       "  'identifier': '3023786531',\n",
       "  'references': [],\n",
       "  'title': 'Learning with kernels'},\n",
       " {'abstract': 'Abstract In this paper, a multilingual automatic speech recognition (ASR) and language identification (LID) system is designed. In contrast to conventional multilingual ASR systems, this paper takes advantage of the complementarity of the ASR and LID modules. First, the LID module contributes to the language adaptive training of the multilingual acoustic model. Then, the ASR decoding information acts as the confidence metric to balance the LID results. To simulate complex multilingual speech recognition situations, two types of LID strategies are conducted. For a multilingual speech recognition task in which only one language is contained in the speech stream, the language information can be directly determined based on utterance-level judgment. Under this condition, a segment-level statistical component and a two-stage update strategy are designed to assist in the utterance-level language classification. In another multilingual speech recognition task, where the speech stream contains multiple languages simultaneously, the Viterbi language state retrieval method based on neural network (NN) classification is used to perform dynamic detection of the language state. In both cases, the ASR decoding information is used to adjust the language classification results. Without prior knowledge of language identity information, the enhanced LID module achieves an accuracy of 99.3% for utterance-level language judgment and 92.4% for dynamic language detection, and the multilingual ASR system also provides performance comparable to that of monolingual ASR systems.',\n",
       "  'authors': ['Danyang Liu ', ' Ji Xu ', ' Pengyuan Zhang ', ' Yonghong Yan'],\n",
       "  'date': '2021',\n",
       "  'identifier': '3115898234',\n",
       "  'references': ['2150769028',\n",
       "   '1524333225',\n",
       "   '1631260214',\n",
       "   '2293634267',\n",
       "   '2890964092',\n",
       "   '2173629880',\n",
       "   '2046932483',\n",
       "   '2142384583',\n",
       "   '2094342469',\n",
       "   '2172287020'],\n",
       "  'title': 'A unified system for multilingual speech recognition and language identification'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Hans Triebel'],\n",
       "  'date': '2008',\n",
       "  'identifier': '162854683',\n",
       "  'references': ['2079724595',\n",
       "   '2963269529',\n",
       "   '2024457004',\n",
       "   '3119524009',\n",
       "   '2075166036',\n",
       "   '1989334447',\n",
       "   '2052129269',\n",
       "   '2122588711',\n",
       "   '1997982786'],\n",
       "  'title': 'Theory of Function Spaces III'},\n",
       " {'abstract': \"In many pattern-recognition problems there exist dependencies among the patterns to be recognized. In the past, these dependencies have not been introduced into the mathematical model when designing an optimal pattern-recognition system. In this paper the optimal decision rule is derived under the assumption of Markov dependence among the patterns to be recognized. Subsequently, this decision rule is applied to character-recognition problems. The main idea is to balance appropriately the information which is obtained from contextual considerations and the information from measurements on the character being recognized and thus arrive at a decision using both. Bayes' decision in Markov chains is presented and this mode of decision is adapted to character recognition. A look-ahead mode of decision is presented. The problem of estimation of transition probabilities is discussed. The experimental system is described and results of experiments on English legal text and names are presented.\",\n",
       "  'authors': ['J. Raviv'],\n",
       "  'date': '1967',\n",
       "  'identifier': '2104474593',\n",
       "  'references': ['2799137445',\n",
       "   '2079145130',\n",
       "   '2087686387',\n",
       "   '2014056039',\n",
       "   '2324007084',\n",
       "   '2163017429',\n",
       "   '2011126126',\n",
       "   '2149701797',\n",
       "   '2090758519',\n",
       "   '2080208532'],\n",
       "  'title': 'Decision making in Markov chains applied to the problem of pattern recognition'},\n",
       " {'abstract': 'Despite significant recent advances in the field of face recognition [10, 14, 15, 17], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors.',\n",
       "  'authors': ['Florian Schroff ', ' Dmitry Kalenichenko ', ' James Philbin'],\n",
       "  'date': '2015',\n",
       "  'identifier': '2096733369',\n",
       "  'references': ['2097117768',\n",
       "   '1849277567',\n",
       "   '2146502635',\n",
       "   '2963911037',\n",
       "   '2168231600',\n",
       "   '2145287260',\n",
       "   '2294059674',\n",
       "   '1782590233',\n",
       "   '1498436455',\n",
       "   '2296073425'],\n",
       "  'title': 'FaceNet: A unified embedding for face recognition and clustering'},\n",
       " {'abstract': 'Emotions are universally recognized from facial expressions--or so it has been claimed. To support that claim, research has been carried out in various modern cultures and in cultures relatively isolated from Western influence. A review of the methods used in that research raises questions of its ecological, convergent, and internal validity. Forced-choice response format, within-subject design, preselected photographs of posed facial expressions, and other features of method are each problematic. When they are altered, less supportive or nonsupportive results occur. When they are combined, these method factors may help to shape the results. Facial expressions and emotion labels are probably associated, but the association may vary with culture and is loose enough to be consistent with various alternative accounts, 8 of which are discussed.',\n",
       "  'authors': ['James A. Russell'],\n",
       "  'date': '1994',\n",
       "  'identifier': '1975238145',\n",
       "  'references': ['1966797434',\n",
       "   '2149628368',\n",
       "   '2156516654',\n",
       "   '2033702744',\n",
       "   '2057894089',\n",
       "   '2139252049',\n",
       "   '2024218186',\n",
       "   '2154611638',\n",
       "   '2471954549',\n",
       "   '628413814'],\n",
       "  'title': 'Is there universal recognition of emotion from facial expression? A review of the cross-cultural studies.'},\n",
       " {'abstract': 'The rapid rate at which the field of digital picture processing has grown in the past five years had necessitated extensive revisions and the introduction of topics not found in the original edition.',\n",
       "  'authors': ['Azriel Rosenfeld ', ' Avinash C. Kak'],\n",
       "  'date': '1976',\n",
       "  'identifier': '1622620102',\n",
       "  'references': ['1992419399',\n",
       "   '1997063559',\n",
       "   '2061240006',\n",
       "   '2160754664',\n",
       "   '2103504761',\n",
       "   '2042316011',\n",
       "   '2003370853',\n",
       "   '1972544340',\n",
       "   '1579236455',\n",
       "   '2012554041'],\n",
       "  'title': 'Digital Picture Processing'},\n",
       " {'abstract': 'We introduce a large equivalence class of graph properties, all of which are shared by so-called random graphs. Unlike random graphs, however, it is often relatively easy to verify that a particular family of graphs possesses some property in this class.',\n",
       "  'authors': ['Fan R. K. Chung 1',\n",
       "   ' Ronald L. Graham 2',\n",
       "   ' Richard M. Wilson 3'],\n",
       "  'date': '1989',\n",
       "  'identifier': '2901284226',\n",
       "  'references': ['2905110430',\n",
       "   '1998400388',\n",
       "   '1552744309',\n",
       "   '2088164510',\n",
       "   '2064197162',\n",
       "   '23397926',\n",
       "   '2079810243',\n",
       "   '184868352',\n",
       "   '1984080039',\n",
       "   '164275096'],\n",
       "  'title': 'Quasi-random graphs'},\n",
       " {'abstract': 'The Audio/Visual Emotion Challenge and Workshop (AVEC 2016) \"Depression, Mood and Emotion\" will be the sixth competition event aimed at comparison of multimedia processing and machine learning methods for automatic audio, visual and physiological depression and emotion analysis, with all participants competing under strictly the same conditions. The goal of the Challenge is to provide a common benchmark test set for multi-modal information processing and to bring together the depression and emotion recognition communities, as well as the audio, video and physiological processing communities, to compare the relative merits of the various approaches to depression and emotion recognition under well-defined and strictly comparable conditions and establish to what extent fusion of the approaches is possible and beneficial. This paper presents the challenge guidelines, the common data used, and the performance of the baseline system on the two tasks.',\n",
       "  'authors': ['Michel Valstar 1',\n",
       "   ' Jonathan Gratch 2',\n",
       "   ' Björn Schuller 3',\n",
       "   ' Fabien Ringeval 4',\n",
       "   ' Denis Lalanne 5',\n",
       "   ' Mercedes Torres Torres 1',\n",
       "   ' Stefan Scherer 2',\n",
       "   ' Giota Stratou 2',\n",
       "   ' Roddy Cowie 6',\n",
       "   ' Maja Pantic 3'],\n",
       "  'date': '2016',\n",
       "  'identifier': '2346454595',\n",
       "  'references': ['2133990480',\n",
       "   '2118585731',\n",
       "   '2157285372',\n",
       "   '2002055708',\n",
       "   '2395639500',\n",
       "   '2048533792',\n",
       "   '2141403362',\n",
       "   '2090777335',\n",
       "   '2293081071',\n",
       "   '2239141610'],\n",
       "  'title': 'AVEC 2016: Depression, Mood, and Emotion Recognition Workshop and Challenge'},\n",
       " {'abstract': \"We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide F1 scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.\",\n",
       "  'authors': ['Bryan Perozzi ', ' Rami Al-Rfou ', ' Steven Skiena'],\n",
       "  'date': '2014',\n",
       "  'identifier': '3104097132',\n",
       "  'references': ['2618530766',\n",
       "   '2153579005',\n",
       "   '1614298861',\n",
       "   '2163922914',\n",
       "   '2168231600',\n",
       "   '2122646361',\n",
       "   '2141599568',\n",
       "   '2118585731',\n",
       "   '2117130368',\n",
       "   '2147768505'],\n",
       "  'title': 'DeepWalk: online learning of social representations'},\n",
       " {'abstract': \"The need to model the relation between discourse structure and linguistic features of utterances is almost universally acknowledged in the literature on discourse. However, there is only weak consensus on what the units of discourse structure are, or the criteria for recognizing and generating them. We present quantitative results of a two-part study using a corpus of spontaneous, narrative monologues. The first part of our paper presents a method for empirically validating multitutterance units referred to as discourse segments. We report highly significant results of segmentations performed by naive subjects, where a commonsense notion of speaker intention is the segmentation criterion. In the second part of our study, data abstracted from the subjects' segmentations serve as a target for evaluating two sets of algorithms that use utterance features to perform segmentation. On the first algorithm set, we evaluate and compare the correlation of discourse segmentation with three types of linguistic cues (referential noun phrases, cue words, and pauses). We then develop a second set using two methods: error analysis and machine learning. Testing the new algorithms on a new data set shows that when multiple sources of linguistic knowledge are used concurrently, algorithm performance improves.\",\n",
       "  'authors': ['Rebecca J. Passonneau 1', ' Diane J. Litman 2'],\n",
       "  'date': '1997',\n",
       "  'identifier': '1669912781',\n",
       "  'references': ['2125055259',\n",
       "   '3085162807',\n",
       "   '2153804780',\n",
       "   '2045738181',\n",
       "   '2124741472',\n",
       "   '2167702024',\n",
       "   '1710422233',\n",
       "   '1513874326',\n",
       "   '2100873065',\n",
       "   '1704669313'],\n",
       "  'title': 'Discourse segmentation by human and automated means'},\n",
       " {'abstract': 'From the Publisher: With this text, you gain an understanding of the fundamental concepts of algorithms, the very heart of computer science. It introduces the basic data structures and programming techniques often used in efficient algorithms. Covers use of lists, push-down stacks, queues, trees, and graphs. Later chapters go into sorting, searching and graphing algorithms, the string-matching algorithms, and the Schonhage-Strassen integer-multiplication algorithm. Provides numerous graded exercises at the end of each chapter. 0201000296B04062001',\n",
       "  'authors': ['Alfred V. Aho ', ' John E. Hopcroft'],\n",
       "  'date': '1974',\n",
       "  'identifier': '1655990431',\n",
       "  'references': ['2340735175',\n",
       "   '1506281249',\n",
       "   '2156186849',\n",
       "   '2080267935',\n",
       "   '1990600049',\n",
       "   '1977496278',\n",
       "   '2001496424',\n",
       "   '2141420453',\n",
       "   '1997841190'],\n",
       "  'title': 'The Design and Analysis of Computer Algorithms'},\n",
       " {'abstract': '',\n",
       "  'authors': ['A. Asuncion'],\n",
       "  'date': '2007',\n",
       "  'identifier': '3120740533',\n",
       "  'references': ['2146502635',\n",
       "   '3100515187',\n",
       "   '2963399829',\n",
       "   '2963977107',\n",
       "   '2106525823',\n",
       "   '2099419573',\n",
       "   '2170505850',\n",
       "   '2786672974',\n",
       "   '2962790689'],\n",
       "  'title': 'UCI Machine Learning Repository'},\n",
       " {'abstract': 'An algorithm for the analysis of multivariate data is presented and is discussed in terms of specific examples. The algorithm seeks to find one-and two-dimensional linear projections of multivariate data that are relatively highly revealing.',\n",
       "  'authors': ['J.H. Friedman ', ' J.W. Tukey'],\n",
       "  'date': '1974',\n",
       "  'identifier': '2082612735',\n",
       "  'references': ['1972969203',\n",
       "   '2026258334',\n",
       "   '2078950386',\n",
       "   '2134312057',\n",
       "   '2114013702',\n",
       "   '1964712205',\n",
       "   '3047768732',\n",
       "   '2318609557',\n",
       "   '2001619934',\n",
       "   '2073127487'],\n",
       "  'title': 'A Projection Pursuit Algorithm for Exploratory Data Analysis'},\n",
       " {'abstract': 'Abstract The two-dimensional controlled selection problem and the problem of maximizing the overlap of old and new primary sampling units after restratification and change of selection probabilities have been studied for several decades but have never been completely solved until now. Using transportation theory, complete solutions are obtained here for these and other problems. The solution to the controlled selection problem is based on a specific transportation model that was originally developed, in a previous paper by Cox and Ernst (1982), to solve completely the controlled rounding problem, namely the problem of optimally rounding real-valued entries in a two-way tabular array to adjacent integer values in a manner that preserves the tabular (additive) structure of the array. This model is also applied to other statistical problems, such as raking and statistical disclosure for frequency count tabulations and microdata.',\n",
       "  'authors': ['Beverley D. Causey ',\n",
       "   ' Lawrence H. Cox ',\n",
       "   ' Lawrence R. Ernst'],\n",
       "  'date': '1985',\n",
       "  'identifier': '2056444916',\n",
       "  'references': ['2074673068',\n",
       "   '2127470768',\n",
       "   '2001792610',\n",
       "   '2061408624',\n",
       "   '2031763823',\n",
       "   '2003637146',\n",
       "   '2077499269',\n",
       "   '2018154207',\n",
       "   '1978796736',\n",
       "   '1975268921'],\n",
       "  'title': 'Applications of Transportation Theory to Statistical Problems'},\n",
       " {'abstract': '',\n",
       "  'authors': ['J. Kennedy ', ' R. Eberhart'],\n",
       "  'date': '2002',\n",
       "  'identifier': '2152195021',\n",
       "  'references': ['1497256448',\n",
       "   '2166843422',\n",
       "   '2176056341',\n",
       "   '2998069724',\n",
       "   '1986051627',\n",
       "   '2996256129',\n",
       "   '1594358512',\n",
       "   '2346323361',\n",
       "   '1943122965',\n",
       "   '2001619934'],\n",
       "  'title': 'Particle swarm optimization'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Richard Clark Pasco'],\n",
       "  'date': '1976',\n",
       "  'identifier': '1519253855',\n",
       "  'references': ['2142901448',\n",
       "   '2751862591',\n",
       "   '1995875735',\n",
       "   '2046419776',\n",
       "   '2131024393',\n",
       "   '2034323860',\n",
       "   '1992371956',\n",
       "   '1965555277',\n",
       "   '158805393',\n",
       "   '2109227390'],\n",
       "  'title': 'Source coding algorithms for fast data compression'},\n",
       " {'abstract': 'The problem of predicting the next word a speaker will say, given the words already spoken; is discussed. Specifically, the problem is to estimate the probability that a given word will be the next word uttered. Algorithms are presented for automatically constructing a binary decision tree designed to estimate these probabilities. At each node of the tree there is a yes/no question relating to the words already spoken, and at each leaf there is a probability distribution over the allowable vocabulary. Ideally, these nodal questions can take the form of arbitrarily complex Boolean expressions, but computationally cheaper alternatives are also discussed. Some results obtained on a 5000-word vocabulary with a tree designed to predict the next word spoken from the preceding 20 words are included. The tree is compared to an equivalent trigram model and shown to be superior. >',\n",
       "  'authors': ['L.R. Bahl ', ' P.F. Brown ', ' P.V. de Souza ', ' R.L. Mercer'],\n",
       "  'date': '1989',\n",
       "  'identifier': '2099345940',\n",
       "  'references': ['2581275558',\n",
       "   '1966812932',\n",
       "   '1990005915',\n",
       "   '1521239006',\n",
       "   '1990822176',\n",
       "   '2120234416',\n",
       "   '193579291',\n",
       "   '1995564620',\n",
       "   '2079145130'],\n",
       "  'title': 'A tree-based statistical language model for natural language speech recognition'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Christos H. Papadimitriou 1',\n",
       "   ' Hisao Tamaki 2',\n",
       "   ' Prabhakar Raghavan 3',\n",
       "   ' Santosh Vempala 4'],\n",
       "  'date': '1998',\n",
       "  'identifier': '2063392856',\n",
       "  'references': ['2138621811',\n",
       "   '2798909945',\n",
       "   '2147152072',\n",
       "   '1956559956',\n",
       "   '2072773380',\n",
       "   '1979750072',\n",
       "   '2013737143',\n",
       "   '2983896310',\n",
       "   '2058616517',\n",
       "   '2106285343'],\n",
       "  'title': 'Latent semantic indexing: a probabilistic analysis'},\n",
       " {'abstract': 'Profile hidden Markov models (profile HMMs) and probabilistic inference methods have made important contributions to the theory of sequence database homology search. However, practical use of profile HMM methods has been hindered by the computational expense of existing software implementations. Here I describe an acceleration heuristic for profile HMMs, the “multiple segment Viterbi” (MSV) algorithm. The MSV algorithm computes an optimal sum of multiple ungapped local alignment segments using a striped vector-parallel approach previously described for fast Smith/Waterman alignment. MSV scores follow the same statistical distribution as gapped optimal local alignment scores, allowing rapid evaluation of significance of an MSV score and thus facilitating its use as a heuristic filter. I also describe a 20-fold acceleration of the standard profile HMM Forward/Backward algorithms using a method I call “sparse rescaling”. These methods are assembled in a pipeline in which high-scoring MSV hits are passed on for reanalysis with the full HMM Forward/Backward algorithm. This accelerated pipeline is implemented in the freely available HMMER3 software package. Performance benchmarks show that the use of the heuristic MSV filter sacrifices negligible sensitivity compared to unaccelerated profile HMM searches. HMMER3 is substantially more sensitive and 100- to 1000-fold faster than HMMER2. HMMER3 is now about as fast as BLAST for protein searches.',\n",
       "  'authors': ['Sean R. Eddy'],\n",
       "  'date': '2011',\n",
       "  'identifier': '2138122982',\n",
       "  'references': ['2158714788',\n",
       "   '2055043387',\n",
       "   '2125838338',\n",
       "   '2141885858',\n",
       "   '2142678478',\n",
       "   '2009570821',\n",
       "   '2116423958',\n",
       "   '2087064593',\n",
       "   '2102122585',\n",
       "   '2612240612'],\n",
       "  'title': 'Accelerated Profile HMM Searches.'},\n",
       " {'abstract': 'We describe the OntoNotes methodology and its result, a large multilingual richly-annotated corpus constructed at 90% interannotator agreement. An initial portion (300K words of English newswire and 250K words of Chinese newswire) will be made available to the community during 2007.',\n",
       "  'authors': ['Eduard Hovy 1',\n",
       "   ' Mitchell Marcus 2',\n",
       "   ' Martha Palmer 3',\n",
       "   ' Lance Ramshaw 4',\n",
       "   ' Ralph Weischedel 4'],\n",
       "  'date': '2006',\n",
       "  'identifier': '2088911157',\n",
       "  'references': ['1632114991',\n",
       "   '2158847908',\n",
       "   '2115792525',\n",
       "   '2075123415',\n",
       "   '1940888119',\n",
       "   '131863957',\n",
       "   '2102423300',\n",
       "   '2113914762',\n",
       "   '2123311362',\n",
       "   '401759911'],\n",
       "  'title': 'OntoNotes: The 90% Solution'},\n",
       " {'abstract': 'Many research designs require the assessment of inter-rater reliability (IRR) to demonstrate consistency among observational ratings provided by multiple coders. However, many studies use incorrect statistical procedures, fail to fully report the information necessary to interpret their results, or do not address how IRR affects the power of their subsequent analyses for hypothesis testing. This paper provides an overview of methodological issues related to the assessment of IRR with a focus on study design, selection of appropriate statistics, and the computation, interpretation, and reporting of some commonly-used IRR statistics. Computational examples include SPSS and R syntax for computing Cohen’s kappa and intra-class correlations to assess IRR.',\n",
       "  'authors': ['Kevin A. Hallgren'],\n",
       "  'date': '2012',\n",
       "  'identifier': '2155243985',\n",
       "  'references': ['2164777277',\n",
       "   '2061504941',\n",
       "   '2141403362',\n",
       "   '2067724039',\n",
       "   '3021916629',\n",
       "   '2063085086',\n",
       "   '2002664886',\n",
       "   '1589586359',\n",
       "   '2070285512',\n",
       "   '2053154970'],\n",
       "  'title': 'Computing Inter-Rater Reliability for Observational Data: An Overview and Tutorial'},\n",
       " {'abstract': \"The increasing volume of data in modern business and science calls for more complex and sophisticated tools. Although advances in data mining technology have made extensive data collection much easier, it's still always evolving and there is a constant need for new techniques and tools that can help us transform this data into useful information and knowledge. Since the previous edition's publication, great advances have been made in the field of data mining. Not only does the third of edition of Data Mining: Concepts and Techniques continue the tradition of equipping you with an understanding and application of the theory and practice of discovering patterns hidden in large data sets, it also focuses on new, important topics in the field: data warehouses and data cube technology, mining stream, mining social networks, and mining spatial, multimedia and other complex data. Each chapter is a stand-alone guide to a critical topic, presenting proven algorithms and sound implementations ready to be used directly or with strategic modification against live data. This is the resource you need if you want to apply today's most powerful data mining techniques to meet real business challenges. * Presents dozens of algorithms and implementation examples, all in pseudo-code and suitable for use in real-world, large-scale data mining projects. * Addresses advanced topics such as mining object-relational databases, spatial databases, multimedia databases, time-series databases, text databases, the World Wide Web, and applications in several fields. *Provides a comprehensive, practical look at the concepts and techniques you need to get the most out of real business data\",\n",
       "  'authors': ['Jiawei Han 1', ' Micheline Kamber 2', ' Jian Pei 2'],\n",
       "  'date': '2000',\n",
       "  'identifier': '2140190241',\n",
       "  'references': ['2156909104',\n",
       "   '2911964244',\n",
       "   '2912565176',\n",
       "   '2148603752',\n",
       "   '1639032689',\n",
       "   '2331432542',\n",
       "   '2008620264',\n",
       "   '1554944419',\n",
       "   '1554663460',\n",
       "   '1570448133'],\n",
       "  'title': 'Data Mining: Concepts and Techniques'},\n",
       " {'abstract': \"Reynolds, Douglas A., Quatieri, Thomas F., and Dunn, Robert B., Speaker Verification Using Adapted Gaussian Mixture Models, Digital Signal Processing10(2000), 19Â?41.In this paper we describe the major elements of MIT Lincoln Laboratory's Gaussian mixture model (GMM)-based speaker verification system used successfully in several NIST Speaker Recognition Evaluations (SREs). The system is built around the likelihood ratio test for verification, using simple but effective GMMs for likelihood functions, a universal background model (UBM) for alternative speaker representation, and a form of Bayesian adaptation to derive speaker models from the UBM. The development and use of a handset detector and score normalization to greatly improve verification performance is also described and discussed. Finally, representative performance benchmarks and system behavior experiments on NIST SRE corpora are presented.\",\n",
       "  'authors': ['Douglas A. Reynolds ',\n",
       "   ' Thomas F. Quatieri ',\n",
       "   ' Robert B. Dunn'],\n",
       "  'date': '2000',\n",
       "  'identifier': '2041823554',\n",
       "  'references': ['2049633694',\n",
       "   '3017143921',\n",
       "   '2165880886',\n",
       "   '2100969003',\n",
       "   '2069883713',\n",
       "   '1586405805',\n",
       "   '2135346934',\n",
       "   '97107628',\n",
       "   '2167768673',\n",
       "   '1997873121'],\n",
       "  'title': 'Speaker Verification Using Adapted Gaussian Mixture Models'},\n",
       " {'abstract': 'The representation of large subsets of the World Wide Web in the form of a directed graph has been extensively used to analyze structure, behavior, and evolution of those so-called Web graphs. However, interesting Web graphs are very large and their classical representations do not fit into the main memory of typical computers, whereas the required graph algorithms perform inefficiently on secondary memory. Compressed graph representations drastically reduce their space requirements while allowing their efficient navigation in compressed form. While the most basic navigation operation is to retrieve the successors of a node, several important Web graph algorithms require support for extended queries, such as finding the predecessors of a node, checking the presence of a link, or retrieving links between ranges of nodes. Those are seldom supported by compressed graph representations. This paper presents the k^2-tree, a novel Web graph representation based on a compact tree structure that takes advantage of large empty areas of the adjacency matrix of the graph. The representation not only retrieves successors and predecessors in symmetric fashion, but also it is particularly efficient to check for specific links between nodes, or between ranges of nodes, or to list the links between ranges. Compared to the best representations in the literature supporting successor and predecessor queries, our technique offers the least space usage (1-3 bits per link) while supporting fast navigation to predecessors and successors ([email protected] per neighbor retrieved) and sharply outperforming the others on the extended queries. The representation is also of general interest and can be used to compress other kinds of graphs and data structures.',\n",
       "  'authors': ['Nieves R. Brisaboa 1', ' Susana Ladra 1', ' Gonzalo Navarro 2'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2017333496',\n",
       "  'references': ['3013264884',\n",
       "   '1854214752',\n",
       "   '2175110005',\n",
       "   '1497953515',\n",
       "   '2151626491',\n",
       "   '2111002549',\n",
       "   '1994727615',\n",
       "   '2020423193',\n",
       "   '2129620481',\n",
       "   '2082773934'],\n",
       "  'title': 'Compact representation of Web graphs with extended functionality'},\n",
       " {'abstract': '',\n",
       "  'authors': ['W. A. Pridmore ', ' F. H. C. Marriott'],\n",
       "  'date': '1993',\n",
       "  'identifier': '2577556340',\n",
       "  'references': ['1506281249',\n",
       "   '2114075052',\n",
       "   '615175138',\n",
       "   '2134816929',\n",
       "   '2010583300',\n",
       "   '2155239356',\n",
       "   '2013605401',\n",
       "   '1498826342',\n",
       "   '1975371918',\n",
       "   '1996605030'],\n",
       "  'title': 'A dictionary of statistical terms.'},\n",
       " {'abstract': \"1 General introduction.- 1.1 Introduction.- 1.2 Some applications of finite mixture distributions.- 1.3 Definition.- 1.4 Estimation methods.- 1.4.1 Maximum likelihood.- 1.4.2 Bayesian estimation.- 1.4.3 Inversion and error minimization.- 1.4.4 Other methods.- 1.4.5 Estimating the number of components.- 1.5 Summary.- 2 Mixtures of normal distributions.- 2.1 Introduction.- 2.2 Some descriptive properties of mixtures of normal distributions.- 2.3 Estimating the parameters in normal mixture distributions.- 2.3.1 Method of moments estimation.- 2.3.2 Maximum likelihood estimation.- 2.3.3 Maximum likelihood estimates for grouped data.- 2.3.4 Obtaining initial parameter values for the maximum likelihood estimation algorithms.- 2.3.5 Graphical estimation techniques.- 2.3.6 Other estimation methods.- 2.4 Summary.- 3 Mixtures of exponential and other continuous distributions.- 3.1 Exponential mixtures.- 3.2 Estimating exponential mixture parameters.- 3.2.1 The method of moments and generalizations.- 3.2.2 Maximum likelihood.- 3.3 Properties of exponential mixtures.- 3.4 Other continuous distributions.- 3.4.1 Non-central chi-squared distribution.- 3.4.2 Non-central F distribution.- 3.4.3 Beta distributions.- 3.4.4 Doubly non-central t distribution.- 3.4.5 Planck's distribution.- 3.4.6 Logistic.- 3.4.7 Laplace.- 3.4.8 Weibull.- 3.4.9 Gamma.- 3.5 Mixtures of different component types.- 3.6 Summary.- 4 Mixtures of discrete distributions.- 4.1 Introduction.- 4.2 Mixtures of binomial distributions.- 4.2.1 Moment estimators for binomial mixtures.- 4.2.2 Maximum likelihood estimators for mixtures of binomial distributions.- 4.2.3 Other estimation methods for mixtures of binomial distributions.- 4.3 Mixtures of Poisson distributions.- 4.3.1 Moment estimators for mixtures of Poisson distributions.- 4.3.2 Maximum likelihood estimators for a Poisson mixture.- 4.4 Mixtures of Poisson and binomial distributions.- 4.5 Mixtures of other discrete distributions.- 4.6 Summary.- 5 Miscellaneous topics.- 5.1 Introduction.- 5.2 Determining the number of components in a mixture.- 5.2.1 Informal diagnostic tools for the detection of mixtures.- 5.2.2 Testing hypotheses on the number of components in a mixture.- 5.3 Probability density function estimation.- 5.4 Miscellaneous problems.- 5.5 Summary.- References.\",\n",
       "  'authors': ['Brian Everitt ', ' D. J. Hand'],\n",
       "  'date': '1981',\n",
       "  'identifier': '2121407732',\n",
       "  'references': ['1966385142',\n",
       "   '1770713873',\n",
       "   '2289748525',\n",
       "   '2098405376',\n",
       "   '1989851822',\n",
       "   '2156142001',\n",
       "   '2171814052',\n",
       "   '2165225968',\n",
       "   '2087284982',\n",
       "   '2033009866'],\n",
       "  'title': 'Finite Mixture Distributions'},\n",
       " {'abstract': 'Abstract We propose that, for the task of object recognition, the visual system decomposes shapes into parts, that it does so using a rule defining part boundaries rather than part shapes, that the rule exploits a uniformity of nature—transversality, and that parts with their descriptions and spatial relations provide a first index into a memory of shapes. This rule allows an explanation of several visual illusions. We stress the role inductive inference in our theory and conclude with a pre´cis of unsolved problems.',\n",
       "  'authors': ['D. D. Hoffman 1', ' W. A. Richards 2'],\n",
       "  'date': '1987',\n",
       "  'identifier': '1501418839',\n",
       "  'references': ['3017143921',\n",
       "   '2740373864',\n",
       "   '2109863423',\n",
       "   '1974525874',\n",
       "   '2164934677',\n",
       "   '2002312729',\n",
       "   '1565212980',\n",
       "   '2081519360',\n",
       "   '1989701469',\n",
       "   '2081981374'],\n",
       "  'title': 'Parts of recognition'},\n",
       " {'abstract': 'Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.',\n",
       "  'authors': ['Marius Cordts 1',\n",
       "   ' Mohamed Omran 2',\n",
       "   ' Sebastian Ramos 3',\n",
       "   ' Timo Rehfeld 1',\n",
       "   ' Markus Enzweiler 3',\n",
       "   ' Rodrigo Benenson 2',\n",
       "   ' Uwe Franke 3',\n",
       "   ' Stefan Roth 1',\n",
       "   ' Bernt Schiele 2'],\n",
       "  'date': '2016',\n",
       "  'identifier': '2340897893',\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '639708223',\n",
       "   '2919115771',\n",
       "   '2102605133',\n",
       "   '2117539524',\n",
       "   '1903029394',\n",
       "   '1536680647',\n",
       "   '2168356304',\n",
       "   '1861492603'],\n",
       "  'title': 'The Cityscapes Dataset for Semantic Urban Scene Understanding'},\n",
       " {'abstract': \"The authors propose a new method for unsupervised classification of terrain types and man-made objects using polarimetric synthetic aperture radar (SAR) data. This technique is a combination of the unsupervised classification based on polarimetric target decomposition, S.R. Cloude et al. (1997), and the maximum likelihood classifier based on the complex Wishart distribution for the polarimetric covariance matrix, J.S. Lee et al. (1994). The authors use Cloude and Pottier's method to initially classify the polarimetric SAR image. The initial classification map defines training sets for classification based on the Wishart distribution. The classified results are then used to define training sets for the next iteration. Significant improvement has been observed in iteration. The iteration ends when the number of pixels switching classes becomes smaller than a predetermined number or when other criteria are met. The authors observed that the class centers in the entropy-alpha plane are shifted by each iteration. The final class centers in the entropy-alpha plane are useful for class identification by the scattering mechanism associated with each zone. The advantages of this method are the automated classification, and the interpretation of each class based on scattering mechanism. The effectiveness of this algorithm is demonstrated using a JPL/AIRSAR polarimetric SAR image.\",\n",
       "  'authors': ['Jong-Sen Lee 1',\n",
       "   ' M.R. Grunes 1',\n",
       "   ' T.L. Ainsworth 1',\n",
       "   ' Li-Jen Du 1',\n",
       "   ' D.L. Schuler 1',\n",
       "   ' S.R. Cloude 2'],\n",
       "  'date': '1999',\n",
       "  'identifier': '2164296623',\n",
       "  'references': ['2078985447',\n",
       "   '2133989913',\n",
       "   '2130762895',\n",
       "   '2101690226',\n",
       "   '1985194020',\n",
       "   '2051224630',\n",
       "   '2115226769',\n",
       "   '2147439738',\n",
       "   '2166960586',\n",
       "   '2095743915'],\n",
       "  'title': 'Unsupervised classification using polarimetric decomposition and the complex Wishart classifier'},\n",
       " {'abstract': 'The purpose of these lecture notes is to provide an introduction to the general theory of empirical risk minimization with an emphasis on excess risk bounds and oracle inequalities in penalized problems. In recent years, there have been new developments in this area motivated by the study of new classes of methods in machine learning such as large margin classification methods (boosting, kernel machines). The main probabilistic tools involved in the analysis of these problems are concentration and deviation inequalities by Talagrand along with other methods of empirical processes theory (symmetrization inequalities, contraction inequality for Rademacher sums, entropy and generic chaining bounds). Sparse recovery based on l_1-type penalization and low rank matrix recovery based on the nuclear norm penalization are other active areas of research, where the main problems can be stated in the framework of penalized empirical risk minimization, and concentration inequalities and empirical processes tools have proved to be very useful.',\n",
       "  'authors': ['Vladimir Koltchinskii ',\n",
       "   \" École d'été de probabilités de Saint-Flour\"],\n",
       "  'date': '2011',\n",
       "  'identifier': '648260396',\n",
       "  'references': ['1631356911',\n",
       "   '2296616510',\n",
       "   '2148603752',\n",
       "   '2164452299',\n",
       "   '2124608575',\n",
       "   '2202343345',\n",
       "   '2015418199',\n",
       "   '403935824',\n",
       "   '2097323375',\n",
       "   '2050834445'],\n",
       "  'title': 'Oracle inequalities in empirical risk minimization and sparse recovery problems'},\n",
       " {'abstract': 'Abstract A new hypothesis about the role of focused attention is proposed. The feature-integration theory of attention suggests that attention must be directed serially to each stimulus in a display whenever conjunctions of more than one separable feature are needed to characterize or distinguish the possible objects presented. A number of predictions were tested in a variety of paradigms including visual search, texture segregation, identification and localization, and using both separable dimensions (shape and color) and local elements or parts of figures (lines, curves, etc. in letters) as the features to be integrated into complex wholes. The results were in general consistent with the hypothesis. They offer a new set of criteria for distinguishing separable from integral features and a new rationale for predicting which tasks will show attention limits and which will not.',\n",
       "  'authors': ['Anne M. Treisman 1', ' Garry Gelade 2'],\n",
       "  'date': '1980',\n",
       "  'identifier': '2149095485',\n",
       "  'references': ['2156046656',\n",
       "   '1708089250',\n",
       "   '2322677917',\n",
       "   '1535782774',\n",
       "   '2077763860',\n",
       "   '1988660784',\n",
       "   '1980429329',\n",
       "   '2060032378',\n",
       "   '2056101986',\n",
       "   '2140403837'],\n",
       "  'title': 'A feature-integration theory of attention'},\n",
       " {'abstract': 'Two different procedures for effecting a frequency analysis of a time-dependent signal locally in time are studied. The first procedure is the short-time or windowed Fourier transform; the second is the wavelet transform, in which high-frequency components are studied with sharper time resolution than low-frequency components. The similarities and the differences between these two methods are discussed. For both schemes a detailed study is made of the reconstruction method and its stability as a function of the chosen time-frequency density. Finally, the notion of time-frequency localization is made precise, within this framework, by two localization theorems. >',\n",
       "  'authors': ['I. Daubechies'],\n",
       "  'date': '1990',\n",
       "  'identifier': '1996021349',\n",
       "  'references': ['2132984323',\n",
       "   '2098914003',\n",
       "   '2166982406',\n",
       "   '2096684483',\n",
       "   '2087377426',\n",
       "   '2013987111',\n",
       "   '1975474302',\n",
       "   '1989491465',\n",
       "   '2014757225',\n",
       "   '2123487365'],\n",
       "  'title': 'The wavelet transform, time-frequency localization and signal analysis'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Dana Harry Ballard ', ' Christopher M. Brown'],\n",
       "  'date': '1982',\n",
       "  'identifier': '2740373864',\n",
       "  'references': ['2124351162',\n",
       "   '2121647436',\n",
       "   '1995903777',\n",
       "   '1504381788',\n",
       "   '2914885528',\n",
       "   '2169805405',\n",
       "   '3111480503',\n",
       "   '1518138188',\n",
       "   '2167501464'],\n",
       "  'title': 'Computer vision'},\n",
       " {'abstract': 'We propose a novel approach to learn and recognize natural scene categories. Unlike previous work, it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a \"theme\". In previous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes without supervision. We report satisfactory categorization performances on a large set of 13 categories of complex scenes.',\n",
       "  'authors': ['L. Fei-Fei ', ' P. Perona'],\n",
       "  'date': '2005',\n",
       "  'identifier': '2107034620',\n",
       "  'references': ['1880262756',\n",
       "   '2124386111',\n",
       "   '2045656233',\n",
       "   '1566135517',\n",
       "   '1484228140',\n",
       "   '2127006916',\n",
       "   '2171188998',\n",
       "   '1699734612',\n",
       "   '2104924585',\n",
       "   '2094414211'],\n",
       "  'title': 'A Bayesian hierarchical model for learning natural scene categories'},\n",
       " {'abstract': \"During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression and path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.\",\n",
       "  'authors': ['Trevor Hastie ', ' Robert J. Tibshirani ', ' Jerome Friedman'],\n",
       "  'date': '2005',\n",
       "  'identifier': '1554944419',\n",
       "  'references': ['2140190241',\n",
       "   '2164278908',\n",
       "   '2179438025',\n",
       "   '2122825543',\n",
       "   '2063978378',\n",
       "   '2131975293',\n",
       "   '2117756735',\n",
       "   '2087681821',\n",
       "   '2109574129'],\n",
       "  'title': 'The elements of statistical learning : data mining, inference,and prediction'},\n",
       " {'abstract': 'This book is an updated version of the information theory classic, first published in 1990. About one-third of the book is devoted to Shannon source and channel coding theorems; the remainder addresses sources, channels, and codes and on information and distortion measures and their properties. New in this edition:Expanded treatment of stationary or sliding-block codes and their relations to traditional block codesExpanded discussion of results from ergodic theory relevant to information theoryExpanded treatment of B-processes -- processes formed by stationary coding memoryless sourcesNew material on trading off information and distortion, including the Marton inequalityNew material on the properties of optimal and asymptotically optimal source codesNew material on the relationships of source coding and rate-constrained simulation or modeling of random processesSignificant material not covered in other information theory texts includes stationary/sliding-block codes, a geometric view of information theory provided by process distance measures, and general Shannon coding theorems for asymptotic mean stationary sources, which may be neither ergodic nor stationary, and d-bar continuous channels.',\n",
       "  'authors': ['Robert M. Gray'],\n",
       "  'date': '1990',\n",
       "  'identifier': '1496462336',\n",
       "  'references': ['1494099594',\n",
       "   '1506281249',\n",
       "   '1515040589',\n",
       "   '2610881169',\n",
       "   '2112943043',\n",
       "   '2024932032',\n",
       "   '2154716422',\n",
       "   '2336317531',\n",
       "   '2096030967',\n",
       "   '2020347709'],\n",
       "  'title': 'Entropy and Information Theory'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Sotiris B. Kotsiantis'],\n",
       "  'date': '2007',\n",
       "  'identifier': '1873332500',\n",
       "  'references': ['2156909104',\n",
       "   '1570448133',\n",
       "   '2139212933',\n",
       "   '2912934387',\n",
       "   '1563088657',\n",
       "   '2119479037',\n",
       "   '1992419399',\n",
       "   '2125055259',\n",
       "   '2154642048',\n",
       "   '1515851193'],\n",
       "  'title': 'Supervised Machine Learning: A Review of Classification Techniques'},\n",
       " {'abstract': 'TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.',\n",
       "  'authors': ['Martín Abadi ',\n",
       "   ' Ashish Agarwal ',\n",
       "   ' Paul Barham ',\n",
       "   ' Eugene Brevdo ',\n",
       "   ' Zhifeng Chen ',\n",
       "   ' Craig Citro ',\n",
       "   ' Gregory S. Corrado ',\n",
       "   ' Andy Davis ',\n",
       "   ' Jeffrey Dean ',\n",
       "   ' Matthieu Devin ',\n",
       "   ' Sanjay Ghemawat ',\n",
       "   ' Ian J. Goodfellow ',\n",
       "   ' Andrew Harp ',\n",
       "   ' Geoffrey Irving ',\n",
       "   ' Michael Isard ',\n",
       "   ' Yangqing Jia ',\n",
       "   ' Rafal Józefowicz ',\n",
       "   ' Lukasz Kaiser ',\n",
       "   ' Manjunath Kudlur ',\n",
       "   ' Josh Levenberg ',\n",
       "   ' Dan Mané ',\n",
       "   ' Rajat Monga ',\n",
       "   ' Sherry Moore ',\n",
       "   ' Derek Gordon Murray ',\n",
       "   ' Chris Olah ',\n",
       "   ' Mike Schuster ',\n",
       "   ' Jonathon Shlens ',\n",
       "   ' Benoit Steiner ',\n",
       "   ' Ilya Sutskever ',\n",
       "   ' Kunal Talwar ',\n",
       "   ' Paul A. Tucker ',\n",
       "   ' Vincent Vanhoucke ',\n",
       "   ' Vijay Vasudevan ',\n",
       "   ' Fernanda B. Viégas ',\n",
       "   ' Oriol Vinyals ',\n",
       "   ' Pete Warden ',\n",
       "   ' Martin Wattenberg ',\n",
       "   ' Martin Wicke ',\n",
       "   ' Yuan Yu ',\n",
       "   ' Xiaoqiang Zheng'],\n",
       "  'date': '2015',\n",
       "  'identifier': '2271840356',\n",
       "  'references': ['2097117768',\n",
       "   '1836465849',\n",
       "   '1614298861',\n",
       "   '2130942839',\n",
       "   '2155893237',\n",
       "   '2064675550',\n",
       "   '2160815625',\n",
       "   '2168231600',\n",
       "   '2016053056',\n",
       "   '2131975293'],\n",
       "  'title': 'TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems'},\n",
       " {'abstract': 'The quantitative evaluation of optical flow algorithms by Barron et al. (1994) led to significant advances in performance. The challenges for optical flow algorithms today go beyond the datasets and evaluation methods proposed in that paper. Instead, they center on problems associated with complex natural scenes, including nonrigid motion, real sensor noise, and motion discontinuities. We propose a new set of benchmarks and evaluation methods for the next generation of optical flow algorithms. To that end, we contribute four types of data to test different aspects of optical flow algorithms: (1) sequences with nonrigid motion where the ground-truth flow is determined by tracking hidden fluorescent texture, (2) realistic synthetic sequences, (3) high frame-rate video used to study interpolation error, and (4) modified stereo sequences of static scenes. In addition to the average angular error used by Barron et al., we compute the absolute flow endpoint error, measures for frame interpolation error, improved statistics, and results at motion discontinuities and in textureless regions. In October 2007, we published the performance of several well-known methods on a preliminary version of our data to establish the current state of the art. We also made the data freely available on the web at http://vision.middlebury.edu/flow/ . Subsequently a number of researchers have uploaded their results to our website and published papers using the data. A significant improvement in performance has already been achieved. In this paper we analyze the results obtained to date and draw a large number of conclusions from them.',\n",
       "  'authors': ['Simon Baker 1',\n",
       "   ' Daniel Scharstein 2',\n",
       "   ' J. P. Lewis 3',\n",
       "   ' Stefan Roth 4',\n",
       "   ' Michael J. Black 5',\n",
       "   ' Richard Szeliski 1'],\n",
       "  'date': '2011',\n",
       "  'identifier': '2147253850',\n",
       "  'references': ['2104974755',\n",
       "   '2143516773',\n",
       "   '2035379092',\n",
       "   '2147253850',\n",
       "   '2137659841',\n",
       "   '2115733720',\n",
       "   '3111480503',\n",
       "   '2119781527',\n",
       "   '2160014001',\n",
       "   '3003662786'],\n",
       "  'title': 'A Database and Evaluation Methodology for Optical Flow'},\n",
       " {'abstract': 'This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.',\n",
       "  'authors': ['Xiang Zhang ', ' Junbo Zhao ', ' Yann LeCun'],\n",
       "  'date': '2015',\n",
       "  'identifier': '2963012544',\n",
       "  'references': ['2153579005',\n",
       "   '2310919327',\n",
       "   '1832693441',\n",
       "   '1904365287',\n",
       "   '2158899491',\n",
       "   '2064675550',\n",
       "   '1665214252',\n",
       "   '104184427',\n",
       "   '2149684865',\n",
       "   '1689711448'],\n",
       "  'title': 'Character-level convolutional networks for text classification'},\n",
       " {'abstract': '',\n",
       "  'authors': ['John H. Holland'],\n",
       "  'date': '1992',\n",
       "  'identifier': '1497256448',\n",
       "  'references': ['2152195021',\n",
       "   '2076063813',\n",
       "   '3102641634',\n",
       "   '1573676079',\n",
       "   '1992419399',\n",
       "   '2134967712',\n",
       "   '2107941094',\n",
       "   '2109364787'],\n",
       "  'title': 'Adaptation in natural and artificial systems'},\n",
       " {'abstract': '',\n",
       "  'authors': ['P. J. Burt'],\n",
       "  'date': '1981',\n",
       "  'identifier': '2978983090',\n",
       "  'references': ['2132984323',\n",
       "   '2109200236',\n",
       "   '2103504761',\n",
       "   '2140257560',\n",
       "   '1480546554',\n",
       "   '1490632837',\n",
       "   '2127230474',\n",
       "   '2042243448',\n",
       "   '2143753158',\n",
       "   '2156053375'],\n",
       "  'title': 'Fast Filter Transforms for Image Processing'},\n",
       " {'abstract': \"Despite a long-term decline in the circus industry, Cirque du Soleil profitably increased revenue 22-fold over the last ten years by reinventing the circus. Rather than competing within the confines of the existing industry or trying to steal customers from rivals, Cirque developed uncontested market space that made the competition irrelevant. Cirque created what the authors call a blue ocean, a previously unknown market space. In blue oceans, demand is created rather than fought over. There is ample opportunity for growth that is both profitable and rapid. In red oceans--that is, in all the industries already existing--companies compete by grabbing for a greater share of limited demand. As the market space gets more crowded, prospects for profits and growth decline. Products turn into commodities, and increasing competition turns the water bloody. There are two ways to create blue oceans. One is to launch completely new industries, as eBay did with online auctions. But it's much more common for a blue ocean to be created from within a red ocean when a company expands the boundaries of an existing industry. In studying more than 150 blue ocean creations in over 30 industries, the authors observed that the traditional units of strategic analysis--company and industry--are of limited use in explaining how and why blue oceans are created. The most appropriate unit of analysis is the strategic move, the set of managerial actions and decisions involved in making a major market-creating business offering. Creating blue oceans builds brands. So powerful is blue ocean strategy, in fact, that a blue ocean strategic move can create brand equity that lasts for decades.\",\n",
       "  'authors': ['W Chan Kim ', ' Renée Mauborgne'],\n",
       "  'date': '2004',\n",
       "  'identifier': '2402811117',\n",
       "  'references': ['1561054077',\n",
       "   '2896849024',\n",
       "   '2113476348',\n",
       "   '2070504166',\n",
       "   '2128753220',\n",
       "   '2066349454',\n",
       "   '2090963801',\n",
       "   '2143194834',\n",
       "   '2002757881'],\n",
       "  'title': 'Blue Ocean Strategy'},\n",
       " {'abstract': 'Python is an excellent \"steering\" language for scientific codes written in other languages. However, with additional basic tools, Python transforms into a high-level language suited for scientific and engineering code that\\'s often fast enough to be immediately useful but also flexible enough to be sped up with additional extensions.',\n",
       "  'authors': ['T.E. Oliphant'],\n",
       "  'date': '2007',\n",
       "  'identifier': '2110114082',\n",
       "  'references': ['2912827267'],\n",
       "  'title': 'Python for Scientific Computing'},\n",
       " {'abstract': 'The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length.',\n",
       "  'authors': ['Ronald J. Williams 1', ' David Zipser 2'],\n",
       "  'date': '1989',\n",
       "  'identifier': '2016589492',\n",
       "  'references': ['2154642048',\n",
       "   '2293063825',\n",
       "   '2110485445',\n",
       "   '2143503258',\n",
       "   '1959983357',\n",
       "   '1881179843',\n",
       "   '1984205520',\n",
       "   '1984375561',\n",
       "   '2119796132',\n",
       "   '1527772862'],\n",
       "  'title': 'A learning algorithm for continually running fully recurrent neural networks'},\n",
       " {'abstract': 'Abstract: Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.',\n",
       "  'authors': ['Andrew M. Saxe ', ' James L. McClelland ', ' Surya Ganguli'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2963504252',\n",
       "  'references': ['2618530766',\n",
       "   '2100495367',\n",
       "   '2072128103',\n",
       "   '1533861849',\n",
       "   '2117130368',\n",
       "   '104184427',\n",
       "   '2110798204',\n",
       "   '1993882792',\n",
       "   '2141125852',\n",
       "   '1815076433'],\n",
       "  'title': 'Exact solutions to the nonlinear dynamics of learning in deep linear neural networks'},\n",
       " {'abstract': 'A new heuristic approach for minimizing possibly nonlinear and non-differentiable continuous space functions is presented. By means of an extensive testbed it is demonstrated that the new method converges faster and with more certainty than many other acclaimed global optimization methods. The new method requires few control variables, is robust, easy to use, and lends itself very well to parallel computation.',\n",
       "  'authors': ['Rainer Storn 1', ' Kenneth Price 2'],\n",
       "  'date': '1997',\n",
       "  'identifier': '1595159159',\n",
       "  'references': ['1639032689',\n",
       "   '2170120409',\n",
       "   '2152150600',\n",
       "   '2144636407',\n",
       "   '1626834557',\n",
       "   '2089419496',\n",
       "   '3011460294',\n",
       "   '2027945080',\n",
       "   '2127130980',\n",
       "   '2040049280'],\n",
       "  'title': 'Differential Evolution – A Simple and Efficient Heuristic for Global Optimization over Continuous Spaces'},\n",
       " {'abstract': 'A formula for the capacity of arbitrary single-user channels without feedback (not necessarily information stable, stationary, etc.) is proved. Capacity is shown to equal the supremum, over all input processes, of the input-output inf-information rate defined as the liminf in probability of the normalized information density. The key to this result is a new converse approach based on a simple new lower bound on the error probability of m-ary hypothesis tests among equiprobable hypotheses. A necessary and sufficient condition for the validity of the strong converse is given, as well as general expressions for /spl epsiv/-capacity. >',\n",
       "  'authors': ['S. Verdu 1', ' Te Sun Han 2'],\n",
       "  'date': '1994',\n",
       "  'identifier': '2020347709',\n",
       "  'references': ['2099111195',\n",
       "   '1496462336',\n",
       "   '1995875735',\n",
       "   '2162635854',\n",
       "   '2296441616',\n",
       "   '1535258871',\n",
       "   '2117048406',\n",
       "   '2157552746',\n",
       "   '2135294057',\n",
       "   '2139657234'],\n",
       "  'title': 'A general formula for channel capacity'},\n",
       " {'abstract': 'A number of recent studies have focused on the statistical properties of networked systems such as social networks and the Worldwide Web. Researchers have concentrated particularly on a few properties that seem to be common to many networks: the small-world property, power-law degree distributions, and network transitivity. In this article, we highlight another property that is found in many networks, the property of community structure, in which network nodes are joined together in tightly knit groups, between which there are only looser connections. We propose a method for detecting such communities, built around the idea of using centrality indices to find community boundaries. We test our method on computer-generated and real-world graphs whose community structure is already known and find that the method detects this known structure with high sensitivity and reliability. We also apply the method to two networks whose community structure is not well known—a collaboration network and a food web—and find that it detects significant and informative community divisions in both cases.',\n",
       "  'authors': ['Michelle Girvan ', ' M. E. J. Newman'],\n",
       "  'date': '2002',\n",
       "  'identifier': '1971421925',\n",
       "  'references': ['2112090702',\n",
       "   '2008620264',\n",
       "   '2164727176',\n",
       "   '1976969221',\n",
       "   '2769133055',\n",
       "   '1977545325',\n",
       "   '2125315567',\n",
       "   '2175110005',\n",
       "   '2144885342',\n",
       "   '2169015768'],\n",
       "  'title': 'Community structure in social and biological networks'},\n",
       " {'abstract': \"We prove that the length of the shortest closed path through n points in a bounded plane region of area v is ‘almost always’ asymptotically proportional to √( nv ) for large n ; and we extend this result to bounded Lebesgue sets in k –dimensional Euclidean space. The constants of proportionality depend only upon the dimensionality of the space, and are independent of the shape of the region. We give numerical bounds for these constants for various values of k ; and we estimate the constant in the particular case k = 2. The results are relevant to the travelling-salesman problem, Steiner's street network problem, and the Loberman—Weinberger wiring problem. They have possible generalizations in the direction of Plateau's problem and Douglas' problem.\",\n",
       "  'authors': ['Jillian Beardwood 1', ' J. H. Halton 2', ' J. M. Hammersley 1'],\n",
       "  'date': '1959',\n",
       "  'identifier': '2014068360',\n",
       "  'references': ['1965680834',\n",
       "   '1489991626',\n",
       "   '2058937865',\n",
       "   '1968778433',\n",
       "   '1994863634',\n",
       "   '2064791171',\n",
       "   '2797039129',\n",
       "   '1975224834',\n",
       "   '2564555264',\n",
       "   '2904374521'],\n",
       "  'title': 'The shortest path through many points'},\n",
       " {'abstract': 'Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.',\n",
       "  'authors': ['Peter D. Turney 1', ' Patrick Pantel 2'],\n",
       "  'date': '2010',\n",
       "  'identifier': '1662133657',\n",
       "  'references': ['2173213060',\n",
       "   '1880262756',\n",
       "   '3013264884',\n",
       "   '1532325895',\n",
       "   '2038721957',\n",
       "   '1660390307',\n",
       "   '2024165284',\n",
       "   '2117130368',\n",
       "   '2166706824',\n",
       "   '1992419399'],\n",
       "  'title': 'From frequency to meaning: vector space models of semantics'},\n",
       " {'abstract': 'Many computer vision problems (e.g., camera calibration, image alignment, structure from motion) are solved through a nonlinear optimization method. It is generally accepted that 2nd order descent methods are the most robust, fast and reliable approaches for nonlinear optimization of a general smooth function. However, in the context of computer vision, 2nd order descent methods have two main drawbacks: (1) The function might not be analytically differentiable and numerical approximations are impractical. (2) The Hessian might be large and not positive definite. To address these issues, this paper proposes a Supervised Descent Method (SDM) for minimizing a Non-linear Least Squares (NLS) function. During training, the SDM learns a sequence of descent directions that minimizes the mean of NLS functions sampled at different points. In testing, SDM minimizes the NLS objective using the learned descent directions without computing the Jacobian nor the Hessian. We illustrate the benefits of our approach in synthetic and real examples, and show how SDM achieves state-of-the-art performance in the problem of facial feature detection. The code is available at www.humansensing.cs. cmu.edu/intraface.',\n",
       "  'authors': ['Xuehan Xiong ', ' Fernando De la Torre'],\n",
       "  'date': '2013',\n",
       "  'identifier': '2157285372',\n",
       "  'references': ['2151103935',\n",
       "   '1678356000',\n",
       "   '2047508432',\n",
       "   '3111950349',\n",
       "   '2035379092',\n",
       "   '1990937109',\n",
       "   '2237250383',\n",
       "   '1963599662',\n",
       "   '2082308025',\n",
       "   '2912990735'],\n",
       "  'title': 'Supervised Descent Method and Its Applications to Face Alignment'},\n",
       " {'abstract': 'Many scientific phenomena are now investigated by complex computer models or codes. Given the input values, the code produces one or more outputs via a complex mathematical model. Often the code is expensive to run, and it may be necessary to build a computationally cheaper predictor to enable, for example, optimization of the inputs. If there are many input factors, an initial step in building a predictor is identifying (screening) the active factors. We model the output of the computer code as the realization of a stochastic process. This model has a number of advantages. First, it provides a statistical basis, via the likelihood, for a stepwise algorithm to determine the important factors. Second, it is very flexible, allowing nonlinear and interaction effects to emerge without explicitly modeling such effects. Third, the same data are used for screening and building the predictor, so expensive runs are efficiently used. We illustrate the methodology with two examples, both having 20 input variables. I...',\n",
       "  'authors': ['William J. Welch 1',\n",
       "   ' Robert J. Buck 2',\n",
       "   ' Jerome Sacks 3',\n",
       "   ' Henry P. Wynn 2',\n",
       "   ' Toby J. Mitchell 4',\n",
       "   ' Max D. Morris 4'],\n",
       "  'date': '1992',\n",
       "  'identifier': '2098043077',\n",
       "  'references': ['2341283081',\n",
       "   '2018044188',\n",
       "   '2038669746',\n",
       "   '2171074980',\n",
       "   '2083027289',\n",
       "   '2044458183',\n",
       "   '2055591121',\n",
       "   '1964105999',\n",
       "   '2006901874',\n",
       "   '1999557823'],\n",
       "  'title': 'Screening, predicting, and computer experiments'},\n",
       " {'abstract': 'We seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.',\n",
       "  'authors': ['Bryan C. Russell 1',\n",
       "   ' Antonio Torralba 1',\n",
       "   ' Kevin P. Murphy 2',\n",
       "   ' William T. Freeman 1'],\n",
       "  'date': '2008',\n",
       "  'identifier': '2110764733',\n",
       "  'references': ['2156909104',\n",
       "   '2164598857',\n",
       "   '2038721957',\n",
       "   '2138451337',\n",
       "   '2154422044',\n",
       "   '2107034620',\n",
       "   '1566135517',\n",
       "   '2166049352',\n",
       "   '2156598602',\n",
       "   '2134557905'],\n",
       "  'title': 'LabelMe: A Database and Web-Based Tool for Image Annotation'},\n",
       " {'abstract': 'Humans have an amazing ability to instantly grasp the overall 3D structure of a scene--ground orientation, relative positions of major landmarks, etc.--even from a single image. This ability is completely missing in most popular recognition algorithms, which pretend that the world is flat and/or view it through a patch-sized peephole. Yet it seems very likely that having a grasp of this \"surface layout\" of a scene should be of great assistance for many tasks, including recognition, navigation, and novel view synthesis. In this paper, we take the first step towards constructing the surface layout, a labeling of the image intogeometric classes. Our main insight is to learn appearance-based models of these geometric classes, which coarsely describe the 3D scene orientation of each image region. Our multiple segmentation framework provides robust spatial support, allowing a wide variety of cues (e.g., color, texture, and perspective) to contribute to the confidence in each geometric label. In experiments on a large set of outdoor images, we evaluate the impact of the individual cues and design choices in our algorithm. We further demonstrate the applicability of our method to indoor images, describe potential applications, and discuss extensions to a more complete notion of surface layout.',\n",
       "  'authors': ['Derek Hoiem ', ' Alexei A. Efros ', ' Martial Hebert'],\n",
       "  'date': '2007',\n",
       "  'identifier': '2125310925',\n",
       "  'references': ['2033819227',\n",
       "   '2147880316',\n",
       "   '2121947440',\n",
       "   '1999478155',\n",
       "   '2143516773',\n",
       "   '1566135517',\n",
       "   '2024046085',\n",
       "   '2209124607',\n",
       "   '1484228140',\n",
       "   '2032210760'],\n",
       "  'title': 'Recovering Surface Layout from an Image'},\n",
       " {'abstract': \"The paper gives a snapshot of the state of the art in affine covariant region detectors, and compares their performance on a set of test images under varying imaging conditions. Six types of detectors are included: detectors based on affine normalization around Harris (Mikolajczyk and Schmid, 2002; Schaffalitzky and Zisserman, 2002) and Hessian points (Mikolajczyk and Schmid, 2002), a detector of `maximally stable extremal regions', proposed by Matas et al. (2002); an edge-based region detector (Tuytelaars and Van Gool, 1999) and a detector based on intensity extrema (Tuytelaars and Van Gool, 2000), and a detector of `salient regions', proposed by Kadir, Zisserman and Brady (2004). The performance is measured against changes in viewpoint, scale, illumination, defocus and image compression. The objective of this paper is also to establish a reference test set of images and performance software, so that future detectors can be evaluated in the same framework.\",\n",
       "  'authors': ['K. Mikolajczyk 1',\n",
       "   ' T. Tuytelaars 2',\n",
       "   ' C. Schmid 3',\n",
       "   ' A. Zisserman 1',\n",
       "   ' J. Matas 4',\n",
       "   ' F. Schaffalitzky 1',\n",
       "   ' T. Kadir 1',\n",
       "   ' L. Van Gool 2'],\n",
       "  'date': '2005',\n",
       "  'identifier': '1980911747',\n",
       "  'references': ['2151103935',\n",
       "   '2033819227',\n",
       "   '2124386111',\n",
       "   '2177274842',\n",
       "   '2131846894',\n",
       "   '2154422044',\n",
       "   '2145023731',\n",
       "   '1625255723',\n",
       "   '2172188317',\n",
       "   '2124404372'],\n",
       "  'title': 'A Comparison of Affine Region Detectors'},\n",
       " {'abstract': \"Much organizational identity research has grappled with the question of identity emergence or change. Yet the question of identity endurance is equally puzzling. Relying primarily on an analysis of 309 internal bulletins produced at a French aeronautics firm over almost 50 years, we theorize a link between collective memory and organizational identity endurance. More specifically, we show how forgetting in a firm's ongoing rhetorical history — here, the bulletins' repeated omission of contradictory elements in the firm's past (i.e., structural omission) or attempts to neutralize them with valued identity cues (i.e., preemptive neutralization) — sustains its identity. Thus, knowing “who we are” might depend in part on repeatedly remembering to forget “who we were not.”\",\n",
       "  'authors': ['Michel J. Anteby 1', ' Virag Molnar 2'],\n",
       "  'date': '2012',\n",
       "  'identifier': '2025419023',\n",
       "  'references': ['2121393013',\n",
       "   '1972949097',\n",
       "   '2085081981',\n",
       "   '2106919063',\n",
       "   '2115497642',\n",
       "   '2015486462',\n",
       "   '2153981305',\n",
       "   '1567663560',\n",
       "   '2140608320',\n",
       "   '2122281343'],\n",
       "  'title': \"Collective Memory Meets Organizational Identity: Remembering to Forget in a Firm's Rhetorical History\"},\n",
       " {'abstract': 'Information filtering agents and collaborative filtering both attempt to alleviate information overload by identifying which items a user will find worthwhile. Information filtering (IF) focuses on the analysis of item content and the development of a personal user interest profile. Collaborative filtering (CF) focuses on identification of other users with similar tastes and the use of their opinions to recommend items. Each technique has advantages and limitations that suggest that the two could be beneficially combined.This paper shows that a CF framework can be used to combine personal IF agents and the opinions of a community of users to produce better recommendations than either agents or users can produce alone. It also shows that using CF to create a personal combination of a set of agents produces better results than either individual agents or other combination mechanisms. One key implication of these results is that users can avoid having to select among agents; they can use them all and let the CF framework select the best ones for them.',\n",
       "  'authors': ['Nathaniel Good 1',\n",
       "   ' J. Ben Schafer 1',\n",
       "   ' Joseph A. Konstan 2',\n",
       "   ' Al Borchers 1',\n",
       "   ' Badrul Sarwar 1',\n",
       "   ' Jon Herlocker 1',\n",
       "   ' John Riedl 1'],\n",
       "  'date': '1999',\n",
       "  'identifier': '1532852017',\n",
       "  'references': ['2110325612',\n",
       "   '2155106456',\n",
       "   '2124591829',\n",
       "   '1966553486',\n",
       "   '1999047234',\n",
       "   '2043403353',\n",
       "   '1978394996',\n",
       "   '175500210',\n",
       "   '1670263352',\n",
       "   '2030144199'],\n",
       "  'title': 'Combining collaborative filtering with personal agents for better recommendations'},\n",
       " {'abstract': 'A general-purpose computer vision system must be capable of recognizing three-dimensional (3-D) objects. This paper proposes a precise definition of the 3-D object recognition problem, discusses basic concepts associated with this problem, and reviews the relevant literature. Because range images (or depth maps) are often used as sensor input instead of intensity images, techniques for obtaining, processing, and characterizing range data are also surveyed.',\n",
       "  'authors': ['Paul J. Besl ', ' Ramesh C. Jain'],\n",
       "  'date': '1985',\n",
       "  'identifier': '1996773532',\n",
       "  'references': ['2740373864',\n",
       "   '1622620102',\n",
       "   '2083632529',\n",
       "   '2164934677',\n",
       "   '2113511941',\n",
       "   '2100185791',\n",
       "   '1587467920',\n",
       "   '2130755868',\n",
       "   '2163775665',\n",
       "   '1975470983'],\n",
       "  'title': 'Three-dimensional object recognition'},\n",
       " {'abstract': 'Deep learning based on artificial neural networks is a very popular approach to modeling, classifying, and recognizing complex data such as images, speech, and text. The unprecedented accuracy of deep learning methods has turned them into the foundation of new AI-based services on the Internet. Commercial companies that collect user data on a large scale have been the main beneficiaries of this trend since the success of deep learning techniques is directly proportional to the amount of data available for training.',\n",
       "  'authors': ['Reza Shokri 1', ' Vitaly Shmatikov 2'],\n",
       "  'date': '2015',\n",
       "  'identifier': '2337093093',\n",
       "  'references': ['2337093093'],\n",
       "  'title': 'Privacy-preserving deep learning'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Donald O. Hebb'],\n",
       "  'date': '1988',\n",
       "  'identifier': '22297218',\n",
       "  'references': ['2076063813',\n",
       "   '2042264548',\n",
       "   '2166667242',\n",
       "   '2885825670',\n",
       "   '2148138104',\n",
       "   '2066785847',\n",
       "   '2763148304',\n",
       "   '1928882148',\n",
       "   '2147101007',\n",
       "   '2016954751'],\n",
       "  'title': 'The organization of behavior'},\n",
       " {'abstract': 'Given a channel and an input process we study the minimum randomness of those input processes whose output statistics approximate the original output statistics with arbitrary accuracy. We introduce the notion of resolvability of a channel, defined as the number of random bits required per channel use in order to generate an input that achieves arbitrarily accurate approximation of the output statistics for any given input process. We obtain a general formula for resolvability which holds regardless of the channel memory structure. We show that, for most channels, resolvability is equal to Shannon capacity. By-products of our analysis are a general formula for the minimum achievable (fixed-length) source coding rate of any finite-alphabet source, and a strong converse of the identification coding theorem, which holds for any channel that satisfies the strong converse of the channel coding theorem.',\n",
       "  'authors': ['Te Sun Han 1', ' S. Verdu 2'],\n",
       "  'date': '1993',\n",
       "  'identifier': '2162635854',\n",
       "  'references': ['2099111195',\n",
       "   '1549664537',\n",
       "   '2065024926',\n",
       "   '2122900782',\n",
       "   '2152845298',\n",
       "   '1992954419',\n",
       "   '2141432745',\n",
       "   '2149869936',\n",
       "   '2091891040',\n",
       "   '2014259536'],\n",
       "  'title': 'Approximation theory of output statistics'},\n",
       " {'abstract': 'Handwriting has continued to persist as a means of communication and recording information in day-to-day life even with the introduction of new technologies. Given its ubiquity in human transactions, machine recognition of handwriting has practical significance, as in reading handwritten notes in a PDA, in postal addresses on envelopes, in amounts in bank checks, in handwritten fields in forms, etc. This overview describes the nature of handwritten language, how it is transduced into electronic data, and the basic concepts behind written language recognition algorithms. Both the online case (which pertains to the availability of trajectory data during writing) and the off-line case (which pertains to scanned images) are considered. Algorithms for preprocessing, character and word recognition, and performance with practical systems are indicated. Other fields of application, like signature verification, writer authentification, handwriting learning tools are also considered.',\n",
       "  'authors': ['R. Plamondon 1', ' S.N. Srihari 2'],\n",
       "  'date': '2000',\n",
       "  'identifier': '2142069714',\n",
       "  'references': ['1554663460',\n",
       "   '2125838338',\n",
       "   '2046079134',\n",
       "   '2133059825',\n",
       "   '2102734279',\n",
       "   '1970800786',\n",
       "   '1549285799',\n",
       "   '2010595692',\n",
       "   '1885639605',\n",
       "   '2178432768'],\n",
       "  'title': 'Online and off-line handwriting recognition: a comprehensive survey'},\n",
       " {'abstract': 'We develop bounds for the second largest eigenvalue and spectral gap of a reversible Markov chain. The bounds depend on geometric quantities such as the maximum degree, diameter and covering number of associated graphs. The bounds compare well with exact answers for a variety of simple chains and seem better than bounds derived tbrough Cheeger-like inequalities. They offer improved rates of convergence for the random walk associated to approximate computation of the permanent.',\n",
       "  'authors': ['Persi Diaconis ', ' Daniel Stroock'],\n",
       "  'date': '1991',\n",
       "  'identifier': '2043694962',\n",
       "  'references': ['2610857016',\n",
       "   '2027808858',\n",
       "   '2151417892',\n",
       "   '1596709027',\n",
       "   '1631603072',\n",
       "   '2072211488',\n",
       "   '2106285343',\n",
       "   '1515707356',\n",
       "   '2006912660',\n",
       "   '1993111701'],\n",
       "  'title': 'Geometric Bounds for Eigenvalues of Markov Chains'},\n",
       " {'abstract': 'Several variants of the long short-term memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful functional ANalysis Of VAriance framework. In total, we summarize the results of 5400 experimental runs ( $\\\\approx 15$ years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.',\n",
       "  'authors': ['Klaus Greff ',\n",
       "   ' Rupesh K. Srivastava ',\n",
       "   ' Jan Koutnik ',\n",
       "   ' Bas R. Steunebrink ',\n",
       "   ' Jurgen Schmidhuber'],\n",
       "  'date': '2017',\n",
       "  'identifier': '1689711448',\n",
       "  'references': ['2157331557',\n",
       "   '2064675550',\n",
       "   '2143612262',\n",
       "   '1947481528',\n",
       "   '1924770834',\n",
       "   '104184427',\n",
       "   '2131241448',\n",
       "   '1810943226',\n",
       "   '2097998348',\n",
       "   '2293634267'],\n",
       "  'title': 'LSTM: A Search Space Odyssey'},\n",
       " {'abstract': 'Abstract Background The eight-item Patient Health Questionnaire depression scale (PHQ-8) is established as a valid diagnostic and severity measure for depressive disorders in large clinical studies. Our objectives were to assess the PHQ-8 as a depression measure in a large, epidemiological population-based study, and to determine the comparability of depression as defined by the PHQ-8 diagnostic algorithm vs. a PHQ-8 cutpoint ≥ 10. Methods Random-digit-dialed telephone survey of 198,678 participants in the 2006 Behavioral Risk Factor Surveillance Survey (BRFSS), a population-based survey in the United States. Current depression as defined by either the DSM-IV based diagnostic algorithm (i.e., major depressive or other depressive disorder) of the PHQ-8 or a PHQ-8 score ≥ 10; respondent sociodemographic characteristics; number of days of impairment in the past 30 days in multiple domains of health-related quality of life (HRQoL). Results The prevalence of current depression was similar whether defined by the diagnostic algorithm or a PHQ-8 score ≥ 10 (9.1% vs. 8.6%). Depressed patients had substantially more days of impairment across multiple domains of HRQoL, and the impairment was nearly identical in depressed groups defined by either method. Of the 17,040 respondents with a PHQ-8 score ≥ 10, major depressive disorder was present in 49.7%, other depressive disorder in 23.9%, depressed mood or anhedonia in another 22.8%, and no evidence of depressive disorder or depressive symptoms in only 3.5%. Limitations The PHQ-8 diagnostic algorithm rather than an independent structured psychiatric interview was used as the criterion standard. Conclusions The PHQ-8 is a useful depression measure for population-based studies, and either its diagnostic algorithm or a cutpoint ≥ 10 can be used for defining current depression.',\n",
       "  'authors': ['Kurt Kroenke 1',\n",
       "   ' Tara W. Strine 2',\n",
       "   ' Robert L. Spitzer 3',\n",
       "   ' Janet B.W. Williams 3',\n",
       "   ' Joyce T. Berry 4',\n",
       "   ' Ali H. Mokdad 2'],\n",
       "  'date': '2009',\n",
       "  'identifier': '2048533792',\n",
       "  'references': ['2159011576',\n",
       "   '2132322340',\n",
       "   '2159061031',\n",
       "   '2137296158',\n",
       "   '1528748012',\n",
       "   '1985329916',\n",
       "   '2105346851',\n",
       "   '2136115410',\n",
       "   '2029421651',\n",
       "   '2963108147'],\n",
       "  'title': 'The PHQ-8 as a measure of current depression in the general population'},\n",
       " {'abstract': 'The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.',\n",
       "  'authors': ['Y. Bengio ', ' A. Courville ', ' P. Vincent'],\n",
       "  'date': '2013',\n",
       "  'identifier': '2163922914',\n",
       "  'references': ['2618530766',\n",
       "   '2136922672',\n",
       "   '3118608800',\n",
       "   '2100495367',\n",
       "   '2310919327',\n",
       "   '2158899491',\n",
       "   '2162915993',\n",
       "   '2187089797',\n",
       "   '1665214252',\n",
       "   '2160815625'],\n",
       "  'title': 'Representation Learning: A Review and New Perspectives'},\n",
       " {'abstract': 'The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible.',\n",
       "  'authors': ['Radford M. Neal ', ' Geoffrey E. Hinton'],\n",
       "  'date': '1998',\n",
       "  'identifier': '2567948266',\n",
       "  'references': ['2049633694',\n",
       "   '2117853077',\n",
       "   '2024476015',\n",
       "   '1580495158',\n",
       "   '1991278573',\n",
       "   '581152777'],\n",
       "  'title': 'A view of the EM algorithm that justifies incremental, sparse, and other variants'},\n",
       " {'abstract': 'This work focuses on algorithms which learn from examples to perform multiclass text and speech categorization tasks. Our approach is based on a new and improved family of boosting algorithms. We describe in detail an implementation, called BoosTexter, of the new boosting algorithms for text categorization tasks. We present results comparing the performance of BoosTexter and a number of other text-categorization algorithms on a variety of tasks. We conclude by describing the application of our system to automatic call-type identification from unconstrained spoken customer responses.',\n",
       "  'authors': ['Robert E. Schapire 1', ' Yoram Singer 2'],\n",
       "  'date': '2000',\n",
       "  'identifier': '2053463056',\n",
       "  'references': ['1988790447',\n",
       "   '2112076978',\n",
       "   '1975846642',\n",
       "   '1956559956',\n",
       "   '2114535528',\n",
       "   '1670263352',\n",
       "   '2096152098',\n",
       "   '2032210760',\n",
       "   '1966280301',\n",
       "   '2067885219'],\n",
       "  'title': 'BoosTexter: A Boosting-based Systemfor Text Categorization'},\n",
       " {'abstract': 'This paper proposes a class of surrogate constraint heuristics for obtaining approximate, near optimal solutions to integer programming problems. These heuristics are based on a simple framework that illuminates the character of several earlier heuristic proposals and provides a variety of new alternatives. The paper also proposes additional heuristics that can be used either to supplement the surrogate constraint procedures or to provide independent solution strategies. Preliminary computational results are reported for applying one of these alternatives to a class of nonlinear generalized set covering problems involving approximately 100 constraints and 300–500 integer variables. The solutions obtained by the tested procedure had objective function values twice as good as values obtained by standard approaches (e.g., reducing the best objective function values of other methods from 85 to 40 on the average. Total solution time for the tested procedure ranged from ten to twenty seconds on the CDC 6600.',\n",
       "  'authors': ['Fred Glover'],\n",
       "  'date': '1977',\n",
       "  'identifier': '2164855953',\n",
       "  'references': ['2076756604',\n",
       "   '2015247562',\n",
       "   '2106433410',\n",
       "   '2022111106',\n",
       "   '2169849417',\n",
       "   '2134857787',\n",
       "   '1982719520',\n",
       "   '211248821',\n",
       "   '2540171881'],\n",
       "  'title': 'HEURISTICS FOR INTEGER PROGRAMMING USING SURROGATE CONSTRAINTS'},\n",
       " {'abstract': 'In this paper we consider the problem of recognizing solid objects from a single two-dimensional image of a three-dimensional scene. We develop a new method for computing a transformation from a three-dimensional model coordinate frame to the two-dimensional image coordinate frame, using three pairs of model and image points. We show that this transformation always exists for three noncollinear points, and is unique up to a reflective ambiguity. The solution method is closed-form and only involves second-order equations. We have implemented a recognition system that uses this transformation method to determine possible alignments of a model with an image. Each of these hypothesized matches is verified by comparing the entire edge contours of the aligned object with the image edges. Using the entire edge contours for verification, rather than a few local feature points, reduces the chance of finding false matches. The system has been tested on partly occluded objects in highly cluttered scenes.',\n",
       "  'authors': ['Daniel P. Huttenlocher 1', ' Shimon Ullman 2'],\n",
       "  'date': '1990',\n",
       "  'identifier': '2033554200',\n",
       "  'references': ['2145023731',\n",
       "   '2085261163',\n",
       "   '3017143921',\n",
       "   '2911709767',\n",
       "   '2131806657',\n",
       "   '2026311529',\n",
       "   '2068204893',\n",
       "   '2000048778',\n",
       "   '2037732452',\n",
       "   '2052277674'],\n",
       "  'title': 'Recognizing solid objects by alignment with an image'},\n",
       " {'abstract': \"An algorithm was developed which facilitates the search for similarities between newly determined amino acid sequences and sequences already available in databases. Because of the algorithm's efficiency on many microcomputers, sensitive protein database searches may now become a routine procedure for molecular biologists. The method efficiently identifies regions of similar sequence and then scores the aligned identical and differing residues in those regions by means of an amino acid replacability matrix. This matrix increases sensitivity by giving high scores to those amino acid replacements which occur frequently in evolution. The algorithm has been implemented in a computer program designed to search protein databases very rapidly. For example, comparison of a 200-amino-acid sequence to the 500,000 residues in the National Biomedical Research Foundation library would take less than 2 minutes on a minicomputer, and less than 10 minutes on a microcomputer (IBM PC).\",\n",
       "  'authors': ['David J. Lipman ', ' William R. Pearson'],\n",
       "  'date': '1985',\n",
       "  'identifier': '2029195137',\n",
       "  'references': ['2087064593',\n",
       "   '2074231493',\n",
       "   '1981014117',\n",
       "   '2025529909',\n",
       "   '2013368806',\n",
       "   '2017963255',\n",
       "   '1982020713',\n",
       "   '1977618577',\n",
       "   '2005313213',\n",
       "   '1987809783'],\n",
       "  'title': 'Rapid and sensitive protein similarity searches'},\n",
       " {'abstract': \"Pylearn2 is a machine learning research library. This does not just mean that it is a collection of machine learning algorithms that share a common API; it means that it has been designed for flexibility and extensibility in order to facilitate research projects that involve new or unusual use cases. In this paper we give a brief history of the library, an overview of its basic philosophy, a summary of the library's architecture, and a description of how the Pylearn2 community functions socially.\",\n",
       "  'authors': ['Ian J. Goodfellow ',\n",
       "   ' David Warde-Farley ',\n",
       "   ' Pascal Lamblin ',\n",
       "   ' Vincent Dumoulin ',\n",
       "   ' Mehdi Mirza ',\n",
       "   ' Razvan Pascanu ',\n",
       "   ' James Bergstra ',\n",
       "   ' Frédéric Bastien ',\n",
       "   ' Yoshua Bengio'],\n",
       "  'date': '2013',\n",
       "  'identifier': '1872489089',\n",
       "  'references': ['2618530766',\n",
       "   '2101234009',\n",
       "   '3118608800',\n",
       "   '2310919327',\n",
       "   '1904365287',\n",
       "   '2168231600',\n",
       "   '2119821739',\n",
       "   '2116064496',\n",
       "   '2294059674',\n",
       "   '2025768430'],\n",
       "  'title': 'Pylearn2: a machine learning research library'},\n",
       " {'abstract': 'Quantization, the process of approximating continuous-amplitude signals by digital (discrete-amplitude) signals, is an important aspect of data compression or coding, the field concerned with the reduction of the number of bits necessary to transmit or store analog data, subject to a distortion or fidelity criterion. The independent quantization of each signal value or parameter is termed scalar quantization, while the joint quantization of a block of parameters is termed block or vector quantization. This tutorial review presents the basic concepts employed in vector quantization and gives a realistic assessment of its benefits and costs when compared to scalar quantization. Vector quantization is presented as a process of redundancy removal that makes effective use of four interrelated properties of vector parameters: linear dependency (correlation), nonlinear dependency, shape of the probability density function (pdf), and vector dimensionality itself. In contrast, scalar quantization can utilize effectively only linear dependency and pdf shape. The basic concepts are illustrated by means of simple examples and the theoretical limits of vector quantizer performance are reviewed, based on results from rate-distortion theory. Practical issues relating to quantizer design, implementation, and performance in actual applications are explored. While many of the methods presented are quite general and can be used for the coding of arbitrary signals, this paper focuses primarily on the coding of speech signals and parameters.',\n",
       "  'authors': ['J. Makhoul ', ' S. Roucos ', ' H. Gish'],\n",
       "  'date': '1985',\n",
       "  'identifier': '2002182716',\n",
       "  'references': ['3017143921',\n",
       "   '2134383396',\n",
       "   '2150593711',\n",
       "   '2913399920',\n",
       "   '2913066018',\n",
       "   '2151626637',\n",
       "   '2069501481',\n",
       "   '1966264494',\n",
       "   '2127218421',\n",
       "   '2058097301'],\n",
       "  'title': 'Vector quantization in speech coding'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Andreas Stolcke'],\n",
       "  'date': '2002',\n",
       "  'identifier': '1631260214',\n",
       "  'references': ['2158195707',\n",
       "   '2121227244',\n",
       "   '1904457459',\n",
       "   '2594610113',\n",
       "   '1549285799',\n",
       "   '2100506586',\n",
       "   '1797288984',\n",
       "   '2097978681',\n",
       "   '1528470941',\n",
       "   '2127836646'],\n",
       "  'title': 'SRILM – An Extensible Language Modeling Toolkit'},\n",
       " {'abstract': '1. We recorded activity of the hand-manipulation-task-related neurons in the posterolateral bank of the anterior intraparietal sulcus (area AIP) of the monkey parietal cortex during a delayed hand manipulation task. 2. We examined mainly the object-type visual-dominant and visual-and-motor neurons that responded to the sight of objects for manipulation. The majority of these neurons (32 of 48) showed sustained activity during the delay period in the dark before manipulation of preferred objects. 3. Six visual-and-motor neurons showed set-related activity before the hand manipulation in the dark, so that their delay period activity was likely to be related to motor preparation. 4. The delay period activity of 18 visual-dominant and visual-and-motor neurons without set-related activity was likely to represent spatial features of objects, because the majority of the neurons showed the same selectivity in the shape and/or orientation during object fixation and the delay period. 5. Of these 18 neurons, 10 showed sustained activity in the dark after brief illumination of objects during a light-interrupted fixation task, suggesting that they store the short-term memory of objects without the intention to remember. 6. The results suggest that the visual memory of three-dimensional features of objects is likely to be incorporated in area AIP and to be used for the guidance of hand manipulation.',\n",
       "  'authors': ['A. Murata ', ' V. Gallese ', ' M. Kaseda ', ' H. Sakata'],\n",
       "  'date': '1996',\n",
       "  'identifier': '1576240289',\n",
       "  'references': ['2082627290',\n",
       "   '2044474088',\n",
       "   '2013755742',\n",
       "   '1939596644',\n",
       "   '1523723941',\n",
       "   '2122354249',\n",
       "   '2042438889',\n",
       "   '2131635906',\n",
       "   '2004547450',\n",
       "   '1569395231'],\n",
       "  'title': 'Parietal neurons related to memory-guided hand manipulation.'},\n",
       " {'abstract': 'Recommender systems make use of a database of user ratings to generate personalized recommendations and help people to find relevant products, items, or documents. In this paper, we present a probabilistic, model-based framework for user ratings based on a novel collaborative filtering technique that performs an automatic decomposition of user preferences. Our approach has several benefits, including highly accurate predictions, task-optimized model learning, mining of interest groups and patterns, as well as a highly efficient and scalable computation of predictions and recommendation lists.',\n",
       "  'authors': ['Thomas Hofmann'],\n",
       "  'date': '2001',\n",
       "  'identifier': '1806731464',\n",
       "  'references': ['2110325612',\n",
       "   '2155106456',\n",
       "   '2107743791',\n",
       "   '2085937320',\n",
       "   '2124591829',\n",
       "   '1966553486',\n",
       "   '1999047234',\n",
       "   '1612003148',\n",
       "   '1629473008',\n",
       "   '2109992782'],\n",
       "  'title': \"Learning What People (Don't) Want\"},\n",
       " {'abstract': 'In cultures of dissociated rat hippocampal neurons, persistent potentiation and depression of glutamatergic synapses were induced by correlated spiking of presynaptic and postsynaptic neurons. The relative timing between the presynaptic and postsynaptic spiking determined the direction and the extent of synaptic changes. Repetitive postsynaptic spiking within a time window of 20 msec after presynaptic activation resulted in long-term potentiation (LTP), whereas postsynaptic spiking within a window of 20 msec before the repetitive presynaptic activation led to long-term depression (LTD). Significant LTP occurred only at synapses with relatively low initial strength, whereas the extent of LTD did not show obvious dependence on the initial synaptic strength. Both LTP and LTD depended on the activation of NMDA receptors and were absent in cases in which the postsynaptic neurons were GABAergic in nature. Blockade of L-type calcium channels with nimodipine abolished the induction of LTD and reduced the extent of LTP. These results underscore the importance of precise spike timing, synaptic strength, and postsynaptic cell type in the activity-induced modification of central synapses and suggest that Hebb’s rule may need to incorporate a quantitative consideration of spike timing that reflects the narrow and asymmetric window for the induction of synaptic modification.',\n",
       "  'authors': ['Guo-qiang Bi ', ' Mu-ming Poo'],\n",
       "  'date': '1998',\n",
       "  'identifier': '2147101007',\n",
       "  'references': ['2049511526',\n",
       "   '2009667219',\n",
       "   '22297218',\n",
       "   '2097560155',\n",
       "   '1986246844',\n",
       "   '2062899906',\n",
       "   '2089316513',\n",
       "   '1549050411',\n",
       "   '2072233684',\n",
       "   '1589549454'],\n",
       "  'title': 'Synaptic Modifications in Cultured Hippocampal Neurons: Dependence on Spike Timing, Synaptic Strength, and Postsynaptic Cell Type'},\n",
       " {'abstract': 'The extrema in a signal and its first few derivatives provide a useful general-purpose qualitative description for many kinds of signals. A fundamental problem in computing such descriptions is scale: a derivative must be taken over some neighborhood, but there is seldom a principled basis for choosing its size. Scale-space filtering is a method that describes signals qualitatively, managing the ambiguity of scale in an organized and natural way. The signal is first expanded by convolution with gaussian masks over a continuum of sizes. This \"scale-space\" image is then collapsed, using its qualitative structure, into a tree providing a concise but complete qualitative description covering all scales of observation. The description is further refined by applying a stability criterion, to identify events that persist of large changes in scale.',\n",
       "  'authors': ['Andrew P. Witkin'],\n",
       "  'date': '1987',\n",
       "  'identifier': '2109863423',\n",
       "  'references': ['1995756857',\n",
       "   '1968245656',\n",
       "   '1530383550',\n",
       "   '2171737600',\n",
       "   '2117900366',\n",
       "   '1870339432',\n",
       "   '2059376358',\n",
       "   '2147081095',\n",
       "   '1965555058',\n",
       "   '2044972977'],\n",
       "  'title': 'Scale-space filtering'},\n",
       " {'abstract': 'This paper describes a pedestrian detection system that integrates image intensity information with motion information. We use a detection style algorithm that scans a detector over two consecutive frames of a video sequence. The detector is trained (using AdaBoost) to take advantage of both motion and appearance information to detect a walking person. Past approaches have built detectors based on appearance information, but ours is the first to combine both sources of information in a single detector. The implementation described runs at about 4 frames/second, detects pedestrians at very small scales (as small as 20/spl times/15 pixels), and has a very low false positive rate. Our approach builds on the detection work of Viola and Jones. Novel contributions of this paper include: i) development of a representation of image motion which is extremely efficient, and ii) implementation of a state of the art pedestrian detection system which operates on low resolution images under difficult conditions (such as rain and snow).',\n",
       "  'authors': ['Viola 1', ' Jones 2'],\n",
       "  'date': '2003',\n",
       "  'identifier': '1992825118',\n",
       "  'references': ['2164598857',\n",
       "   '1988790447',\n",
       "   '2217896605',\n",
       "   '2115763357',\n",
       "   '2155511848',\n",
       "   '2798643531',\n",
       "   '2032210760',\n",
       "   '2145073242',\n",
       "   '2162919312',\n",
       "   '2089181482'],\n",
       "  'title': 'Detecting pedestrians using patterns of motion and appearance'},\n",
       " {'abstract': 'It is known that, in general, the number of measurements in a pattern classification problem cannot be increased arbitrarily, when the class-conditional densities are not completely known and only a finite number of learning samples are available. Above a certain number of measurements, the performance starts deteriorating instead of improving steadily. It was earlier shown by one of the authors that an exception to this \"curse of finite sample size\" is constituted by the case of binary independent measurements if a Bayesian approach is taken and uniform a priori on the unknown parameters are assumed. In this paper, the following generalizations are considered: arbitrary quantization and the use of maximum likelihood estimates. Further, the existence of an optimal quantization complexity is demonstrated, and its relationship to both the dimensionality of the measurement vector and the sample size are discussed. It is shown that the optimum number of quantization levels decreases with increasing dimensionality for a fixed sample size, and increases with the sample size for fixed dimensionality.',\n",
       "  'authors': ['B. Chandrasekaran ', ' A.K. Jain'],\n",
       "  'date': '1974',\n",
       "  'identifier': '2000367563',\n",
       "  'references': ['2098057602',\n",
       "   '2065283057',\n",
       "   '1983893626',\n",
       "   '2153338385',\n",
       "   '209146099',\n",
       "   '216801357'],\n",
       "  'title': 'Quantization Complexity and Independent Measurements'},\n",
       " {'abstract': 'Let G be a locally compact group, which need not be unimodular. Let x→U(x) (x∈G) be an irreducible unitary representation of G in a Hilbert space H(U). Assume that U is square integrable, i.e., that there exists in H(U) at least one nonzero vector g such that ∫‖(U(x)g,g)‖2\\u2009dx<∞. We give here a reasonably self‐contained analysis of the correspondence associating to every vector f∈H(U) the function (U(x)g,f) on G, discussing its isometry, characterization of the range, inversion, and simplest interpolation properties. This correspondence underlies many properties of generalized coherent states.',\n",
       "  'authors': ['A. Grossmann ', ' J. Morlet ', ' T. Paul'],\n",
       "  'date': '1985',\n",
       "  'identifier': '2014757225',\n",
       "  'references': ['2096684483',\n",
       "   '1575147392',\n",
       "   '1515431404',\n",
       "   '2022096757',\n",
       "   '1986119538',\n",
       "   '2319308948',\n",
       "   '2088753904',\n",
       "   '1603491459',\n",
       "   '42253355',\n",
       "   '1969304434'],\n",
       "  'title': 'Transforms associated to square integrable group representations. I. General results'},\n",
       " {'abstract': 'We present the lifting scheme, a simple construction of second generation wavelets; these are wavelets that are not necessarily translates and dilates of one fixed function. Such wavelets can be adapted to intervals, domains, surfaces, weights, and irregular samples. We show how the lifting scheme leads to a faster, in-place calculation of the wavelet transform. Several examples are included.',\n",
       "  'authors': ['Wim Sweldens'],\n",
       "  'date': '1998',\n",
       "  'identifier': '2020919250',\n",
       "  'references': ['2132984323',\n",
       "   '2098914003',\n",
       "   '1489213177',\n",
       "   '2015370045',\n",
       "   '2156447271',\n",
       "   '2004217976',\n",
       "   '2117188745',\n",
       "   '1975358183',\n",
       "   '1980149518',\n",
       "   '2166982406'],\n",
       "  'title': 'The lifting scheme: a construction of second generation wavelets'},\n",
       " {'abstract': \"From the Publisher: This book presents a coherent approach to the fast-moving field of computer vision, using a consistent notation based on a detailed understanding of the image formation process. It covers even the most recent research and will provide a useful and current reference for professionals working in the fields of machine vision, image processing, and pattern recognition. An outgrowth of the author's course at MIT, Robot Vision presents a solid framework for understanding existing work and planning future research. Its coverage includes a great deal of material that is important to engineers applying machine vision methods in the real world. The chapters on binary image processing, for example, help explain and suggest how to improve the many commercial devices now available. And the material on photometric stereo and the extended Gaussian image points the way to what may be the next thrust in commercialization of the results in this area. Chapters in the first part of the book emphasize the development of simple symbolic descriptions from images, while the remaining chapters deal with methods that exploit these descriptions. The final chapter offers a detailed description of how to integrate a vision system into an overall robotics system, in this case one designed to pick parts out of a bin. The many exercises complement and extend the material in the text, and an extensive bibliography will serve as a useful guide to current research. Errata (164k PDF)\",\n",
       "  'authors': ['Berthold Klaus Paul Horn'],\n",
       "  'date': '1986',\n",
       "  'identifier': '2911709767',\n",
       "  'references': ['2102625004',\n",
       "   '2111918405',\n",
       "   '2147253850',\n",
       "   '2142159465',\n",
       "   '2069281566',\n",
       "   '3003662786',\n",
       "   '2167501464',\n",
       "   '2033959528',\n",
       "   '2165949425'],\n",
       "  'title': 'Robot Vision'},\n",
       " {'abstract': 'Data Mining: Practical Machine Learning Tools and Techniques offers a thorough grounding in machine learning concepts as well as practical advice on applying machine learning tools and techniques in real-world data mining situations. This highly anticipated third edition of the most acclaimed work on data mining and machine learning will teach you everything you need to know about preparing inputs, interpreting outputs, evaluating results, and the algorithmic methods at the heart of successful data mining. Thorough updates reflect the technical changes and modernizations that have taken place in the field since the last edition, including new material on Data Transformations, Ensemble Learning, Massive Data Sets, Multi-instance Learning, plus a new version of the popular Weka machine learning software developed by the authors. Witten, Frank, and Hall include both tried-and-true techniques of today as well as methods at the leading edge of contemporary research. *Provides a thorough grounding in machine learning concepts as well as practical advice on applying the tools and techniques to your data mining projects *Offers concrete tips and techniques for performance improvement that work by transforming the input or output in machine learning methods *Includes downloadable Weka software toolkit, a collection of machine learning algorithms for data mining tasks-in an updated, interactive interface. Algorithms in toolkit cover: data pre-processing, classification, regression, clustering, association rules, visualization',\n",
       "  'authors': ['Ian H. Witten ', ' Eibe Frank ', ' Mark A. Hall'],\n",
       "  'date': '1999',\n",
       "  'identifier': '1570448133',\n",
       "  'references': ['2156909104',\n",
       "   '2140190241',\n",
       "   '1639032689',\n",
       "   '2331432542',\n",
       "   '1554663460',\n",
       "   '3013264884',\n",
       "   '2119821739',\n",
       "   '2139212933',\n",
       "   '2138621811',\n",
       "   '2912934387'],\n",
       "  'title': 'Data Mining: Practical Machine Learning Tools and Techniques'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Jacob Cohen'],\n",
       "  'date': '1960',\n",
       "  'identifier': '2053154970',\n",
       "  'references': ['2163329495',\n",
       "   '2133943399',\n",
       "   '2071206549',\n",
       "   '3016188653',\n",
       "   '2783254024',\n",
       "   '2093349432'],\n",
       "  'title': 'A Coefficient of agreement for nominal Scales'},\n",
       " {'abstract': 'Supersaturated designs are useful in situations in which the number of active factors is very small compared to the total number of factors being considered. In this article, a new class of supersaturated designs is constructed using half fractions of Hadamard matrices. When a Hadamard matrix of order N is used, such a design can investigate up to N – 2 factors in N/2 runs. Results are given for N ≤ 60. Extension to larger N is straightforward. These designs are superior to other existing supersaturated designs and are easy to construct. An example with real data is used to illustrate the ideas.',\n",
       "  'authors': ['Dennis K. J. Lin'],\n",
       "  'date': '1993',\n",
       "  'identifier': '2078495506',\n",
       "  'references': ['2000836282',\n",
       "   '2034562813',\n",
       "   '1957100706',\n",
       "   '2006901874',\n",
       "   '1993092509',\n",
       "   '2022498557',\n",
       "   '1991663098',\n",
       "   '2799326321',\n",
       "   '2010374567'],\n",
       "  'title': 'A new class of supersaturated designs'},\n",
       " {'abstract': 'We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech ...',\n",
       "  'authors': ['Anthony J. Bell ', ' Terrence J. Sejnowski'],\n",
       "  'date': '1999',\n",
       "  'identifier': '3110653090',\n",
       "  'references': ['2124776405',\n",
       "   '1548802052',\n",
       "   '2099741732',\n",
       "   '1996355918',\n",
       "   '2038085771',\n",
       "   '2180838288',\n",
       "   '2912889105',\n",
       "   '1667165204',\n",
       "   '2006544565',\n",
       "   '2056211671'],\n",
       "  'title': 'An information-maximization approach to blind separation and blind deconvolution'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Breck Baldwin ',\n",
       "   ' Thomas S. Morton ',\n",
       "   ' Amit Bagga ',\n",
       "   ' Jason Baldridge ',\n",
       "   ' Raman Chandraseker ',\n",
       "   ' Alexis Dimitriadis ',\n",
       "   ' Kieran Snyder ',\n",
       "   ' Magdalena Wolska'],\n",
       "  'date': '1998',\n",
       "  'identifier': '121091192',\n",
       "  'references': ['1965693266'],\n",
       "  'title': 'Description of the UPENN CAMP System as Used for Coreference.'},\n",
       " {'abstract': 'A four-component scattering model is proposed to decompose polarimetric synthetic aperture radar (SAR) images. The covariance matrix approach is used to deal with the nonreflection symmetric scattering case. This scheme includes and extends the three-component decomposition method introduced by Freeman and Durden dealing with the reflection symmetry condition that the co-pol and the cross-pol correlations are close to zero. Helix scattering power is added as the fourth component to the three-component scattering model which describes surface, double bounce, and volume scattering. This helix scattering term is added to take account of the co-pol and the cross-pol correlations which generally appear in complex urban area scattering and disappear for a natural distributed scatterer. This term is relevant for describing man-made targets in urban area scattering. In addition, asymmetric volume scattering covariance matrices are introduced in dependence of the relative backscattering magnitude between HH and VV. A modification of probability density function for a cloud of dipole scatterers yields asymmetric covariance matrices. An appropriate choice among the symmetric or asymmetric volume scattering covariance matrices allows us to make a best fit to the measured data. A four-component decomposition algorithm is developed to deal with a general scattering case. The result of this decomposition is demonstrated with L-band Pi-SAR images taken over the city of Niigata, Japan.',\n",
       "  'authors': ['Y. Yamaguchi 1',\n",
       "   ' T. Moriyama 2',\n",
       "   ' M. Ishido 1',\n",
       "   ' H. Yamada 1'],\n",
       "  'date': '2005',\n",
       "  'identifier': '2141424348',\n",
       "  'references': ['2078985447',\n",
       "   '2097272115',\n",
       "   '2164296623',\n",
       "   '2008826820',\n",
       "   '2148043654',\n",
       "   '1534963718',\n",
       "   '2127730845',\n",
       "   '2162877575',\n",
       "   '1537575538',\n",
       "   '1918853835'],\n",
       "  'title': 'Four-component scattering model for polarimetric SAR image decomposition'},\n",
       " {'abstract': 'Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge.',\n",
       "  'authors': ['Christian Szegedy ',\n",
       "   ' Sergey Ioffe ',\n",
       "   ' Vincent Vanhoucke ',\n",
       "   ' Alexander A Alemi'],\n",
       "  'date': '2016',\n",
       "  'identifier': '2964350391',\n",
       "  'references': ['2604319603',\n",
       "   '2955425717',\n",
       "   '2996428491',\n",
       "   '2965658867',\n",
       "   '2886335102',\n",
       "   '2942841021',\n",
       "   '2963918968',\n",
       "   '2774644650',\n",
       "   '2963402313'],\n",
       "  'title': 'Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning'},\n",
       " {'abstract': \"In content analysis and similar methods, data are typically generated by trained human observers who record or transcribe textual, pictorial, or audible matter in terms suitable for analysis. Conclusions from such data can be trusted only after demonstrating their reliability. Unfortunately, the content analysis literature is full of proposals for so-called reliability coefficients, leaving investigators easily confused, not knowing which to choose. After describing the criteria for a good measure of reliability, we propose Krippendorff's alpha as the standard reliability measure. It is general in that it can be used regardless of the number of observers, levels of measurement, sample sizes, and presence or absence of missing data. To facilitate the adoption of this recommendation, we describe a freely available macro written for SPSS and SAS to calculate Krippendorff's alpha and illustrate its use with a simple example.\",\n",
       "  'authors': ['Andrew F. Hayes 1', ' Klaus Krippendorff 2'],\n",
       "  'date': '2007',\n",
       "  'identifier': '2061504941',\n",
       "  'references': ['2331432542',\n",
       "   '2313581450',\n",
       "   '3021916629',\n",
       "   '2160375265',\n",
       "   '2012378416',\n",
       "   '2002664886',\n",
       "   '2107958340',\n",
       "   '2053154970',\n",
       "   '2069585723',\n",
       "   '1975879668'],\n",
       "  'title': 'Answering the Call for a Standard Reliability Measure for Coding Data'},\n",
       " {'abstract': 'A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two‐dimensional rigid‐sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four‐term virial coefficient expansion.',\n",
       "  'authors': ['Nicholas Metropolis ',\n",
       "   ' Arianna W. Rosenbluth ',\n",
       "   ' Marshall N. Rosenbluth ',\n",
       "   ' Augusta H. Teller ',\n",
       "   ' Edward Teller'],\n",
       "  'date': '1953',\n",
       "  'identifier': '2056760934',\n",
       "  'references': ['2052864234'],\n",
       "  'title': 'Equation of state calculations by fast computing machines'},\n",
       " {'abstract': 'Rumelhart (1987), has proposed a method for choosing minimal or \"simple\" representations during learning in Back-propagation networks. This approach can be used to (a) dynamically select the number of hidden units, (b) construct a representation that is appropriate for the problem and (c) thus improve the generalization ability of Back-propagation networks. The method Rumelhart suggests involves adding penalty terms to the usual error function. In this paper we introduce Rumelhart\\'s minimal networks idea and compare two possible biases on the weight search space. These biases are compared in both simple counting problems and a speech recognition problem. In general, the constrained search does seem to minimize the number of hidden units required with an expected increase in local minima.',\n",
       "  'authors': ['Stephen Jose Hanson 1', ' Lorien Y. Pratt 2'],\n",
       "  'date': '1988',\n",
       "  'identifier': '2120972216',\n",
       "  'references': ['2581275558', '2154642048', '2034562813'],\n",
       "  'title': 'Comparing Biases for Minimal Network Construction with Back-Propagation'},\n",
       " {'abstract': '',\n",
       "  'authors': ['John L. Pfaltz ', ' Azriel Rosenfeld'],\n",
       "  'date': '1967',\n",
       "  'identifier': '1996992876',\n",
       "  'references': ['2158240273',\n",
       "   '2108729336',\n",
       "   '2170633497',\n",
       "   '1982931780',\n",
       "   '2339569364',\n",
       "   '2478402337',\n",
       "   '2083377752'],\n",
       "  'title': 'Computer representation of planar regions by their skeletons'},\n",
       " {'abstract': 'In this paper, we describe a statistical method for 3D object detection. We represent the statistics of both object appearance and \"non-object\" appearance using a product of histograms. Each histogram represents the joint statistics of a subset of wavelet coefficients and their position on the object. Our approach is to use many such histograms representing a wide variety of visual attributes. Using this method, we have developed the first algorithm that can reliably detect human faces with out-of-plane rotation and the first algorithm that can reliably detect passenger cars over a wide range of viewpoints.',\n",
       "  'authors': ['H. Schneiderman ', ' T. Kanade'],\n",
       "  'date': '2000',\n",
       "  'identifier': '2155511848',\n",
       "  'references': ['2156909104',\n",
       "   '1988790447',\n",
       "   '2217896605',\n",
       "   '2117812871',\n",
       "   '1658679052',\n",
       "   '2159686933',\n",
       "   '2140785063',\n",
       "   '3003716168',\n",
       "   '2166713160',\n",
       "   '2138560582'],\n",
       "  'title': 'A statistical method for 3D object detection applied to faces and cars'},\n",
       " {'abstract': 'In medicine we often want to compare two different methods of measuring some quantity, such as blood pressure, gestational age, or cardiac stroke volume. Sometimes we compare an approximate or simple method with a very precise one. This is a calibration problem, and we shall not discuss it further here. Frequently, however, we cannot regard either method as giving the true value of the quantity being measured. In this case we want to know whether the methods give answers which are, in some sense, comparable. For example, we may wish to see whether a new, cheap and quick method produces answers that agree with those from an established method sufficiently well for clinical purposes. Many such studies, using a variety of statistical techniques, have been reported. Yet few really answer the question “Do the two methods of measurement agree sufficiently closely?” In this paper we shall describe what is usually done, show why this is inappropriate, suggest a better approach, and ask why such studies are done so badly. We will restrict our consideration to the comparison of two methods of measuring a continuous variable, although similar problems can arise with categorical variables.',\n",
       "  'authors': ['D. G. Altman 1', ' J. M. Bland 2'],\n",
       "  'date': '1983',\n",
       "  'identifier': '2133750711',\n",
       "  'references': ['2798643531',\n",
       "   '2109316327',\n",
       "   '2132970644',\n",
       "   '2795740701',\n",
       "   '2089771080',\n",
       "   '2144348065',\n",
       "   '2009635173',\n",
       "   '1991656581',\n",
       "   '2037135439',\n",
       "   '587168508'],\n",
       "  'title': 'Measurement in Medicine: The Analysis of Method Comparison Studies'},\n",
       " {'abstract': 'Abstract The independent component analysis (ICA) of a random vector consists of searching for a linear transformation that minimizes the statistical dependence between its components. In order to define suitable search criteria, the expansion of mutual information is utilized as a function of cumulants of increasing orders. An efficient algorithm is proposed, which allows the computation of the ICA of a data matrix within a polynomial time. The concept of ICA may actually be seen as an extension of the principal component analysis (PCA), which can only impose independence up to the second order and, consequently, defines directions that are orthogonal. Potential applications of ICA include data analysis and compression, Bayesian detection, localization of sources, and blind identification and deconvolution.',\n",
       "  'authors': ['Pierre Comon'],\n",
       "  'date': '1994',\n",
       "  'identifier': '2099741732',\n",
       "  'references': ['1996355918',\n",
       "   '2018388266',\n",
       "   '2114018052',\n",
       "   '2098301339',\n",
       "   '1560089794',\n",
       "   '2796930440',\n",
       "   '2225937484',\n",
       "   '1995963238',\n",
       "   '2140352766',\n",
       "   '2052958516'],\n",
       "  'title': 'Independent component analysis, a new concept?'},\n",
       " {'abstract': 'Since the Lucas-Kanade algorithm was proposed in 1981 image alignment has become one of the most widely used techniques in computer vision. Applications range from optical flow and tracking to layered motion, mosaic construction, and face coding. Numerous algorithms have been proposed and a wide variety of extensions have been made to the original formulation. We present an overview of image alignment, describing most of the algorithms and their extensions in a consistent framework. We concentrate on the inverse compositional algorithm, an efficient algorithm that we recently proposed. We examine which of the extensions to Lucas-Kanade can be used with the inverse compositional algorithm without any significant loss of efficiency, and which cannot. In this paper, Part 1 in a series of papers, we cover the quantity approximated, the warp update rule, and the gradient descent approximation. In future papers, we will cover the choice of the error function, how to allow linear appearance variation, and how to impose priors on the parameters.',\n",
       "  'authors': ['Simon Baker ', ' Iain Matthews'],\n",
       "  'date': '2004',\n",
       "  'identifier': '2035379092',\n",
       "  'references': ['3111950349',\n",
       "   '2082308025',\n",
       "   '2140257560',\n",
       "   '2103876808',\n",
       "   '2118877769',\n",
       "   '2954064014',\n",
       "   '1938714998',\n",
       "   '2753461371',\n",
       "   '2293264518',\n",
       "   '2206723888'],\n",
       "  'title': 'Lucas-Kanade 20 Years On: A Unifying Framework'},\n",
       " {'abstract': \"The perceptual recognition of objects is conceptualized to be a process in which the image of the input is segmented at regions of deep concavity into simple volumetric components, such as blocks, cylinders, wedges, and cones. The fundamental assumption of the proposed theory, recognition-by-components (RBC), is that a modest set of components [ N probably ≤ 36] can be derived from contrasts of five readily detectable properties of edges in a 2-dimensional image: curvature, collinearity, symmetry, parallelism, and cotermination. The detection of these properties is generally invariant over viewing position and image quality and consequently allows robust object perception when the image is projected from a novel viewpoint or degraded. RBC thus provides a principled account of the heretofore undecided relation between the classic principles of perceptual organization and pattern recognition: The constraints toward regularization (Pragnanz) characterize not the complete object but the object's components. A principle of componential recovery can account for the major phenomena of object recognition: If an arrangement of two or three primitive components can be recovered from the input, objects can be quickly recognized even when they are occluded, rotated in depth, novel, or extensively degraded. The results from experiments on the perception of briefly presented pictures by human observers provide empirical support for the theory.\",\n",
       "  'authors': ['Irving Biederman'],\n",
       "  'date': '1985',\n",
       "  'identifier': '2125756925',\n",
       "  'references': ['2740373864',\n",
       "   '2149095485',\n",
       "   '2059975159',\n",
       "   '2073257493',\n",
       "   '1513966746',\n",
       "   '2032533296',\n",
       "   '2059799772',\n",
       "   '1501418839',\n",
       "   '1984314602',\n",
       "   '2065343501'],\n",
       "  'title': 'Human image understanding : Recent research and a theory'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Durbin'],\n",
       "  'date': '1998',\n",
       "  'identifier': '2890040444',\n",
       "  'references': ['2136879569',\n",
       "   '2158266063',\n",
       "   '2095680943',\n",
       "   '2120340025',\n",
       "   '2152770371',\n",
       "   '1519266993',\n",
       "   '2105842272',\n",
       "   '2041257508',\n",
       "   '2150131459',\n",
       "   '2607669908'],\n",
       "  'title': 'Biological Sequence Analysis'},\n",
       " {'abstract': 'The self-organizing map (SOM) is an excellent tool in exploratory phase of data mining. It projects input space on prototypes of a low-dimensional regular grid that can be effectively utilized to visualize and explore properties of the data. When the number of SOM units is large, to facilitate quantitative analysis of the map and the data, similar units need to be grouped, i.e., clustered. In this paper, different approaches to clustering of the SOM are considered. In particular, the use of hierarchical agglomerative clustering and partitive clustering using K-means are investigated. The two-stage procedure-first using SOM to produce the prototypes that are then clustered in the second stage-is found to perform well when compared with direct clustering of the data and to reduce the computation time.',\n",
       "  'authors': ['J. Vesanto ', ' E. Alhoniemi'],\n",
       "  'date': '2000',\n",
       "  'identifier': '2110802877',\n",
       "  'references': ['1679913846',\n",
       "   '2171277043',\n",
       "   '2141585940',\n",
       "   '1530010412',\n",
       "   '2162161511',\n",
       "   '2171975443',\n",
       "   '1992402718',\n",
       "   '1975152892',\n",
       "   '2088709281',\n",
       "   '2169507824'],\n",
       "  'title': 'Clustering of the self-organizing map'},\n",
       " {'abstract': 'Recurrent neural networks are powerful sequence learners. They are able to incorporate context information in a flexible way, and are robust to localised distortions of the input data. These properties make them well suited to sequence labelling, where input sequences are transcribed with streams of labels. The aim of this thesis is to advance the state-of-the-art in supervised sequence labelling with recurrent networks. Its two main contributions are (1) a new type of output layer that allows recurrent networks to be trained directly for sequence labelling tasks where the alignment between the inputs and the labels is unknown, and (2) an extension of the long short-term memory network architecture to multidimensional data, such as images and video sequences.',\n",
       "  'authors': ['Alexander Graves'],\n",
       "  'date': '2012',\n",
       "  'identifier': '2144499799',\n",
       "  'references': ['2156909104',\n",
       "   '1663973292',\n",
       "   '2136922672',\n",
       "   '2310919327',\n",
       "   '2064675550',\n",
       "   '1554663460',\n",
       "   '2147880316',\n",
       "   '2125838338',\n",
       "   '2110798204',\n",
       "   '1993882792'],\n",
       "  'title': 'Supervised Sequence Labelling with Recurrent Neural Networks'},\n",
       " {'abstract': 'Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.',\n",
       "  'authors': ['G. Hinton 1',\n",
       "   ' Li Deng 2',\n",
       "   ' Dong Yu 2',\n",
       "   ' G. E. Dahl 1',\n",
       "   ' A. Mohamed 1',\n",
       "   ' N. Jaitly 1',\n",
       "   ' Andrew Senior 3',\n",
       "   ' V. Vanhoucke 3',\n",
       "   ' P. Nguyen 3',\n",
       "   ' T. N. Sainath 4',\n",
       "   ' B. Kingsbury 4'],\n",
       "  'date': '2012',\n",
       "  'identifier': '2160815625',\n",
       "  'references': [],\n",
       "  'title': 'Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups'},\n",
       " {'abstract': 'The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of stateof-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.',\n",
       "  'authors': ['David Silver ',\n",
       "   ' Aja Huang ',\n",
       "   ' Christopher J. Maddison ',\n",
       "   ' Arthur Guez ',\n",
       "   ' Laurent Sifre ',\n",
       "   ' George van den Driessche ',\n",
       "   ' Julian Schrittwieser ',\n",
       "   ' Ioannis Antonoglou ',\n",
       "   ' Veda Panneershelvam ',\n",
       "   ' Marc Lanctot ',\n",
       "   ' Sander Dieleman ',\n",
       "   ' Dominik Grewe ',\n",
       "   ' John Nham ',\n",
       "   ' Nal Kalchbrenner ',\n",
       "   ' Ilya Sutskever ',\n",
       "   ' Timothy Lillicrap ',\n",
       "   ' Madeleine Leach ',\n",
       "   ' Koray Kavukcuoglu ',\n",
       "   ' Thore Graepel ',\n",
       "   ' Demis Hassabis'],\n",
       "  'date': '2016',\n",
       "  'identifier': '2257979135',\n",
       "  'references': ['2618530766',\n",
       "   '2919115771',\n",
       "   '2145339207',\n",
       "   '2168231600',\n",
       "   '2121863487',\n",
       "   '2126316555',\n",
       "   '1625390266',\n",
       "   '2155027007',\n",
       "   '1714211023',\n",
       "   '2100677568'],\n",
       "  'title': 'Mastering the game of Go with deep neural networks and tree search'},\n",
       " {'abstract': 'A convolutional encoder is defined as any constant linear sequential circuit. The associated code is the set of all output sequences resulting from any set of input sequences beginning at any time. Encoders are called equivalent if they generate the same code. The invariant factor theorem is used to determine when a convolutional encoder has a feedback-free inverse, and the minimum delay of any inverse. All encoders are shown to be equivalent to minimal encoders, which are feedback-free encoders with feedback-free delay-free inverses, and which can be realized in the conventional manner with as few memory elements as any equivalent encoder, Minimal encoders are shown to be immune to catastrophic error propagation and, in fact, to lead in a certain sense to the shortest decoded error sequences possible per error event. In two appendices, we introduce dual codes and syndromes, and show that a minimal encoder for a dual code has exactly the complexity of the original encoder; we show that systematic encoders with feedback form a canonical class, and compare this class to the minimal class.',\n",
       "  'authors': ['Jr. G. Forney'],\n",
       "  'date': '1970',\n",
       "  'identifier': '2153810958',\n",
       "  'references': ['1991133427',\n",
       "   '2064759901',\n",
       "   '2039660064',\n",
       "   '2151814693',\n",
       "   '2087932696',\n",
       "   '2079363366',\n",
       "   '2030585053',\n",
       "   '1999621872',\n",
       "   '1997590300',\n",
       "   '2141628061'],\n",
       "  'title': 'Convolutional codes I: Algebraic structure'},\n",
       " {'abstract': '',\n",
       "  'authors': ['J. H. Friedman'],\n",
       "  'date': '1991',\n",
       "  'identifier': '139959648',\n",
       "  'references': ['2135046866',\n",
       "   '2912934387',\n",
       "   '1678356000',\n",
       "   '2024046085',\n",
       "   '2158940042',\n",
       "   '2079724595',\n",
       "   '2033686454',\n",
       "   '2144404214',\n",
       "   '2289748525',\n",
       "   '1580948147'],\n",
       "  'title': 'Multivariate adaptive regression splines (with discussion)'},\n",
       " {'abstract': \"The basic theory of Markov chains has been known to mathematicians and engineers for close to 80 years, but it is only in the past decade that it has been applied explicitly to problems in speech processing. One of the major reasons why speech models, based on Markov chains, have not been developed until recently was the lack of a method for optimizing the parameters of the Markov model to match observed signal patterns. Such a method was proposed in the late 1960's and was immediately applied to speech processing in several research institutions. Continued refinements in the theory and implementation of Markov modelling techniques have greatly enhanced the method, leading to a wide range of applications of these models. It is the purpose of this tutorial paper to give an introduction to the theory of Markov models, and to illustrate how they have been applied to problems in speech recognition.\",\n",
       "  'authors': ['L. Rabiner ', ' B. Juang'],\n",
       "  'date': '1986',\n",
       "  'identifier': '2105594594',\n",
       "  'references': ['2142384583',\n",
       "   '2171850596',\n",
       "   '2020127877',\n",
       "   '1990005915',\n",
       "   '2021760654',\n",
       "   '2086699924',\n",
       "   '2112197391',\n",
       "   '2139576349',\n",
       "   '2163929346',\n",
       "   '2165253089'],\n",
       "  'title': 'An introduction to hidden Markov models'},\n",
       " {'abstract': 'Finding useful patterns in large datasets has attracted considerable interest recently, and one of the most widely studied problems in this area is the identification of clusters, or densely populated regions, in a multi-dimensional dataset. Prior work does not adequately address the problem of large datasets and minimization of I/O costs.This paper presents a data clustering method named BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies), and demonstrates that it is especially suitable for very large databases. BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints). BIRCH can typically find a good clustering with a single scan of the data, and improve the quality further with a few additional scans. BIRCH is also the first clustering algorithm proposed in the database area to handle \"noise\" (data points that are not part of the underlying pattern) effectively.We evaluate BIRCH\\'s time/space efficiency, data input order sensitivity, and clustering quality through several experiments. We also present a performance comparisons of BIRCH versus CLARANS, a clustering method proposed recently for large datasets, and show that BIRCH is consistently superior.',\n",
       "  'authors': ['Tian Zhang ', ' Raghu Ramakrishnan ', ' Miron Livny'],\n",
       "  'date': '1996',\n",
       "  'identifier': '2095897464',\n",
       "  'references': ['1634005169',\n",
       "   '2999729612',\n",
       "   '3017143921',\n",
       "   '1575476631',\n",
       "   '1493454437',\n",
       "   '2073308541',\n",
       "   '2101616188',\n",
       "   '2049631158',\n",
       "   '1577072181',\n",
       "   '2024868117'],\n",
       "  'title': 'BIRCH: an efficient data clustering method for very large databases'},\n",
       " {'abstract': 'PART ONE: INTRODUCTION Traditional Parametric Statistical Inference Bootstrap Statistical Inference Bootstrapping a Regression Model Theoretical Justification The Jackknife Monte Carlo Evaluation of the Bootstrap PART TWO: STATISTICAL INFERENCE USING THE BOOTSTRAP Bias Estimation Bootstrap Confidence Intervals PART THREE: APPLICATIONS OF BOOTSTRAP CONFIDENCE INTERVALS Confidence Intervals for Statistics With Unknown Sampling Distributions Inference When Traditional Distributional Assumptions Are Violated PART FOUR: CONCLUSION Future Work Limitations of the Bootstrap Concluding Remarks',\n",
       "  'authors': ['Eric R. Ziegel ', ' C. Mooney ', ' R. Duvall'],\n",
       "  'date': '1993',\n",
       "  'identifier': '2107958340',\n",
       "  'references': ['2123466232'],\n",
       "  'title': 'Bootstrapping: A Nonparametric Approach to Statistical Inference'},\n",
       " {'abstract': 'The authors outline a new scheme for parameterizing polarimetric scattering problems, which has application in the quantitative analysis of polarimetric SAR data. The method relies on an eigenvalue analysis of the coherency matrix and employs a three-level Bernoulli statistical model to generate estimates of the average target scattering matrix parameters from the data. The scattering entropy is a key parameter is determining the randomness in this model and is seen as a fundamental parameter in assessing the importance of polarimetry in remote sensing problems. The authors show application of the method to some important classical random media scattering problems and apply it to POLSAR data from the NASA/JPL AIRSAR data base.',\n",
       "  'authors': ['S.R. Cloude ', ' E. Pottier'],\n",
       "  'date': '1997',\n",
       "  'identifier': '2133989913',\n",
       "  'references': ['2078985447',\n",
       "   '1588167619',\n",
       "   '1985194020',\n",
       "   '2115226769',\n",
       "   '2199513852',\n",
       "   '1986603680',\n",
       "   '2147439738',\n",
       "   '2044781968',\n",
       "   '2089940122',\n",
       "   '2166960586'],\n",
       "  'title': 'An entropy based classification scheme for land applications of polarimetric SAR'},\n",
       " {'abstract': 'In this paper we review classification algorithms used to design brain–computer interface (BCI) systems based on electroencephalography (EEG). We briefly present the commonly employed algorithms and describe their critical properties. Based on the literature, we compare them in terms of performance and provide guidelines to choose the suitable classification algorithm(s) for a specific BCI.',\n",
       "  'authors': ['Fabien Lotte 1',\n",
       "   ' Marco Congedo 2',\n",
       "   ' Anatole Lécuyer 3',\n",
       "   ' Fabrice Lamarche 3',\n",
       "   ' Bruno Arnaldi 3'],\n",
       "  'date': '2007',\n",
       "  'identifier': '2075647286',\n",
       "  'references': ['1554663460',\n",
       "   '2139212933',\n",
       "   '2125838338',\n",
       "   '2106006415',\n",
       "   '2132549764',\n",
       "   '2046079134',\n",
       "   '2149298154',\n",
       "   '1588351438',\n",
       "   '2151669316',\n",
       "   '2145302786'],\n",
       "  'title': 'A review of classification algorithms for EEG-based brain–computer interfaces'},\n",
       " {'abstract': 'Research aimed at correcting words in text has focused on three progressively more difficult problems:(1) nonword error detection; (2) isolated-word error correction; and (3) context-dependent work correction. In response to the first problem, efficient pattern-matching and n -gram analysis techniques have been developed for detecting strings that do not appear in a given word list. In response to the second problem, a variety of general and application-specific spelling correction techniques have been developed. Some of them were based on detailed studies of spelling error patterns. In response to the third problem, a few experiments using natural-language-processing tools or statistical-language models have been carried out. This article surveys documented findings on spelling error patterns, provides descriptions of various nonword detection and isolated-word error correction techniques, reviews the state of the art of context-dependent word correction techniques, and discusses research issues related to all three areas of automatic error correction in text.',\n",
       "  'authors': ['Karen Kukich'],\n",
       "  'date': '1992',\n",
       "  'identifier': '2010595692',\n",
       "  'references': ['2154642048',\n",
       "   '2147152072',\n",
       "   '1833785989',\n",
       "   '2121227244',\n",
       "   '2099247782',\n",
       "   '2097333193',\n",
       "   '1966812932',\n",
       "   '2142384583',\n",
       "   '1501400124',\n",
       "   '23758216'],\n",
       "  'title': 'Techniques for automatically correcting words in text'},\n",
       " {'abstract': 'This paper demonstrates that a simple rule system can be constructed that supports a more powerful view system than available in current commercial systems. Not only can views be specified by using rules but also special semantics for resolving ambiguous view updates are simply additional rules. Moreover, procedural data types as proposed in POSTGRES are also efficiently simulated by the same rules system. Lastly, caching of the action part of certain rules is a possible performance enhancement and can be applied to materialize views as well as to cache procedural data items. Hence, we conclude that a rule system is a fundamental concept in a next generation DBMS, and it subsumes both views and procedures as special cases.',\n",
       "  'authors': ['Michael Stonebraker ',\n",
       "   ' Anant Jhingran ',\n",
       "   ' Jeffrey Goh ',\n",
       "   ' Spyros Potamianos'],\n",
       "  'date': '1994',\n",
       "  'identifier': '138816814',\n",
       "  'references': ['2072627547',\n",
       "   '2120052337',\n",
       "   '2103920867',\n",
       "   '2158655585',\n",
       "   '1573833643',\n",
       "   '1505165377',\n",
       "   '2065419751',\n",
       "   '52590956'],\n",
       "  'title': 'On rules, procedures, caching and views in database systems'},\n",
       " {'abstract': 'From the Publisher: Computer networking is changing the way people work and the way organizations function. \"Connections\" is an accessible guide to the promise and the pitfalls of this latest phase of the computer revolution.',\n",
       "  'authors': ['Lee S. Sproull 1', ' Sara Kiesler 2'],\n",
       "  'date': '1991',\n",
       "  'identifier': '2024372407',\n",
       "  'references': ['2147050295',\n",
       "   '2576297379',\n",
       "   '2062866147',\n",
       "   '2080957047',\n",
       "   '1560831680',\n",
       "   '2040817488',\n",
       "   '2159730225',\n",
       "   '2039688301',\n",
       "   '1535174579',\n",
       "   '1918878574'],\n",
       "  'title': 'Connections: New Ways of Working in the Networked Organization'},\n",
       " {'abstract': \"Abstract : This reprint will introduce and study the most basic properties of three new variational problems which are suggested by applications to computer vision. In computer vision, a fundamental problem is to appropriately decompose the domain R of a function g (x,y) of two variables. This problem starts by describing the physical situation which produces images: assume that a three-dimensional world is observed by an eye or camera from some point P and that g1(rho) represents the intensity of the light in this world approaching the point sub 1 from a direction rho. If one has a lens at P focusing this light on a retina or a film-in both cases a plane domain R in which we may introduce coordinates x, y then let g(x,y) be the strength of the light signal striking R at a point with coordinates (x,y); g(x,y) is essentially the same as sub 1 (rho) -possibly after a simple transformation given by the geometry of the imaging syste. The function g(x,y) defined on the plane domain R will be called an image. What sort of function is g? The light reflected off the surfaces Si of various solid objects O sub i visible from P will strike the domain R in various open subsets R sub i. When one object O1 is partially in front of another object O2 as seen from P, but some of object O2 appears as the background to the sides of O1, then the open sets R1 and R2 will have a common boundary (the 'edge' of object O1 in the image defined on R) and one usually expects the image g(x,y) to be discontinuous along this boundary. (JHD)\",\n",
       "  'authors': ['David Bryant Mumford 1', ' Jayant Shah 2'],\n",
       "  'date': '1989',\n",
       "  'identifier': '2114487471',\n",
       "  'references': ['1997063559',\n",
       "   '2120062331',\n",
       "   '1586623601',\n",
       "   '2147035016',\n",
       "   '2564031986',\n",
       "   '1601420106',\n",
       "   '2130711343',\n",
       "   '2033954961',\n",
       "   '2042352549',\n",
       "   '2039343706'],\n",
       "  'title': 'Optimal approximations by piecewise smooth functions and associated variational problems'},\n",
       " {'abstract': 'There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.',\n",
       "  'authors': ['Honglak Lee ',\n",
       "   ' Roger Grosse ',\n",
       "   ' Rajesh Ranganath ',\n",
       "   ' Andrew Y. Ng'],\n",
       "  'date': '2009',\n",
       "  'identifier': '2130325614',\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2162915993',\n",
       "   '2116064496',\n",
       "   '2110798204',\n",
       "   '2166049352',\n",
       "   '2145889472',\n",
       "   '2122922389',\n",
       "   '2147800946',\n",
       "   '2168002178'],\n",
       "  'title': 'Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations'},\n",
       " {'abstract': 'We present a new approach to summary evaluation which combines two novel aspects, namely (a) content comparison between gold standard summary and system summary via factoids, a pseudo-semantic representation based on atomic information units which can be robustly marked in text, and (b) use of a gold standard consensus summary, in our case based on 50 individual summaries of one text. Even though future work on more than one source text is imperative, our experiments indicate that (1) ranking with regard to a single gold standard summary is insufficient as rankings based on any two randomly chosen summaries are very dissimilar (correlations average ρ = 0.20), (2) a stable consensus summary can only be expected if a larger number of summaries are collected (in the range of at least 30--40 summaries), and (3) similarity measurement using unigrams shows a similarly low ranking correlation when compared with factoid-based ranking.',\n",
       "  'authors': ['Hans van Halteren 1', ' Simone Teufel 2'],\n",
       "  'date': '2003',\n",
       "  'identifier': '1977747299',\n",
       "  'references': ['2101105183',\n",
       "   '2293771131',\n",
       "   '2120308175',\n",
       "   '2062432826',\n",
       "   '2160204597',\n",
       "   '2137591918',\n",
       "   '2065553296',\n",
       "   '2171633935',\n",
       "   '2119187129',\n",
       "   '1589319550'],\n",
       "  'title': 'Examining the consensus between human summaries: initial experiments with factoid analysis'},\n",
       " {'abstract': '1. Background material from rings and modules 2. Homological algebra 3. Modules for group algebra 4. Methods from the representation of algebra 5. Representation rings and Burnside rings 6. Block theory.',\n",
       "  'authors': ['Charles W Curtis ', ' Irving Reiner'],\n",
       "  'date': '1962',\n",
       "  'identifier': '2039660064',\n",
       "  'references': ['1972151784',\n",
       "   '3101453708',\n",
       "   '585933500',\n",
       "   '1964692317',\n",
       "   '2013327405',\n",
       "   '2963897359',\n",
       "   '2560386163',\n",
       "   '2087256203'],\n",
       "  'title': 'Representation Theory of Finite Groups and Associative Algebras'},\n",
       " {'abstract': \"An approach has been developed that involves the fit of a combination of three simple scattering mechanisms to polarimetric SAR observations. The mechanisms are canopy scatter from a cloud of randomly oriented dipoles, evenor double-bounce scatter from a pair of orthogonal surfaces with different dielectric constants and Bragg scatter from a moderately rough surface. This composite scattering model is used to describe the polarimetric backscatter from naturally occurring scatterers. The model is shown to describe the behavior of polarimetric backscatter from tropical rain forests quite well by applying it to data from NASA/Jet Propulsion Laboratory's (JPLs) airborne polarimetric synthetic aperture radar (AIRSAR) system. The model fit allows clear discrimination between flooded and nonflooded forest and between forested and deforested areas, for example. The model is also shown to be usable as a predictive tool to estimate the effects of forest inundation and disturbance on the fully polarimetric radar signature. An advantage of this model fit approach is that the scattering contributions from the three basic scattering mechanisms can be estimated for clusters of pixels in polarimetric SAR images. Furthermore, it is shown that the contributions of the three scattering mechanisms to the HH, HV, and VV backscatter can be calculated from the model fit. Finally, this model fit approach is justified as a simplification of more complicated scattering models, which require many inputs to solve the forward scattering problem.\",\n",
       "  'authors': ['A. Freeman ', ' S.L. Durden'],\n",
       "  'date': '1998',\n",
       "  'identifier': '2097272115',\n",
       "  'references': ['2078985447',\n",
       "   '2105781415',\n",
       "   '2152269371',\n",
       "   '2141348340',\n",
       "   '2115226769',\n",
       "   '2099614632',\n",
       "   '2199513852',\n",
       "   '2044781968',\n",
       "   '2005509492',\n",
       "   '2028419268'],\n",
       "  'title': 'A three-component scattering model for polarimetric SAR data'},\n",
       " {'abstract': 'Over the last couple of years, face recognition researchers have been developing new techniques. These developments are being fueled by advances in computer vision techniques, computer design, sensor design, and interest in fielding face recognition systems. Such advances hold the promise of reducing the error rate in face recognition systems by an order of magnitude over Face Recognition Vendor Test (FRVT) 2002 results. The face recognition grand challenge (FRGC) is designed to achieve this performance goal by presenting to researchers a six-experiment challenge problem along with data corpus of 50,000 images. The data consists of 3D scans and high resolution still imagery taken under controlled and uncontrolled conditions. This paper describes the challenge problem, data corpus, and presents baseline performance and preliminary results on natural statistics of facial imagery.',\n",
       "  'authors': ['P.J. Phillips 1',\n",
       "   ' P.J. Flynn 2',\n",
       "   ' T. Scruggs 3',\n",
       "   ' K.W. Bowyer 2',\n",
       "   ' Jin Chang 2',\n",
       "   ' K. Hoffman 3',\n",
       "   ' J. Marques 4',\n",
       "   ' Jaesik Min 2',\n",
       "   ' W. Worek 3'],\n",
       "  'date': '2005',\n",
       "  'identifier': '2137659841',\n",
       "  'references': ['2138451337',\n",
       "   '2102773363',\n",
       "   '2143542740',\n",
       "   '2137385871',\n",
       "   '2120838001',\n",
       "   '1555969862',\n",
       "   '138943044'],\n",
       "  'title': 'Overview of the face recognition grand challenge'},\n",
       " {'abstract': 'Accumulating neuropsychological, electrophysiological and behavioural evidence suggests that the neural substrates of visual perception may be quite distinct from those underlying the visual control of actions. In other words, the set of object descriptions that permit identification and recognition may be computed independently of the set of descriptions that allow an observer to shape the hand appropriately to pick up an object. We propose that the ventral stream of projections from the striate cortex to the inferotemporal cortex plays the major role in the perceptual identification of objects, while the dorsal stream projecting from the striate cortex to the posterior parietal region mediates the required sensorimotor transformations for visually guided actions directed at such objects.',\n",
       "  'authors': ['Melvyn A. Goodale 1', ' A.David Milner 2'],\n",
       "  'date': '1992',\n",
       "  'identifier': '2082627290',\n",
       "  'references': ['1601242444',\n",
       "   '1979741733',\n",
       "   '2033118251',\n",
       "   '1986450498',\n",
       "   '2139721641',\n",
       "   '576881103',\n",
       "   '1990848854',\n",
       "   '2042201713',\n",
       "   '2058395663',\n",
       "   '2044539750'],\n",
       "  'title': 'Separate visual pathways for perception and action.'},\n",
       " {'abstract': 'This paper presents and evaluates some new heuristic procedures for seeking an approximate solution of pure integer linear programming problems having only inequality constraints. The computation time required by these methods after obtaining the optimal noninteger solution by the simplex method has generally been only a small fraction of that used by the simplex method for the problems tested which have 15 to 300 original variables. Furthermore, the solution obtained by the better procedures consistently has been close to optimal and frequently has actually been optimal. Plans for generalizing these methods also are outlined. A companion paper presents an optimal \"bound-and-scan\" algorithm that may be used in conjunction with these approximate procedures.',\n",
       "  'authors': ['Frederick S. Hillier'],\n",
       "  'date': '1969',\n",
       "  'identifier': '1982719520',\n",
       "  'references': ['2123241324',\n",
       "   '2751335644',\n",
       "   '2100890245',\n",
       "   '2009683816',\n",
       "   '2313810840',\n",
       "   '1992428442',\n",
       "   '2076756604',\n",
       "   '288256330',\n",
       "   '2046076510',\n",
       "   '2045411541'],\n",
       "  'title': 'Efficient Heuristic Procedures for Integer Linear Programming with an Interior'},\n",
       " {'abstract': 'A hidden Markov model (HMM) based word recognition algorithm for the recognition of legal amounts from French bank checks is presented. This algorithm is part of the A2iA INTERCHEQUE recognition system. The algorithm starts from images of handwritten words which have been automatically segmented from binary check images. After finding the lower-case zone on the complete amount, words are slant corrected and then segmented into graphemes. Then, features are extracted from the graphemes, and the feature vectors are vector quantized resulting in a sequence of symbols for each word. Likelihoods of all word classes are computed by a set of HMMs, which have been previously trained using either the Viterbi algorithm or the Baum?Welch algorithm. The various parameters of the system have been identified and their importance evaluated. Results have been obtained on large real-life data bases of French handwritten checks. The HMM-based system has been shown to outperform a holistic word recognizer and another HMM-type word recognizer from the A2iA INTERCHEQUE recognition system. Word recognition rates of about 89% for the 26-word vocabulary relevant for legal amount recognition on French bank checks have been obtained. More recently, a Neural Network?HMM hybrid has been designed, which produces even better recognition rates.',\n",
       "  'authors': ['S. Knerr ', ' E. Augustin ', ' O. Baret ', ' D. Price'],\n",
       "  'date': '1998',\n",
       "  'identifier': '2064838583',\n",
       "  'references': ['2105594594',\n",
       "   '1553004968',\n",
       "   '2135346934',\n",
       "   '2171850596',\n",
       "   '2099070536',\n",
       "   '2086699924',\n",
       "   '2077574412',\n",
       "   '2131877510',\n",
       "   '2120163503',\n",
       "   '1988925211'],\n",
       "  'title': 'Hidden Markov Model Based Word Recognition and Its Application to Legal Amount Reading on French Checks'},\n",
       " {'abstract': '',\n",
       "  'authors': ['L. G. Ungerleider'],\n",
       "  'date': '1982',\n",
       "  'identifier': '1523723941',\n",
       "  'references': ['2159929956',\n",
       "   '2153791616',\n",
       "   '2136518234',\n",
       "   '2150375089',\n",
       "   '2118615399',\n",
       "   '2169134378',\n",
       "   '2101790396',\n",
       "   '2109995759',\n",
       "   '2152871995',\n",
       "   '1984214648'],\n",
       "  'title': 'Two cortical visual systems'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Takeo Kanade ', ' Hideo Saito ', ' Sundar Vedula'],\n",
       "  'date': '1998',\n",
       "  'identifier': '2144855601',\n",
       "  'references': ['2042418341',\n",
       "   '1770284412',\n",
       "   '2167953651',\n",
       "   '1816382305',\n",
       "   '2146131512',\n",
       "   '2149901450'],\n",
       "  'title': 'The 3D Room: Digitizing Time-Varying 3D Events by Synchronized Multiple Video Streams'},\n",
       " {'abstract': 'Relevance feedback is an automatic process, introduced over 20 years ago, designed to produce query formulations following an initial retrieval operation. The principal relevance feedback methods described over the years are examined briefly, and evaluation data are included to demonstrate the effectiveness of the various methods. Prescriptions are given for conducting text retrieval operations iteratively using relevance feedback.',\n",
       "  'authors': ['Gerard Salton ', ' Chris Buckley'],\n",
       "  'date': '1997',\n",
       "  'identifier': '2000672666',\n",
       "  'references': ['1978394996',\n",
       "   '2043909051',\n",
       "   '2019087979',\n",
       "   '2164547069',\n",
       "   '2026937586',\n",
       "   '2053039612',\n",
       "   '120261275',\n",
       "   '2030350774',\n",
       "   '2174678383',\n",
       "   '1555286071'],\n",
       "  'title': 'Improving Retrieval Performance by Relevance Feedback'},\n",
       " {'abstract': 'We present a method to learn and recognize object class models from unlabeled and unsegmented cluttered scenes in a scale invariant manner. Objects are modeled as flexible constellations of parts. A probabilistic representation is used for all aspects of the object: shape, appearance, occlusion and relative scale. An entropy-based feature detector is used to select regions and their scale within the image. In learning the parameters of the scale-invariant object model are estimated. This is done using expectation-maximization in a maximum-likelihood setting. In recognition, this model is used in a Bayesian manner to classify images. The flexible nature of the model is demonstrated by excellent results over a range of datasets including geometrically constrained classes (e.g. faces, cars) and flexible objects (such as animals).',\n",
       "  'authors': ['R. Fergus 1', ' P. Perona 2', ' A. Zisserman 1'],\n",
       "  'date': '2003',\n",
       "  'identifier': '2154422044',\n",
       "  'references': ['2164598857',\n",
       "   '2217896605',\n",
       "   '2049633694',\n",
       "   '2119747362',\n",
       "   '2109200236',\n",
       "   '2159686933',\n",
       "   '2155511848',\n",
       "   '1949116567',\n",
       "   '2160225842',\n",
       "   '1699734612'],\n",
       "  'title': 'Object class recognition by unsupervised scale-invariant learning'},\n",
       " {'abstract': 'In this final installment of the paper we consider the case where the signals or the messages or both are continuously variable, in contrast with the discrete nature assumed until now. To a considerable extent the continuous case can be obtained through a limiting process from the discrete case by dividing the continuum of messages and signals into a large but finite number of small regions and calculating the various parameters involved on a discrete basis. As the size of the regions is decreased these parameters in general approach as limits the proper values for the continuous case. There are, however, a few new effects that appear and also a general change of emphasis in the direction of specialization of the general results to particular cases.',\n",
       "  'authors': ['C. E. Shannon'],\n",
       "  'date': '1948',\n",
       "  'identifier': '1995875735',\n",
       "  'references': ['2054692642',\n",
       "   '2076063813',\n",
       "   '1660562555',\n",
       "   '2150498905',\n",
       "   '1964357740',\n",
       "   '2165363188',\n",
       "   '2106006415',\n",
       "   '2106864314',\n",
       "   '2790166049'],\n",
       "  'title': 'A mathematical theory of communication'},\n",
       " {'abstract': 'It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quautization schemes for 2^{b} quanta, b=1,2, \\\\cdots, 7 , are given numerically for Gaussian and for Laplacian distribution of signal amplitudes.',\n",
       "  'authors': ['S. Lloyd'],\n",
       "  'date': '1982',\n",
       "  'identifier': '2150593711',\n",
       "  'references': ['2004003571', '2324075949', '1995871078', '2068071220'],\n",
       "  'title': 'Least squares quantization in PCM'},\n",
       " {'abstract': '1. Curves: Parametrized Curves. 2. Regular Surfaces: Regular Surfaces Inverse Images of Regular Values. 3. Geometry of the Gauss Map: Definition of the Gauss Map and Its Fundamental Properties. 4. Intrinsic Geometry of Surfaces: Isometrics Conformal Maps. 5. Global Differential Geometry: Rigidity of the Sphere.',\n",
       "  'authors': ['Manfredo Perdigão do Carmo'],\n",
       "  'date': '1976',\n",
       "  'identifier': '1565212980',\n",
       "  'references': ['1804110266',\n",
       "   '1564897360',\n",
       "   '2151721316',\n",
       "   '2113319997',\n",
       "   '2151130155',\n",
       "   '2170167891',\n",
       "   '2079017595',\n",
       "   '1974954013',\n",
       "   '2099893201',\n",
       "   '2107338474'],\n",
       "  'title': 'Differential geometry of curves and surfaces'},\n",
       " {'abstract': 'A new approach toward target representation and localization, the central component in visual tracking of nonrigid objects, is proposed. The feature histogram-based target representations are regularized by spatial masking with an isotropic kernel. The masking induces spatially-smooth similarity functions suitable for gradient-based optimization, hence, the target localization problem can be formulated using the basin of attraction of the local maxima. We employ a metric derived from the Bhattacharyya coefficient as similarity measure, and use the mean shift procedure to perform the optimization. In the presented tracking examples, the new method successfully coped with camera motion, partial occlusions, clutter, and target scale variations. Integration with motion filters and data association techniques is also discussed. We describe only a few of the potential applications: exploitation of background information, Kalman tracking using motion models, and face tracking.',\n",
       "  'authors': ['D. Comaniciu 1', ' V. Ramesh 2', ' P. Meer 2'],\n",
       "  'date': '2003',\n",
       "  'identifier': '2132103241',\n",
       "  'references': ['2170120409',\n",
       "   '2099111195',\n",
       "   '2341283081',\n",
       "   '2160337655',\n",
       "   '2067191022',\n",
       "   '2125838338',\n",
       "   '2313307644',\n",
       "   '2140235142',\n",
       "   '2159128898',\n",
       "   '2126736494'],\n",
       "  'title': 'Kernel-based object tracking'},\n",
       " {'abstract': \"We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.\",\n",
       "  'authors': ['Ian Goodfellow ',\n",
       "   ' David Warde-Farley ',\n",
       "   ' Mehdi Mirza ',\n",
       "   ' Aaron Courville ',\n",
       "   ' Yoshua Bengio'],\n",
       "  'date': '2013',\n",
       "  'identifier': '2294059674',\n",
       "  'references': ['2618530766',\n",
       "   '3118608800',\n",
       "   '2310919327',\n",
       "   '1904365287',\n",
       "   '2912934387',\n",
       "   '2546302380',\n",
       "   '2131241448',\n",
       "   '2335728318',\n",
       "   '2156387975',\n",
       "   '189596042'],\n",
       "  'title': 'Maxout Networks'},\n",
       " {'abstract': 'We propose to shift the goal of recognition from naming to describing. Doing so allows us not only to name familiar objects, but also: to report unusual aspects of a familiar object (“spotty dog”, not just “dog”); to say something about unfamiliar objects (“hairy and four-legged”, not just “unknown”); and to learn how to recognize new objects with few or no visual examples. Rather than focusing on identity assignment, we make inferring attributes the core problem of recognition. These attributes can be semantic (“spotty”) or discriminative (“dogs have it but sheep do not”). Learning attributes presents a major new challenge: generalization across object categories, not just across instances within a category. In this paper, we also introduce a novel feature selection method for learning attributes that generalize well across categories. We support our claims by thorough evaluation that provides insights into the limitations of the standard recognition paradigm of naming and demonstrates the new abilities provided by our attribute-based framework.',\n",
       "  'authors': ['Ali Farhadi ',\n",
       "   ' Ian Endres ',\n",
       "   ' Derek Hoiem ',\n",
       "   ' David Forsyth'],\n",
       "  'date': '2009',\n",
       "  'identifier': '2098411764',\n",
       "  'references': ['2161969291',\n",
       "   '2131846894',\n",
       "   '2154422044',\n",
       "   '2120419212',\n",
       "   '2134270519',\n",
       "   '2108082645',\n",
       "   '2149489787',\n",
       "   '2914746235',\n",
       "   '2171188998',\n",
       "   '2152444902'],\n",
       "  'title': 'Describing objects by their attributes'},\n",
       " {'abstract': 'The feasibility of coding two-dimensional data arrays by first performing a two-dimensional linear transformation on the data and then block quantizing the transformed data is investigated. The Fourier, Hadamard, and Karhunen-Loeve transformations are considered. Theoretical results for Markov data and experimental results for four pictures comparing these transform methods to the standard method of raster scanning, sampling, and pulse-count modulation code are presented.',\n",
       "  'authors': ['A. Habibi ', ' P. Wintz'],\n",
       "  'date': '1971',\n",
       "  'identifier': '2160050411',\n",
       "  'references': ['2109808436',\n",
       "   '2134809980',\n",
       "   '2172074673',\n",
       "   '2110380152',\n",
       "   '1997318623',\n",
       "   '2006704346',\n",
       "   '2133927618',\n",
       "   '2027897266',\n",
       "   '2134897728',\n",
       "   '1983939602'],\n",
       "  'title': 'Image Coding by Linear Transformation and Block Quantization'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Keinosuke Fukunaga'],\n",
       "  'date': '1990',\n",
       "  'identifier': '1770825568',\n",
       "  'references': ['2067191022',\n",
       "   '1992419399',\n",
       "   '1989702938',\n",
       "   '2132103241',\n",
       "   '2132549764',\n",
       "   '3111038685',\n",
       "   '2161160262',\n",
       "   '2108995755',\n",
       "   '2136040699',\n",
       "   '2099509424'],\n",
       "  'title': 'Introduction to statistical pattern recognition (2nd ed.)'},\n",
       " {'abstract': 'Data mining applications place special requirements on clustering algorithms including: the ability to find clusters embedded in subspaces of high dimensional data, scalability, end-user comprehensibility of the results, non-presumption of any canonical data distribution, and insensitivity to the order of input records. We present CLIQUE, a clustering algorithm that satisfies each of these requirements. CLIQUE identifies dense clusters in subspaces of maximum dimensionality. It generates cluster descriptions in the form of DNF expressions that are minimized for ease of comprehension. It produces identical results irrespective of the order in which input records are presented and does not presume any specific mathematical form for data distribution. Through experiments, we show that CLIQUE efficiently finds accurate cluster in large high dimensional datasets.',\n",
       "  'authors': ['Rakesh Agrawal ',\n",
       "   ' Johannes Gehrke ',\n",
       "   ' Dimitrios Gunopulos ',\n",
       "   ' Prabhakar Raghavan'],\n",
       "  'date': '1998',\n",
       "  'identifier': '1977496278',\n",
       "  'references': ['1673310716',\n",
       "   '1971784203',\n",
       "   '2999729612',\n",
       "   '2095897464',\n",
       "   '1553696291',\n",
       "   '3017143921',\n",
       "   '1575476631',\n",
       "   '1770825568',\n",
       "   '2129249398',\n",
       "   '1655990431'],\n",
       "  'title': 'Automatic subspace clustering of high dimensional data for data mining applications'},\n",
       " {'abstract': 'It is shown that from a monocular view of a rigid, textured, curved surface it is possible, in principle, to determine the gradient of the surface at any point, and the motion of the eye relative to it, from the velocity field of the changing retinal image, and its first and second spatial derivatives. The relevant equations are redundant, thus providing a test of the rigidity assumption. They involve, among other observable quantities, the components of shear of the retinal velocity field, suggesting that the visual system may possess specialized channels for computing these components.',\n",
       "  'authors': ['Hugh Christopher Longuet-Higgins 1', ' K. Prazdny 2'],\n",
       "  'date': '1980',\n",
       "  'identifier': '1989701469',\n",
       "  'references': ['1933657216',\n",
       "   '2164934677',\n",
       "   '2035108601',\n",
       "   '2048330959',\n",
       "   '2130355536',\n",
       "   '2117731089',\n",
       "   '2039170454',\n",
       "   '2061645173',\n",
       "   '2041880269',\n",
       "   '1587153270'],\n",
       "  'title': 'The interpretation of a moving retinal image'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Leo Breiman'],\n",
       "  'date': '1996',\n",
       "  'identifier': '2912934387',\n",
       "  'references': ['2331432542',\n",
       "   '3085162807',\n",
       "   '2102201073',\n",
       "   '139959648',\n",
       "   '2111814036',\n",
       "   '1969557815',\n",
       "   '2030748132',\n",
       "   '1482451543',\n",
       "   '1531648066',\n",
       "   '1541145887'],\n",
       "  'title': 'Bagging predictors'},\n",
       " {'abstract': 'Abstract Choice of a classification algorithm is generally based upon a number of factors, among which are availability of software, ease of use, and performance, measured here by overall classification accuracy. The maximum likelihood (ML) procedure is, for many users, the algorithm of choice because of its ready availability and the fact that it does not require an extended training process. Artificial neural networks (ANNs) are now widely used by researchers, but their operational applications are hindered by the need for the user to specify the configuration of the network architecture and to provide values for a number of parameters, both of which affect performance. The ANN also requires an extended training phase. In the past few years, the use of decision trees (DTs) to classify remotely sensed data has increased. Proponents of the method claim that it has a number of advantages over the ML and ANN algorithms. The DT is computationally fast, make no statistical assumptions, and can handle data that are represented on different measurement scales. Software to implement DTs is readily available over the Internet. Pruning of DTs can make them smaller and more easily interpretable, while the use of boosting techniques can improve performance. In this study, separate test and training data sets from two different geographical areas and two different sensors—multispectral Landsat ETM+ and hyperspectral DAIS—are used to evaluate the performance of univariate and multivariate DTs for land cover classification. Factors considered are: the effects of variations in training data set size and of the dimensionality of the feature space, together with the impact of boosting, attribute selection measures, and pruning. The level of classification accuracy achieved by the DT is compared to results from back-propagating ANN and the ML classifiers. Our results indicate that the performance of the univariate DT is acceptably good in comparison with that of other classifiers, except with high-dimensional data. Classification accuracy increases linearly with training data set size to a limit of 300 pixels per class in this case. Multivariate DTs do not appear to perform better than univariate DTs. While boosting produces an increase in classification accuracy of between 3% and 6%, the use of attribute selection methods does not appear to be justified in terms of accuracy increases. However, neither the univariate DT nor the multivariate DT performed as well as the ANN or ML classifiers with high-dimensional data.',\n",
       "  'authors': ['Mahesh Pal ', ' Paul M Mather'],\n",
       "  'date': '2003',\n",
       "  'identifier': '2082874195',\n",
       "  'references': ['1554663460',\n",
       "   '2125055259',\n",
       "   '3085162807',\n",
       "   '2112076978',\n",
       "   '1966280301',\n",
       "   '1578368825',\n",
       "   '2128420091',\n",
       "   '1604329830',\n",
       "   '2035549557',\n",
       "   '1515620500'],\n",
       "  'title': 'An assessment of the effectiveness of decision tree methods for land cover classification'},\n",
       " {'abstract': 'Abstract This paper describes a device-independent diagnostic program called dart. dart differs from previous approaches to diagnosis taken in the Artificial Intelligence community in that it works directly from design descriptions rather than mycin -like symptom-fault rules. dart differs from previous approaches to diagnosis taken in the design-automation community in that it is more general and in many cases more efficient. dart uses a device-independent language for describing devices and a device-independent inference procedure for diagnosis. The resulting generality allows it to be applied to a wide class of devices ranging from digital logic to nuclear reactors. Although this generality engenders some computational overhead on small problems, it facilitates the use of multiple design descriptions and thereby makes possible combinatoric savings that more than offsets this overhead on problems of realistic size.',\n",
       "  'authors': ['Michael R. Genesereth'],\n",
       "  'date': '1984',\n",
       "  'identifier': '1983382292',\n",
       "  'references': ['2110293626',\n",
       "   '2100738443',\n",
       "   '2021337406',\n",
       "   '2033755422',\n",
       "   '1677852443',\n",
       "   '2103766969',\n",
       "   '34539469',\n",
       "   '1895928490',\n",
       "   '2111334369',\n",
       "   '2403798461'],\n",
       "  'title': 'The use of design descriptions in automated diagnosis'},\n",
       " {'abstract': 'We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an extension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm (FISTA) which preserves the computational simplicity of ISTA but with a global rate of convergence which is proven to be significantly better, both theoretically and practically. Initial promising numerical results for wavelet-based image deblurring demonstrate the capabilities of FISTA which is shown to be faster than ISTA by several orders of magnitude.',\n",
       "  'authors': ['Amir Beck 1', ' Marc Teboulle 2'],\n",
       "  'date': '2009',\n",
       "  'identifier': '2100556411',\n",
       "  'references': ['2078204800',\n",
       "   '2798766386',\n",
       "   '2109449402',\n",
       "   '2115706991',\n",
       "   '2028349405',\n",
       "   '2079724595',\n",
       "   '1543439990',\n",
       "   '2006262045',\n",
       "   '2126607811',\n",
       "   '1568307856'],\n",
       "  'title': 'A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems'},\n",
       " {'abstract': 'Haplotype-based methods offer a powerful approach to disease gene mapping, based on the association between causal mutations and the ancestral haplotypes on which they arose. As part of The SNP Consortium Allele Frequency Projects, we characterized haplotype patterns across 51 autosomal regions (spanning 13 megabases of the human genome) in samples from Africa, Europe, and Asia. We show that the human genome can be parsed objectively into haplotype blocks: sizable regions over which there is little evidence for historical recombination and within which only a few common haplotypes are observed. The boundaries of blocks and specific haplotypes they contain are highly correlated across populations. We demonstrate that such haplotype frameworks provide substantial statistical power in association studies of common genetic variation across each region. Our results provide a foundation for the construction of a haplotype map of the human genome, facilitating comprehensive genetic association studies of human disease.',\n",
       "  'authors': ['Stacey B. Gabriel 1',\n",
       "   ' Stephen F. Schaffner 1',\n",
       "   ' Huy Nguyen 1',\n",
       "   ' Jamie M. Moore 1',\n",
       "   ' Jessica Roy 1',\n",
       "   ' Brendan Blumenstiel 1',\n",
       "   ' John Higgins 1',\n",
       "   ' Matthew DeFelice 1',\n",
       "   ' Amy Lochner 1',\n",
       "   ' Maura Faggart 1',\n",
       "   ' Shau Neen Liu-Cordero 1',\n",
       "   ' Charles Rotimi 2',\n",
       "   ' Adebowale Adeyemo 3',\n",
       "   ' Richard Cooper 4',\n",
       "   ' Ryk Ward 5',\n",
       "   ' Eric S. Lander 1',\n",
       "   ' Mark J. Daly 1',\n",
       "   ' David Altshuler 1',\n",
       "   ' 6'],\n",
       "  'date': '2002',\n",
       "  'identifier': '2152664025',\n",
       "  'references': ['2042103448',\n",
       "   '1508985588',\n",
       "   '2330519911',\n",
       "   '2079501979',\n",
       "   '2074754379',\n",
       "   '1765516395',\n",
       "   '1730861791',\n",
       "   '2005935256',\n",
       "   '2107889007',\n",
       "   '2056713533'],\n",
       "  'title': 'The Structure of Haplotype Blocks in the Human Genome'},\n",
       " {'abstract': 'Proposes a new method to calibrate L band polarimetric SAR using the forest data. Model based covariance matrix containing the distortion matrices and the scattering components from the forest were solved using the Pi-SAR data over the Japan uniform forest of Tomakomai, Hokkaido. The result showed that the distortion matrices depend on the function of the incidence angle and the operation timings.',\n",
       "  'authors': ['M. Shimada ', ' T. Tadono ', ' M. Watanabe'],\n",
       "  'date': '2004',\n",
       "  'identifier': '2127730845',\n",
       "  'references': ['2097272115', '2148043654', '2075475458'],\n",
       "  'title': 'Determination of polarimetric calibration parameters of L band SAR using uniform forest data'},\n",
       " {'abstract': 'The introduction of the fast Fourier transform algorithm has led to the development of the Fourier transform image coding technique whereby the two-dimensional Fourier transform of an image is transmitted over a channel rather than the image itself. This devlopement has further led to a related image coding technique in which an image is transformed by a Hadamard matrix operator. The Hadamard matrix is a square array of plus and minus ones whose rows and columns are orthogonal to one another. A high-speed computational algorithm, similar to the fast Fourier transform algorithm, which performs the Hadamard transformation has been developed. Since only real number additions and subtractions are required with the Hadamard transform, an order of magnitude speed advantage is possible compared to the complex number Fourier transform. Transmitting the Hadamard transform of an image rather than the spatial representation of the image provides a potential toleration to channel errors and the possibility of reduced bandwidth transmission.',\n",
       "  'authors': ['W.K. Pratt ', ' J. Kane ', ' H.C. Andrews'],\n",
       "  'date': '1969',\n",
       "  'identifier': '2134809980',\n",
       "  'references': ['2061171222',\n",
       "   '2106445473',\n",
       "   '2955672675',\n",
       "   '2148192447',\n",
       "   '2028045978',\n",
       "   '2904838066',\n",
       "   '1493574560',\n",
       "   '2155112231',\n",
       "   '2076819986',\n",
       "   '2068376470'],\n",
       "  'title': 'Hadamard transform image coding'},\n",
       " {'abstract': 'Decision tree classification algorithms have significant potential for land cover mapping problems and have not been tested in detail by the remote sensing community relative to more conventional pattern recognition techniques such as maximum likelihood classification. In this paper, we present several types of decision tree classification algorithms arid evaluate them on three different remote sensing data sets. The decision tree classification algorithms tested include an univariate decision tree, a multivariate decision tree, and a hybrid decision tree capable of including several different types of classification algorithms within a single decision tree structure. Classification accuracies produced by each of these decision tree algorithms are compared with both maximum likelihood and linear discriminant function classifiers. Results from this analysis show that the decision tree algorithms consistently outperform the maximum likelihood and linear discriminant function classifiers in regard to classf — cation accuracy. In particular, the hybrid tree consistently produced the highest classification accuracies for the data sets tested. More generally, the results from this work show that decision trees have several advantages for remote sensing applications by virtue of their relatively simple, explicit, and intuitive classification structure. Further, decision tree algorithms are strictly nonparametric and, therefore, make no assumptions regarding the distribution of input data, and are flexible and robust with respect to nonlinear and noisy relations among input features and class labels.',\n",
       "  'authors': ['M.A. Friedl 1', ' C.E. Brodley 2'],\n",
       "  'date': '1997',\n",
       "  'identifier': '2035549557',\n",
       "  'references': ['2122410182',\n",
       "   '2125055259',\n",
       "   '3085162807',\n",
       "   '2149706766',\n",
       "   '2128420091',\n",
       "   '2162480849',\n",
       "   '1604329830',\n",
       "   '1515620500',\n",
       "   '2125283600',\n",
       "   '2117329279'],\n",
       "  'title': 'Decision tree classification of land cover from remotely sensed data'},\n",
       " {'abstract': 'The study of networks pervades all of science, from neurobiology to statistical physics. The most basic issues are structural: how does one characterize the wiring diagram of a food web or the Internet or the metabolic network of the bacterium Escherichia coli? Are there any unifying principles underlying their topology? From the perspective of nonlinear dynamics, we would also like to understand how an enormous network of interacting dynamical systems — be they neurons, power stations or lasers — will behave collectively, given their individual dynamics and coupling architecture. Researchers are only now beginning to unravel the structure and dynamics of complex networks.',\n",
       "  'authors': ['Steven H. Strogatz'],\n",
       "  'date': '2001',\n",
       "  'identifier': '2164727176',\n",
       "  'references': ['2112090702',\n",
       "   '2008620264',\n",
       "   '2065769502',\n",
       "   '2061901927',\n",
       "   '1976969221',\n",
       "   '2905110430',\n",
       "   '2125315567',\n",
       "   '2175110005',\n",
       "   '2144885342',\n",
       "   '2104085672'],\n",
       "  'title': 'Exploring complex networks'},\n",
       " {'abstract': 'The purpose of this paper is to characterize a constituent boundary parsing algorithm, using an information-theoretic measure called generalized mutual information, which serves as an alternative to traditional grammar-based parsing methods. This method is based on the hypothesis that constituent boundaries can be extracted from a given sentence (or word sequence) by analyzing the mutual information values of the part of speech n-grams within the sentence. This hypothesis is supported by the performance of an implementation of this parsing algorithm which determines a recursive unlabeled bracketing of unrestricted English text with a relatively low error rate. This paper derives the generalized mutual information statistic, describes the parsing algorithm, and presents results and sample output from the parser.',\n",
       "  'authors': ['David M. Magerman ', ' Mitchell P. Marcus'],\n",
       "  'date': '1990',\n",
       "  'identifier': '2110190189',\n",
       "  'references': ['2099247782',\n",
       "   '1593045043',\n",
       "   '2134237567',\n",
       "   '1483126227',\n",
       "   '2017580301',\n",
       "   '2034274945',\n",
       "   '2126477387'],\n",
       "  'title': 'Parsing a natural language using mutual information statistics'},\n",
       " {'abstract': 'For many applications, the control of a complex nonlinear system can be made easier by modeling the system as a collection of simplified hybrid modes, each representing a particular operating regime. An example of this is the decomposition of complex aerobatic flights into sequences of discrete maneuvers, an approach that has proven very successful for both human piloted and autonomously controlled aircraft. However, a critical step when designing such control systems is to ensure the safety and feasibility of transitions between these maneuvers. This work presents a hybrid dynamics framework for the design of guaranteed safe switching regions and is applied to a quadrotor helicopter performing an autonomous backflip. The regions are constructed using reachable sets calculated via a Hamilton-Jacobi differential game formulation, and experimental results are presented from flight tests on the STARMAC quadrotor platform.',\n",
       "  'authors': ['Jeremy H. Gillula 1',\n",
       "   ' Haomiao Huang 1',\n",
       "   ' Michael P. Vitus 1',\n",
       "   ' Claire J. Tomlin 2'],\n",
       "  'date': '2010',\n",
       "  'identifier': '1969160376',\n",
       "  'references': ['2141666765',\n",
       "   '2116364955',\n",
       "   '2169209873',\n",
       "   '2153219011',\n",
       "   '2337770697',\n",
       "   '2132714442',\n",
       "   '167786883',\n",
       "   '168874853',\n",
       "   '2001006503',\n",
       "   '2153012550'],\n",
       "  'title': 'Design of guaranteed safe maneuvers using reachable sets: Autonomous quadrotor aerobatics in theory and practice'},\n",
       " {'abstract': \"This paper takes the regulation of identity as a focus for examining organizational control. It considers how employees are enjoined to develop self-images and work orientations that are deemed congruent with managerially defined objectives. This focus on identity extends and deepens themes developed within other analyses of normative control. Empirical materials are deployed to illustrate how managerial intervention operates, more or less intentionally and in/effectively, to influence employees' self-constructions in terms of coherence, distinctiveness and commitment. The processual nature of such control is emphasized, arguing that it exists in tension with other intra and extra-organizational claims upon employees' sense of identity in a way that can open a space for forms of micro-emancipation.\",\n",
       "  'authors': ['Mats Alvesson 1', ' Hugh Willmott 2'],\n",
       "  'date': '2002',\n",
       "  'identifier': '2140608320',\n",
       "  'references': ['2116199508',\n",
       "   '2033952602',\n",
       "   '2027320617',\n",
       "   '2048683619',\n",
       "   '1994743485',\n",
       "   '2062571617',\n",
       "   '1965278510',\n",
       "   '2125275681',\n",
       "   '2126549818',\n",
       "   '2006770647'],\n",
       "  'title': 'Identity regulation as organizational control: Producing the appropriate individual'},\n",
       " {'abstract': 'We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models out-perform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy word-level alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.',\n",
       "  'authors': ['Philipp Koehn ', ' Franz Josef Och ', ' Daniel Marcu'],\n",
       "  'date': '2003',\n",
       "  'identifier': '2153653739',\n",
       "  'references': ['2101105183',\n",
       "   '2006969979',\n",
       "   '1508165687',\n",
       "   '1973923101',\n",
       "   '1986543644',\n",
       "   '2116316001',\n",
       "   '1517947178',\n",
       "   '2161792612',\n",
       "   '1549285799',\n",
       "   '2158388102'],\n",
       "  'title': 'Statistical phrase-based translation'},\n",
       " {'abstract': 'Because the user interface in a relatonal data base management system may be decoupled from the storage representation of data, novel, powerful and efficient integrity control schemes are possible. This paper indicates the mechanism being implemented in one relational system to prevent integrity violations which can result from improper updates by a process. Basically each interaction with the data is immediately modified at the query language level to one guaranteed to have no integrity violations. Also, a similar modification technique is indicated to support the use of \"views,\" i.e. relations which are not physically present in the data base but are defined in terms of ones that are.',\n",
       "  'authors': ['Michael Stonebraker'],\n",
       "  'date': '1975',\n",
       "  'identifier': '2103920867',\n",
       "  'references': ['2988119170',\n",
       "   '2012056301',\n",
       "   '2145343598',\n",
       "   '2047499522',\n",
       "   '1505165377',\n",
       "   '2163887589',\n",
       "   '2029241887',\n",
       "   '2085772207',\n",
       "   '2043571746',\n",
       "   '127981436'],\n",
       "  'title': 'Implementation of integrity constraints and views by query modification'},\n",
       " {'abstract': 'Introduction to a Transient World. Fourier Kingdom. Discrete Revolution. Time Meets Frequency. Frames. Wavelet Zoom. Wavelet Bases. Wavelet Packet and Local Cosine Bases. An Approximation Tour. Estimations are Approximations. Transform Coding. Appendix A: Mathematical Complements. Appendix B: Software Toolboxes.',\n",
       "  'authors': ['Stéphane Mallat'],\n",
       "  'date': '1998',\n",
       "  'identifier': '2115755118',\n",
       "  'references': ['1903029394',\n",
       "   '2296616510',\n",
       "   '2129638195',\n",
       "   '2964288706',\n",
       "   '2117853853',\n",
       "   '2099641086',\n",
       "   '2964321699',\n",
       "   '2109449402',\n",
       "   '2136396015'],\n",
       "  'title': 'A wavelet tour of signal processing'},\n",
       " {'abstract': 'This paper describes AutoClass II, a program for automatically discovering (inducing) classes from a database, based on a Bayesian statistical technique which automatically determines the most probable number of classes, their probabilistic descriptions, and the probability that each object is a member of each class. AutoClass has been tested on several large, real databases and has discovered previously unsuspected classes. There is no doubt that these classes represent new phenomena.',\n",
       "  'authors': ['Peter C. Cheeseman ',\n",
       "   ' James Kelly ',\n",
       "   ' Matthew Self ',\n",
       "   ' John C. Stutz ',\n",
       "   ' Will Taylor ',\n",
       "   ' Don Freeman'],\n",
       "  'date': '1993',\n",
       "  'identifier': '1992880122',\n",
       "  'references': ['2049633694',\n",
       "   '2029520384',\n",
       "   '2096059947',\n",
       "   '2037591014',\n",
       "   '3049188988',\n",
       "   '2072533619',\n",
       "   '2989873570',\n",
       "   '2000613518',\n",
       "   '2118570622',\n",
       "   '30272054'],\n",
       "  'title': 'AutoClass: a Bayesian classification system'},\n",
       " {'abstract': 'Abstract We construct a prediction rule on the basis of some data, and then wish to estimate the error rate of this rule in classifying future observations. Cross-validation provides a nearly unbiased estimate, using only the original data. Cross-validation turns out to be related closely to the bootstrap estimate of the error rate. This article has two purposes: to understand better the theoretical basis of the prediction problem, and to investigate some related estimators, which seem to offer considerably improved estimation in small samples.',\n",
       "  'authors': ['Bradley Efron'],\n",
       "  'date': '1983',\n",
       "  'identifier': '2040615655',\n",
       "  'references': ['2117897510',\n",
       "   '2124181495',\n",
       "   '3026721701',\n",
       "   '1976927254',\n",
       "   '2112081648',\n",
       "   '1989898472',\n",
       "   '1992068214',\n",
       "   '1981251392',\n",
       "   '2079100340',\n",
       "   '2090578816'],\n",
       "  'title': 'Estimating the Error Rate of a Prediction Rule: Improvement on Cross-Validation'},\n",
       " {'abstract': 'In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimensional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category.',\n",
       "  'authors': ['Aude Oliva 1', ' Antonio Torralba 2'],\n",
       "  'date': '2001',\n",
       "  'identifier': '1566135517',\n",
       "  'references': ['2117812871',\n",
       "   '2128716185',\n",
       "   '2012352340',\n",
       "   '2130259898',\n",
       "   '2156406284',\n",
       "   '2180838288',\n",
       "   '1524408959',\n",
       "   '2142796031',\n",
       "   '2104825706',\n",
       "   '2167034998'],\n",
       "  'title': 'Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope'},\n",
       " {'abstract': 'Abstract A variety of procedures for change detection based on comparison of multitemporal digital remote sensing data have been developed. An evaluation of results indicates that various procedures of change detection produce different maps of change even in the same environment.',\n",
       "  'authors': ['Ashbindu Singh'],\n",
       "  'date': '1989',\n",
       "  'identifier': '2036798369',\n",
       "  'references': ['2740373864',\n",
       "   '2059432853',\n",
       "   '2063623478',\n",
       "   '2005073772',\n",
       "   '1526740462',\n",
       "   '2134893701',\n",
       "   '1485678107',\n",
       "   '2036479590',\n",
       "   '2005270253',\n",
       "   '2020141522'],\n",
       "  'title': 'Review Article Digital change detection techniques using remotely-sensed data'},\n",
       " {'abstract': 'Every eye movement produces a shift in the visual image on the retina. The receptive field, or retinal response area, of an individual visual neuron moves with the eyes so that after an eye movement it covers a new portion of visual space. For some parietal neurons, the location of the receptive field is shown to shift transiently before an eye movement. In addition, nearly all parietal neurons respond when an eye movement brings the site of a previously flashed stimulus into the receptive field. Parietal cortex both anticipates the retinal consequences of eye movements and updates the retinal coordinates of remembered stimuli to generate a continuously accurate representation of visual space.',\n",
       "  'authors': ['Jean-Rene Duhamel ',\n",
       "   ' Carol L. Colby ',\n",
       "   ' Michael E. Goldberg'],\n",
       "  'date': '1992',\n",
       "  'identifier': '2074854285',\n",
       "  'references': ['1569395231',\n",
       "   '1898788858',\n",
       "   '2188405233',\n",
       "   '2411927066',\n",
       "   '2269963436',\n",
       "   '2406002377',\n",
       "   '2033417793',\n",
       "   '2121663528',\n",
       "   '1976999744',\n",
       "   '2409716789'],\n",
       "  'title': 'The updating of the representation of visual space in parietal cortex by intended eye movements.'},\n",
       " {'abstract': 'An original approach is presented for the localisation of objects in an image which approach is neuronal and has two steps. In the first step, a rough localisation is performed by presenting each pixel with its neighbourhood to a neural net which is able to indicate whether this pixel and its neighbourhood are the image of the search object. This first filter does not discriminate for position. From its result, areas which might contain an image of the object can be selected. In the second step, these areas are presented to another neural net which can determine the exact position of the object in each area. This algorithm is applied to the problem of localising faces in images.',\n",
       "  'authors': ['R. Vaillant 1', ' C. Monrocq 1', ' Y. Le Cun 2'],\n",
       "  'date': '1994',\n",
       "  'identifier': '2056695679',\n",
       "  'references': ['2102605133',\n",
       "   '2310919327',\n",
       "   '2076063813',\n",
       "   '2884561390',\n",
       "   '2565639579',\n",
       "   '2022508996',\n",
       "   '2217896605',\n",
       "   '2161381512'],\n",
       "  'title': 'Original approach for the localisation of objects in images'},\n",
       " {'abstract': 'Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.',\n",
       "  'authors': ['Kishore Papineni ',\n",
       "   ' Salim Roukos ',\n",
       "   ' Todd Ward ',\n",
       "   ' Wei-Jing Zhu'],\n",
       "  'date': '2002',\n",
       "  'identifier': '2101105183',\n",
       "  'references': ['2001810881', '3037252522', '2732923061'],\n",
       "  'title': 'Bleu: a Method for Automatic Evaluation of Machine Translation'},\n",
       " {'abstract': '',\n",
       "  'authors': ['H. B. Barlow'],\n",
       "  'date': '1999',\n",
       "  'identifier': '2912889105',\n",
       "  'references': ['2076063813',\n",
       "   '3110653090',\n",
       "   '2146474141',\n",
       "   '2321533354',\n",
       "   '1520997877',\n",
       "   '2140499889',\n",
       "   '2306706380',\n",
       "   '2964101383'],\n",
       "  'title': 'Unsupervised learning'},\n",
       " {'abstract': 'The ability to cheaply train text classifiers is critical to their use in information retrieval, content analysis, natural language processing, and other tasks involving data which is partly or fully textual. An algorithm for sequential sampling during machine learning of statistical classifiers was developed and tested on a newswire text categorization task. This method, which we call uncertainty sampling, reduced by as much as 500-fold the amount of training data that would have to be manually classified to achieve a given level of effectiveness.',\n",
       "  'authors': ['David D. Lewis ', ' William A. Gale'],\n",
       "  'date': '1994',\n",
       "  'identifier': '2085989833',\n",
       "  'references': ['3015463134',\n",
       "   '1528905581',\n",
       "   '3017143921',\n",
       "   '2000672666',\n",
       "   '2151023586',\n",
       "   '2139709458',\n",
       "   '1513874326',\n",
       "   '2080021732',\n",
       "   '1977182536',\n",
       "   '2088538739'],\n",
       "  'title': 'A sequential algorithm for training text classifiers'},\n",
       " {'abstract': 'Unlike older children and adults, children of less than about 10 years of age remember photographs of faces presented upside down almost as well as those shown upright and are easily fooled by simple disguises. The development at age 10 of the ability to encode orientation-specific configurational aspects of a face may reflect completion of certain maturational changes in the right cerebral hemisphere.',\n",
       "  'authors': ['Susan Carey ', ' Rhea Diamond'],\n",
       "  'date': '1977',\n",
       "  'identifier': '1998186877',\n",
       "  'references': ['2139220163',\n",
       "   '2046442873',\n",
       "   '2056115137',\n",
       "   '2044703390',\n",
       "   '2329045102',\n",
       "   '2022737180',\n",
       "   '2031378282',\n",
       "   '1990279616',\n",
       "   '2009214135',\n",
       "   '1976622121'],\n",
       "  'title': 'From piecemeal to configurational representation of faces'},\n",
       " {'abstract': \"It is already true that Big Data has drawn huge attention from researchers in information sciences, policy and decision makers in governments and enterprises. As the speed of information growth exceeds Moore's Law at the beginning of this new century, excessive data is making great troubles to human beings. However, there are so much potential and highly useful values hidden in the huge volume of data. A new scientific paradigm is born as data-intensive scientific discovery (DISD), also known as Big Data problems. A large number of fields and sectors, ranging from economic and business activities to public administration, from national security to scientific researches in many areas, involve with Big Data problems. On the one hand, Big Data is extremely valuable to produce productivity in businesses and evolutionary breakthroughs in scientific disciplines, which give us a lot of opportunities to make great progresses in many fields. There is no doubt that the future competitions in business productivity and technologies will surely converge into the Big Data explorations. On the other hand, Big Data also arises with many challenges, such as difficulties in data capture, data storage, data analysis and data visualization. This paper is aimed to demonstrate a close-up view about Big Data, including Big Data applications, Big Data opportunities and challenges, as well as the state-of-the-art techniques and technologies we currently adopt to deal with the Big Data problems. We also discuss several underlying methodologies to handle the data deluge, for example, granular computing, cloud computing, bio-inspired computing, and quantum computing. © 2014 Elsevier Inc. All rights reserved.\",\n",
       "  'authors': ['C. L. Philip Chen ', ' Chun-Yang Zhang'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2109574129',\n",
       "  'references': ['1631356911',\n",
       "   '2173213060',\n",
       "   '2136922672',\n",
       "   '2140190241',\n",
       "   '2100495367',\n",
       "   '1554944419',\n",
       "   '2163922914',\n",
       "   '2072128103',\n",
       "   '1981420413',\n",
       "   '1494137514'],\n",
       "  'title': 'Data-intensive applications, challenges, techniques and technologies: A survey on Big Data'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Egon G. Guba 1', ' Yvonna S. Lincoln 2'],\n",
       "  'date': '2005',\n",
       "  'identifier': '2910572681',\n",
       "  'references': [],\n",
       "  'title': 'Paradigmatic Controversies, Contradictions, and Emerging Confluences.'},\n",
       " {'abstract': 'Abstract The wide-baseline stereo problem, i.e. the problem of establishing correspondences between a pair of images taken from different viewpoints is studied. A new set of image elements that are put into correspondence, the so called extremal regions , is introduced. Extremal regions possess highly desirable properties: the set is closed under (1) continuous (and thus projective) transformation of image coordinates and (2) monotonic transformation of image intensities. An efficient (near linear complexity) and practically fast detection algorithm (near frame rate) is presented for an affinely invariant stable subset of extremal regions, the maximally stable extremal regions (MSER). A new robust similarity measure for establishing tentative correspondences is proposed. The robustness ensures that invariants from multiple measurement regions (regions obtained by invariant constructions from extremal regions), some that are significantly larger (and hence discriminative) than the MSERs, may be used to establish tentative correspondences. The high utility of MSERs, multiple measurement regions and the robust metric is demonstrated in wide-baseline experiments on image pairs from both indoor and outdoor scenes. Significant change of scale (3.5×), illumination conditions, out-of-plane rotation, occlusion, locally anisotropic scale change and 3D translation of the viewpoint are all present in the test problems. Good estimates of epipolar geometry (average distance from corresponding points to the epipolar line below 0.09 of the inter-pixel distance) are obtained.',\n",
       "  'authors': ['Jiri Matas 1',\n",
       "   ' Ondrej Chum 2',\n",
       "   ' Martin Urban 2',\n",
       "   ' Tomás Pajdla 2'],\n",
       "  'date': '2004',\n",
       "  'identifier': '2124404372',\n",
       "  'references': [],\n",
       "  'title': 'Robust wide-baseline stereo from maximally stable extremal regions'},\n",
       " {'abstract': 'Influence maximization, defined by Kempe, Kleinberg, and Tardos (2003), is the problem of finding a small set of seed nodes in a social network that maximizes the spread of influence under certain influence cascade models. The scalability of influence maximization is a key factor for enabling prevalent viral marketing in large-scale online social networks. Prior solutions, such as the greedy algorithm of Kempe et al. (2003) and its improvements are slow and not scalable, while other heuristic algorithms do not provide consistently good performance on influence spreads. In this paper, we design a new heuristic algorithm that is easily scalable to millions of nodes and edges in our experiments. Our algorithm has a simple tunable parameter for users to control the balance between the running time and the influence spread of the algorithm. Our results from extensive simulations on several real-world and synthetic networks demonstrate that our algorithm is currently the best scalable solution to the influence maximization problem: (a) our algorithm scales beyond million-sized graphs where the greedy algorithm becomes infeasible, and (b) in all size ranges, our algorithm performs consistently well in influence spread --- it is always among the best algorithms, and in most cases it significantly outperforms all other scalable heuristics to as much as 100%--260% increase in influence spread.',\n",
       "  'authors': ['Wei Chen 1', ' Chi Wang 2', ' Yajun Wang 1'],\n",
       "  'date': '2010',\n",
       "  'identifier': '1984069252',\n",
       "  'references': ['3013264884',\n",
       "   '2402962589',\n",
       "   '2141403143',\n",
       "   '2108858998',\n",
       "   '2042123098',\n",
       "   '2611804663',\n",
       "   '2021314079',\n",
       "   '2143996311',\n",
       "   '2097147952',\n",
       "   '2056609785'],\n",
       "  'title': 'Scalable influence maximization for prevalent viral marketing in large-scale social networks'},\n",
       " {'abstract': '',\n",
       "  'authors': ['George E. Forsythe ',\n",
       "   ' Michael A. Malcolm ',\n",
       "   ' Cleve B. Moler'],\n",
       "  'date': '1977',\n",
       "  'identifier': '1964262399',\n",
       "  'references': [],\n",
       "  'title': 'Computer methods for mathematical computations'},\n",
       " {'abstract': 'A method for estimating the parameters of hidden Markov models of speech is described. Parameter values are chosen to maximize the mutual information between an acoustic observation sequence and the corresponding word sequence. Recognition results are presented comparing this method with maximum likelihood estimation.',\n",
       "  'authors': ['L. Bahl ', ' P. Brown ', ' P. de Souza ', ' R. Mercer'],\n",
       "  'date': '1986',\n",
       "  'identifier': '1877570817',\n",
       "  'references': ['1966812932', '1575431606', '2099074650'],\n",
       "  'title': 'Maximum mutual information estimation of hidden Markov model parameters for speech recognition'},\n",
       " {'abstract': '',\n",
       "  'authors': ['B. W. Silverman'],\n",
       "  'date': '1981',\n",
       "  'identifier': '1576534100',\n",
       "  'references': ['2117897510',\n",
       "   '2751862591',\n",
       "   '2049742719',\n",
       "   '2167641446',\n",
       "   '2014268383',\n",
       "   '61071295',\n",
       "   '2797237606',\n",
       "   '1979665684'],\n",
       "  'title': 'Using Kernel Density Estimates to Investigate Multimodality'},\n",
       " {'abstract': 'Abstract Many existing rule learning systems are computationally expensive on large noisy datasets. In this paper we evaluate the recently-proposed rule learning algorithm IREP on a large and diverse collection of benchmark problems. We show that while IREP is extremely efficient, it frequently gives error rates higher than those of C4.5 and C4.5rules. We then propose a number of modifications resulting in an algorithm RIPPERk that is very competitive with C4.5rules with respect to error rates, but much more efficient on large samples. RIPPERk obtains error rates lower than or equivalent to C4.5rules on 22 of 37 benchmark problems, scales nearly linearly with the number of training examples, and can efficiently process noisy datasets containing hundreds of thousands of examples.',\n",
       "  'authors': ['William W. Cohen'],\n",
       "  'date': '1995',\n",
       "  'identifier': '1670263352',\n",
       "  'references': ['2128420091',\n",
       "   '1999138184',\n",
       "   '1604329830',\n",
       "   '1531743498',\n",
       "   '2111746072',\n",
       "   '2089967664',\n",
       "   '146100937',\n",
       "   '2037689320',\n",
       "   '1510806966',\n",
       "   '165133269'],\n",
       "  'title': 'Fast effective rule induction'},\n",
       " {'abstract': 'A Fast Discrete Cosine Transform algorithm has been developed which provides a factor of six improvement in computational complexity when compared to conventional Discrete Cosine Transform algorithms using the Fast Fourier Transform. The algorithm is derived in the form of matrices and illustrated by a signal-flow graph, which may be readily translated to hardware or software implementations.',\n",
       "  'authors': ['Wen-Hsiung Chen 1', ' C. Smith 2', ' S. Fralick 2'],\n",
       "  'date': '1977',\n",
       "  'identifier': '2105815873',\n",
       "  'references': ['2031614119', '3023802253', '2163889646', '2089997223'],\n",
       "  'title': 'A Fast Computational Algorithm for the Discrete Cosine Transform'},\n",
       " {'abstract': 'Abstract Conceptual clustering has proved an effective means of summarizing data in an understandable manner. However, the recency of the conceptual clustering paradigm has allowed little exploration of conceptual clustering as a means of improving performance. This paper describes results obtained by COBWEB, a conceptual clustering system that organizes data so as to maximize inference abilities. The performance task for COBWEB (and implied for all conceptual clustering systems) generalizes the performance requirements typically associated with the better known task of learning from examples. Furthermore, criteria aimed at improving inference seem compatible with traditional conceptual clustering virtues of conceptual simplicity and comprehensibility.',\n",
       "  'authors': ['Douglas H. Fisher'],\n",
       "  'date': '1987',\n",
       "  'identifier': '30272054',\n",
       "  'references': ['2159047538',\n",
       "   '2070429602',\n",
       "   '167515793',\n",
       "   '2031977678',\n",
       "   '13520770',\n",
       "   '2037591014',\n",
       "   '2277957941',\n",
       "   '1601905586',\n",
       "   '2030647854',\n",
       "   '1564805656'],\n",
       "  'title': 'Conceptual Clustering, Learning from Examples, and Inference'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Günther Meinrath'],\n",
       "  'date': '1998',\n",
       "  'identifier': '2294788433',\n",
       "  'references': ['2752853835',\n",
       "   '2171074980',\n",
       "   '2036432298',\n",
       "   '1979424160',\n",
       "   '2133097426',\n",
       "   '2078841894',\n",
       "   '2006820723',\n",
       "   '2081709688',\n",
       "   '2522940733',\n",
       "   '2050059289'],\n",
       "  'title': 'Aquatic Chemistry of Uranium'},\n",
       " {'abstract': 'In the context of the development of prototypic assessment instruments in the areas of cognition, personality, and adaptive functioning, the issues of standardization, norming procedures, and the important psychometrics of test reliability and validity are evaluated critically. Criteria, guidelines, and simple rules of thumb are provided to assist the clinician faced with the challenge of choosing an appropriate test instrument for a given psychological assessment. Clinicians are often faced with the critical challenge of choosing the most appropriate available test instrument for a given psychological assessment of a child, adolescent, or adult of a particular age, gender, and class of disability. It is the purpose of this report to provide some criteria, guidelines, or simple rules of thumb to aid in this complex scientific decision. As such, it draws upon my experience with issues of test development, standardization, norming procedures, and important psychometrics, namely, test reliability and validity. As I and my colleagues noted in an earlier publication, the major areas of psychological functioning, in the normal development of infants, children, adolescents, adults, and elderly people, include cognitive, academic, personality, and adaptive behaviors (Sparrow, Fletcher, & Cicchetti, 1985). As such, the major examples or applications discussed in this article derive primarily, although not exclusively, from these several areas of human functioning.',\n",
       "  'authors': ['Domenic V. Cicchetti'],\n",
       "  'date': '1994',\n",
       "  'identifier': '2063085086',\n",
       "  'references': ['2164777277',\n",
       "   '2313581450',\n",
       "   '1533171997',\n",
       "   '2141403362',\n",
       "   '2050006242',\n",
       "   '1964100346',\n",
       "   '1989528006',\n",
       "   '2015391954',\n",
       "   '2067495470',\n",
       "   '1483571652'],\n",
       "  'title': 'Guidelines, Criteria, and Rules of Thumb for Evaluating Normed and Standardized Assessment Instruments in Psychology.'},\n",
       " {'abstract': 'This paper outlines a technique for treating input texture images as probability density estimators from which new textures, with similar appearance and structural properties, can be sampled. In a two-phase process, the input texture is first analyzed by measuring the joint occurrence of texture discrimination features at multiple resolutions. In the second phase, a new texture is synthesized by sampling successive spatial frequency bands from the input texture, conditioned on the similar joint occurrence of features at lower spatial frequencies. Textures synthesized with this method more successfully capture the characteristics of input textures than do previous techniques.',\n",
       "  'authors': ['Jeremy S. De Bonet'],\n",
       "  'date': '1997',\n",
       "  'identifier': '2042371054',\n",
       "  'references': ['2103504761',\n",
       "   '1490632837',\n",
       "   '2150920547',\n",
       "   '2162703509',\n",
       "   '2113834765',\n",
       "   '2120420595',\n",
       "   '2019299491',\n",
       "   '2998496342',\n",
       "   '179221956',\n",
       "   '164346725'],\n",
       "  'title': 'Multiresolution sampling procedure for analysis and synthesis of texture images'},\n",
       " {'abstract': '1 The general nature of Monte Carlo methods.- 2 Short resume of statistical terms.- 3 Random, pseudorandom, and quasirandom numbers.- 4 Direct simulation.- 5 General principles of the Monte Carlo method.- 6 Conditional Monte Carlo.- 7 Solution of linear operator equations.- 8 Radiation shielding and reactor criticality.- 9 Problems in statistical mechanics.- 10 Long polymer molecules.- 11 Percolation processes.- 12 Multivariable problems.- References.',\n",
       "  'authors': ['J. M. Hammersley ', ' D. C. Handscomb'],\n",
       "  'date': '1964',\n",
       "  'identifier': '2014208555',\n",
       "  'references': ['3102641634',\n",
       "   '2121863487',\n",
       "   '1997063559',\n",
       "   '1991252559',\n",
       "   '1979104937',\n",
       "   '2432517183',\n",
       "   '2141113219',\n",
       "   '2157082398',\n",
       "   '2136796925'],\n",
       "  'title': 'Monte Carlo methods'},\n",
       " {'abstract': 'This tutorial paper begins with an elementary presentation of the fundamental properties and structure of convolutional codes and proceeds with the development of the maximum likelihood decoder. The powerful tool of generating function analysis is demonstrated to yield for arbitrary codes both the distance properties and upper bounds on the bit error probability for communication over any memoryless channel. Previous results on code ensemble average error probabilities are also derived and extended by these techniques. Finally, practical considerations concerning finite decoding memory, metric representation, and synchronization are discussed.',\n",
       "  'authors': ['A. Viterbi'],\n",
       "  'date': '1971',\n",
       "  'identifier': '2122683098',\n",
       "  'references': ['1991133427',\n",
       "   '2153810958',\n",
       "   '2035227369',\n",
       "   '1515682093',\n",
       "   '1993944611',\n",
       "   '2151814693',\n",
       "   '2087362480',\n",
       "   '2005530146',\n",
       "   '2079363366',\n",
       "   '2148981728'],\n",
       "  'title': 'Convolutional Codes and Their Performance in Communication Systems'},\n",
       " {'abstract': 'Introduction The Accuracy of a Sample Mean Random Samples and Probabilities The Empirical Distribution Function and the Plug-In Principle Standard Errors and Estimated Standard Errors The Bootstrap Estimate of Standard Error Bootstrap Standard Errors: Some Examples More Complicated Data Structures Regression Models Estimates of Bias The Jackknife Confidence Intervals Based on Bootstrap \"Tables\" Confidence Intervals Based on Bootstrap Percentiles Better Bootstrap Confidence Intervals Permutation Tests Hypothesis Testing with the Bootstrap Cross-Validation and Other Estimates of Prediction Error Adaptive Estimation and Calibration Assessing the Error in Bootstrap Estimates A Geometrical Representation for the Bootstrap and Jackknife An Overview of Nonparametric and Parametric Inference Further Topics in Bootstrap Confidence Intervals Efficient Bootstrap Computations Approximate Likelihoods Bootstrap Bioequivalence Discussion and Further Topics Appendix: Software for Bootstrap Computations References',\n",
       "  'authors': ['Bradley Efron 1', ' Robert J. Tibshirani 2'],\n",
       "  'date': '1995',\n",
       "  'identifier': '2331432542',\n",
       "  'references': ['2056279562',\n",
       "   '2119444539',\n",
       "   '1970142344',\n",
       "   '2100805904',\n",
       "   '2151226564',\n",
       "   '222053410',\n",
       "   '2157452732',\n",
       "   '2125837480',\n",
       "   '2165786903',\n",
       "   '2147105902'],\n",
       "  'title': 'An Introduction to the Bootstrap.'},\n",
       " {'abstract': \"This paper deals with applications of Quadrature Mirror Filters (QMF) to coding of voice signal in sub-bands. Use of QMF's enables to avoid the aliasing effects due to samples decimation when signal is split into sub-bands. Each sub-band is then coded independently with use of Block Companded PCM (BCPCM) quantizers. Then a variable number of bits is allocated to each sub-band quantizer in order to take advantage of the relative perceptual effect of the quantizing error. The paper is organized as follows: - First, splitting in two sub-bands with QMF's is analysed. -Then, a general description of a splitband voice coding scheme using QMF's is made. -Finally, two coding schemes are considered, operating respectively at 16 KBps and 32 KBps. Averaged values of S/N performances are given when encoding both male and female voices. Comparisons are made with conventional BCPCM and CCITT A-Law. Taped results will be played at the conference.\",\n",
       "  'authors': ['D. Esteban ', ' C. Galand'],\n",
       "  'date': '1977',\n",
       "  'identifier': '1690240707',\n",
       "  'references': ['2162153812', '1627215994', '3082891365'],\n",
       "  'title': 'Application of quadrature mirror filters to split band voice coding schemes'},\n",
       " {'abstract': \"From the Publisher: Classifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. C4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies. This book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses.\",\n",
       "  'authors': ['J. Ross Quinlan'],\n",
       "  'date': '1992',\n",
       "  'identifier': '2125055259',\n",
       "  'references': ['1484413656',\n",
       "   '2148143831',\n",
       "   '2149684865',\n",
       "   '2112076978',\n",
       "   '3100785508',\n",
       "   '2132549764',\n",
       "   '2142827986',\n",
       "   '2153010521',\n",
       "   '2017337590',\n",
       "   '607505555'],\n",
       "  'title': 'C4.5: Programs for Machine Learning'},\n",
       " {'abstract': 'The paper proposes a Constrained Entity-Alignment F-Measure (CEAF) for evaluating coreference resolution. The metric is computed by aligning reference and system entities (or coreference chains) with the constraint that a system (reference) entity is aligned with at most one reference (system) entity. We show that the best alignment is a maximum bipartite matching problem which can be solved by the Kuhn-Munkres algorithm. Comparative experiments are conducted to show that the widely-known MUC F-measure has serious flaws in evaluating a coreference system. The proposed metric is also compared with the ACE-Value, the official evaluation metric in the Automatic Content Extraction (ACE) task, and we conclude that the proposed metric possesses some properties such as symmetry and better interpretability missing in the ACE-Value.',\n",
       "  'authors': ['Xiaoqiang Luo'],\n",
       "  'date': '2005',\n",
       "  'identifier': '2098345921',\n",
       "  'references': ['2077658674',\n",
       "   '1965693266',\n",
       "   '2131340601',\n",
       "   '2222512263',\n",
       "   '1975675300',\n",
       "   '2141461755',\n",
       "   '2013280707'],\n",
       "  'title': 'On Coreference Resolution Performance Metrics'},\n",
       " {'abstract': 'Statistical procedures for missing data have vastly improved, yet misconception and unsound practice still abound. The authors frame the missing-data problem, review methods, offer advice, and raise issues that remain unresolved. They clear up common misunderstandings regarding the missing at random (MAR) concept. They summarize the evidence against older procedures and, with few exceptions, discourage their use. They present, in both technical and practical language, 2 general approaches that come highly recommended: maximum likelihood (ML) and Bayesian multiple imputation (MI). Newer developments are discussed, including some for dealing with missing data that are not MAR. Although not yet in the mainstream, these procedures may eventually extend the ML and MI methods that currently represent the state of the art.',\n",
       "  'authors': ['Joseph L. Schafer 1', ' John W. Graham 2'],\n",
       "  'date': '2002',\n",
       "  'identifier': '2156267802',\n",
       "  'references': ['2045656233',\n",
       "   '1579271636',\n",
       "   '1512719169',\n",
       "   '2049633694',\n",
       "   '1980911127',\n",
       "   '3015463134',\n",
       "   '2044758663',\n",
       "   '1550443206',\n",
       "   '1528905581',\n",
       "   '2117853077'],\n",
       "  'title': 'Missing data: Our view of the state of the art.'},\n",
       " {'abstract': 'Abstract Uncertainty sampling methods iteratively request class labels for training instances whose classes are uncertain despite the previous labeled instances. These methods can greatly reduce the number of instances that an expert need label. One problem with this approach is that the classifier best suited for an application may be too expensive to train or use during the selection of instances. We test the use of one classifier (a highly efficient probabilistic one) to select examples for training another (the C4.5 rule induction program). Despite being chosen by this heterogeneous approach, the uncertainty samples yielded classifiers with lower error rates than random samples ten times larger.',\n",
       "  'authors': ['David D. Lewis ', ' Jason Catlett'],\n",
       "  'date': '1994',\n",
       "  'identifier': '1513874326',\n",
       "  'references': ['3085162807',\n",
       "   '3015463134',\n",
       "   '1528905581',\n",
       "   '1594031697',\n",
       "   '1504694836',\n",
       "   '1833785989',\n",
       "   '2139709458',\n",
       "   '2080021732',\n",
       "   '1977182536',\n",
       "   '2088538739'],\n",
       "  'title': 'Heterogenous uncertainty sampling for supervised learning'},\n",
       " {'abstract': '',\n",
       "  'authors': ['S S Stevens'],\n",
       "  'date': '1958',\n",
       "  'identifier': '2071206549',\n",
       "  'references': ['1984314602',\n",
       "   '1778685146',\n",
       "   '2029654180',\n",
       "   '2063252428',\n",
       "   '2783254024',\n",
       "   '2781567914',\n",
       "   '2038371355',\n",
       "   '2315997189',\n",
       "   '2321557395',\n",
       "   '2796175200'],\n",
       "  'title': 'Problems and methods of psychophysics.'},\n",
       " {'abstract': 'The principle that, for optimal retrieval, documents should be ranked in order of the probability of relevance or usefulness has been brought into question by Cooper. It is shown that the principle can be justified under certain assumptions, but that in cases where these assumptions do not hold, the principle is not valid. The major problem appears to lie in the way the principle considers each document independently of the rest. The nature of the information on the basis of which the system decides whether or not to retrieve the documents determines whether the document‐by‐document approach is valid.',\n",
       "  'authors': ['S. E. Robertson'],\n",
       "  'date': '1997',\n",
       "  'identifier': '2048045485',\n",
       "  'references': [],\n",
       "  'title': 'The probability ranking principle in IR'},\n",
       " {'abstract': 'The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.',\n",
       "  'authors': ['Y. LeCun ',\n",
       "   ' B. Boser ',\n",
       "   ' J. S. Denker ',\n",
       "   ' D. Henderson ',\n",
       "   ' R. E. Howard ',\n",
       "   ' W. Hubbard ',\n",
       "   ' L. D. Jackel'],\n",
       "  'date': '1989',\n",
       "  'identifier': '2147800946',\n",
       "  'references': ['2154642048',\n",
       "   '2165758113',\n",
       "   '169539560',\n",
       "   '19621276',\n",
       "   '2101926813',\n",
       "   '2157475639',\n",
       "   '1965770722',\n",
       "   '56903235',\n",
       "   '2606594511',\n",
       "   '2116360511'],\n",
       "  'title': 'Backpropagation applied to handwritten zip code recognition'},\n",
       " {'abstract': 'In this paper we describe an in-depth study on some data misclassified by a collection of classifiers produced by different authors. First of all, we divide the errors into three categories based on their quality and analyze their distributions according to category. Common errors made by three or more classifiers out of five have been identified and analyzed to deduce the reasons of misclassification. Finally, based on systematic analyses, two possible solutions to reduce errors and improve system reliability are proposed: (a) a verification module, and (b) combination of complementary multiple classifiers.',\n",
       "  'authors': ['Ching Y. Suen ', ' Jinna Tan'],\n",
       "  'date': '2005',\n",
       "  'identifier': '2010332406',\n",
       "  'references': ['2310919327',\n",
       "   '2159737176',\n",
       "   '2101706260',\n",
       "   '2014986661',\n",
       "   '1568787085',\n",
       "   '1757231607',\n",
       "   '2104766934',\n",
       "   '1936922812',\n",
       "   '2092405628',\n",
       "   '1517108682'],\n",
       "  'title': 'Analysis of errors of handwritten digits made by a multitude of classifiers'},\n",
       " {'abstract': 'A pattern-recognition method, making use of Fourier transformations to extract features which are significant for a pattern, is described. The ordinary Fourier coefficients are difficult to use as input to categorizers because they contain factors dependent upon size and rotation as well as an arbitrary phase angle. From these Fourier coefficients, however, other more useful features can easily be derived. By using these derived property constants, a distinction can be made between genuine shape constants and constants representing size, location, and orientation. The usefulness of the method has been tested with a computer program that was used to classify 175 samples of handprinted letters, e.g., 7 sets of the 25 letters A to Z. In this test, 98 percent were correctly recognized when a simple nonoptimized decision method was used. The last section contains some considerations of the technical realizability of a fast preprocessing system for reading printed text.',\n",
       "  'authors': ['G. H. Granlund'],\n",
       "  'date': '1972',\n",
       "  'identifier': '1966591781',\n",
       "  'references': ['2065973527',\n",
       "   '2115235609',\n",
       "   '2163200437',\n",
       "   '2076432184',\n",
       "   '2038363923'],\n",
       "  'title': 'Fourier Preprocessing for Hand Print Character Recognition'},\n",
       " {'abstract': 'While computer-mediated communication use and research are proliferating rapidly, findings offer contrasting images regarding the interpersonal character of this technology. Research trends over the history of these media are reviewed with observations across trends suggested so as to provide integrative principles with which to apply media to different circumstances. First, the notion that the media reduce personal influences—their impersonal effects—is reviewed. Newer theories and research are noted explaining normative “interpersonal” uses of the media. From this vantage point, recognizing that impersonal communication is sometimes advantageous, strategies for the intentional depersonalization of media use are inferred, with implications for Group Decision Support Systems effects. Additionally, recognizing that media sometimes facilitate communication that surpasses normal interpersonal levels, a new perspective on “hyperpersonal” communication is introduced. Subprocesses are discussed pertaining to re...',\n",
       "  'authors': ['Joseph B. Walther'],\n",
       "  'date': '1996',\n",
       "  'identifier': '2576297379',\n",
       "  'references': ['2116339812',\n",
       "   '2340966270',\n",
       "   '2108752510',\n",
       "   '2024372407',\n",
       "   '2098572648',\n",
       "   '2038281539',\n",
       "   '1573558308',\n",
       "   '1524086313',\n",
       "   '1595891490',\n",
       "   '2042846381'],\n",
       "  'title': 'Computer-Mediated Communication Impersonal, Interpersonal, and Hyperpersonal Interaction'},\n",
       " {'abstract': \"Abstract : It is widely believed that to efficiently represent an otherwise smooth object with discontinuities along edges, one must use an adaptive representation that in some sense 'tracks' the shape of the discontinuity set. This folk-belief - some would say folk-theorem - is incorrect. At the very least, the possible quantitative advantage of such adaptation is vastly smaller than commonly believed. We have recently constructed a tight frame of curvelets which provides stable, efficient, and near-optimal representation of otherwise smooth objects having discontinuities along smooth curves. By applying naive thresholding to the curvelet transform of such an object, one can form m-term approximations with rate of L(sup 2) approximation rivaling the rate obtainable by complex adaptive schemes which attempt to track' the discontinuity set. In this article we explain the basic issues of efficient m-term approximation, the construction of efficient adaptive representation, the construction of the curvelet frame, and a crude analysis of the performance of curvelet schemes.\",\n",
       "  'authors': ['Emmanuel J. Candes ', ' David L. Donoho'],\n",
       "  'date': '2000',\n",
       "  'identifier': '1485280399',\n",
       "  'references': ['2156447271',\n",
       "   '2101789093',\n",
       "   '2109504624',\n",
       "   '574370508',\n",
       "   '2033367330',\n",
       "   '2031299600',\n",
       "   '1556281046',\n",
       "   '2337715572',\n",
       "   '2017366455',\n",
       "   '1534852844'],\n",
       "  'title': 'Curvelets: A Surprisingly Effective Nonadaptive Representation for Objects with Edges'},\n",
       " {'abstract': 'In this paper, we study the linking patterns and discussion topics of political bloggers. Our aim is to measure the degree of interaction between liberal and conservative blogs, and to uncover any differences in the structure of the two communities. Specifically, we analyze the posts of 40 \"A-list\" blogs over the period of two months preceding the U.S. Presidential Election of 2004, to study how often they referred to one another and to quantify the overlap in the topics they discussed, both within the liberal and conservative communities, and also across communities. We also study a single day snapshot of over 1,000 political blogs. This snapshot captures blogrolls (the list of links to other blogs frequently found in sidebars), and presents a more static picture of a broader blogosphere. Most significantly, we find differences in the behavior of liberal and conservative blogs, with conservative blogs linking to each other more frequently and in a denser pattern.',\n",
       "  'authors': ['Lada A. Adamic 1', ' Natalie Glance 2'],\n",
       "  'date': '2005',\n",
       "  'identifier': '2152284345',\n",
       "  'references': ['1880262756',\n",
       "   '2061901927',\n",
       "   '1972645849',\n",
       "   '2144799688',\n",
       "   '2136407110',\n",
       "   '1547561528',\n",
       "   '1583729138',\n",
       "   '2162783380',\n",
       "   '2118009636',\n",
       "   '161681269'],\n",
       "  'title': 'The political blogosphere and the 2004 U.S. election: divided they blog'},\n",
       " {'abstract': '1. Introduction. 2. Partitioning Around Medoids (Program PAM). 3. Clustering large Applications (Program CLARA). 4. Fuzzy Analysis. 5. Agglomerative Nesting (Program AGNES). 6. Divisive Analysis (Program DIANA). 7. Monothetic Analysis (Program MONA). Appendix 1. Implementation and Structure of the Programs. Appendix 2. Running the Programs. Appendix 3. Adapting the Programs to Your Needs. Appendix 4. The Program CLUSPLOT. References. Author Index. Subject Index.',\n",
       "  'authors': ['Leonard Kaufman ', ' Peter J. Rousseeuw'],\n",
       "  'date': '1990',\n",
       "  'identifier': '2999729612',\n",
       "  'references': ['2140190241',\n",
       "   '2067191022',\n",
       "   '2011430131',\n",
       "   '2165835468',\n",
       "   '1673310716',\n",
       "   '2032230795',\n",
       "   '2153233077',\n",
       "   '2162088497',\n",
       "   '1966327575',\n",
       "   '2155707112'],\n",
       "  'title': 'Finding Groups in Data: An Introduction to Cluster Analysis'},\n",
       " {'abstract': 'A new method of estimating the entropy and redundancy of a language is described. This method exploits the knowledge of the language statistics possessed by those who speak the language, and depends on experimental results in prediction of the next letter when the preceding text is known. Results of experiments in prediction are given, and some properties of an ideal predictor are developed.',\n",
       "  'authors': ['C. E. Shannon'],\n",
       "  'date': '1951',\n",
       "  'identifier': '2079145130',\n",
       "  'references': ['2158899491',\n",
       "   '2126419817',\n",
       "   '2076257979',\n",
       "   '2104583100',\n",
       "   '1515040589',\n",
       "   '2054626033',\n",
       "   '1966812932',\n",
       "   '2912780347',\n",
       "   '2138879556'],\n",
       "  'title': 'Prediction and entropy of printed English'},\n",
       " {'abstract': 'The high lights of the history of color measurement and of color photography are reviewed. Following this introduction, the principles of modern 3-color colorimetry are developed from a hypothetical experiment in color matching. The conventional theory of \"perfect color reproduction\" by color television is built up from colorimetric background. Some of the difficulties to be expected in applying colorimetry to color television are brought out. Finally, there is some discussion which tends to show that colorimetry may not be a sufficiently powerful tool to provide answers to all of the questions which will arise in the reproduction of scenes in color by television. The advantage of colorimetry as a background is indicated, however.',\n",
       "  'authors': ['W. T. Wintringham'],\n",
       "  'date': '1951',\n",
       "  'identifier': '2044913388',\n",
       "  'references': ['2318643436',\n",
       "   '1969591126',\n",
       "   '2077956501',\n",
       "   '2321689380',\n",
       "   '2328411590',\n",
       "   '2013755574',\n",
       "   '2061017297',\n",
       "   '2171965325',\n",
       "   '2003382909',\n",
       "   '2063659390'],\n",
       "  'title': 'Color Television and Colorimetry'},\n",
       " {'abstract': 'For many pattern recognition tasks, the ideal input feature would be invariant to multiple confounding properties (such as illumination and viewing angle, in computer vision applications). Recently, deep architectures trained in an unsupervised manner have been proposed as an automatic method for extracting useful features. However, it is difficult to evaluate the learned features by any means other than using them in a classifier. In this paper, we propose a number of empirical tests that directly measure the degree to which these learned features are invariant to different input transformations. We find that stacked autoencoders learn modestly increasingly invariant features with depth when trained on natural images. We find that convolutional deep belief networks learn substantially more invariant features in each layer. These results further justify the use of \"deep\" vs. \"shallower\" representations, but suggest that mechanisms beyond merely stacking one autoencoder on top of another may be important for achieving invariance. Our evaluation metrics can also be used to evaluate future work in deep learning, and thus help the development of future algorithms.',\n",
       "  'authors': ['Ian Goodfellow ',\n",
       "   ' Honglak Lee ',\n",
       "   ' Quoc V. Le ',\n",
       "   ' Andrew Saxe ',\n",
       "   ' Andrew Y. Ng'],\n",
       "  'date': '2009',\n",
       "  'identifier': '2105728138',\n",
       "  'references': ['2136922672',\n",
       "   '2110798204',\n",
       "   '2130325614',\n",
       "   '2166049352',\n",
       "   '1498436455',\n",
       "   '2134557905',\n",
       "   '1994197834',\n",
       "   '2122922389',\n",
       "   '2147800946',\n",
       "   '2139427956'],\n",
       "  'title': 'Measuring Invariances in Deep Networks'},\n",
       " {'abstract': 'In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).',\n",
       "  'authors': ['Michael Collins'],\n",
       "  'date': '1997',\n",
       "  'identifier': '1986543644',\n",
       "  'references': ['1632114991',\n",
       "   '1773803948',\n",
       "   '2110882317',\n",
       "   '2153439141',\n",
       "   '2052449326',\n",
       "   '2093647425',\n",
       "   '1972573551',\n",
       "   '2087165009',\n",
       "   '2069912724',\n",
       "   '2162455891'],\n",
       "  'title': 'Three Generative, Lexicalised Models for Statistical Parsing'},\n",
       " {'abstract': 'An introduction is given to a theory of early visual information processing. The theory has been implemented, and examples are given of images at various stages of analysis. It is argued that the first step of consequence is to compute a primitive but rich description of the grey-level changes present in an image. The description is expressed in a vocabulary of kinds of intensity change (EDGE, SHADING-EDGE, EXTENDED-EDGE, LINE, BLOB etc.). Modifying parameters are bound to the elements in the description, specifying their POSITION, ORIENTATION, TERMINATION points, CONTRAST, SIZE and FUZZINESS. This description is obtained from the intensity array by fixed techniques, and it is called the primal sketch. For most images, the primal sketch is large and unwieldy. The second important step in visual information processing is to group its contents in a way that is appropriate for later recognition. From our ability to interpret drawings with little semantic content, one may infer the presence in our perceptual equipment of symbolic processes that can define \"place-tokens\" in an image in various ways, and can group them according to certain rules. Homomorphic techniques fail to account for many of these grouping phenomena, whose explanations require mechanisms of construction rather than mechanisms of detection. The necessary grouping of elements in the primal sketch may be achieved by a mechanism that has available the processes inferred from above, together with the ability to select items by first order discriminations acting on the elements\\' parameters. Only occasionally do these mechanisms use downward-flowing information about the contents of the particular image being processed. It is argued that \"non-attentive\" vision is in practice implemented by these grouping operations and first order discriminations acting on the primal sketch. The class of computations so obtained differs slightly from the class of second order operations on the intensity array. The extraction of a form from the primal sketch using these techniques amounts to the separation of figure from ground. It is concluded that most of the separation can be carried out by using techniques that do not depend upon the particular image in question. Therefore, figure-ground separation can normally precede the description of the shape of the extracted form. Up to this point, higher-level knowledge and purpose are brought to bear on only a few of the decisions taken during the processing. This relegates the widespread use of downward-flowing information to a later stage than is found in current machine-vision programs, and implies that such knowledge should influence the control of, rather than interfering with, the actual data-processing that is taking place lower down.',\n",
       "  'authors': ['David Marr'],\n",
       "  'date': '1976',\n",
       "  'identifier': '2130355536',\n",
       "  'references': ['1968245656',\n",
       "   '2116360511',\n",
       "   '1980429329',\n",
       "   '2028750032',\n",
       "   '2119051448',\n",
       "   '2057683929',\n",
       "   '2017600612',\n",
       "   '2066548055',\n",
       "   '1967767463',\n",
       "   '2070616620'],\n",
       "  'title': 'Early processing of visual information'},\n",
       " {'abstract': 'Safe handling of dynamic highway and inner city scenarios with autonomous vehicles involves the problem of generating traffic-adapted trajectories. In order to account for the practical requirements of the holistic autonomous system, we propose a semi-reactive trajectory generation method, which can be tightly integrated into the behavioral layer. The method realizes long-term objectives such as velocity keeping, merging, following, stopping, in combination with a reactive collision avoidance by means of optimal-control strategies within the Frenet-Frame [12] of the street. The capabilities of this approach are demonstrated in the simulation of a typical high-speed highway scenario.',\n",
       "  'authors': ['Moritz Werling 1',\n",
       "   ' Julius Ziegler 1',\n",
       "   ' Soren Kammel 2',\n",
       "   ' Sebastian Thrun 3'],\n",
       "  'date': '2010',\n",
       "  'identifier': '2107338474',\n",
       "  'references': ['2121806728',\n",
       "   '2000359213',\n",
       "   '131069610',\n",
       "   '1971998222',\n",
       "   '1565212980',\n",
       "   '2136445059',\n",
       "   '2127005930',\n",
       "   '2144293566',\n",
       "   '2103639171',\n",
       "   '2151104204'],\n",
       "  'title': 'Optimal trajectory generation for dynamic street scenarios in a Frenét Frame'},\n",
       " {'abstract': 'Introduction to Response Surface Methodology. The Use of Graduating Functions. Least Squares for Response Surface Work. Factorial Designs at Two Levels. Blocking and Fractionating 2 k Factorial Designs. The Use of Steepest Ascent to Achieve System Improvement. Fitting Second--Order Models. Adequacy of Estimation and the Use of Transformation. Exploration of Maxima and Ridge Systems with Second--Order Response Surfaces. Occurrence and Elucidation of Ridge Systems, I. Occurrence and Elucidation of Ridge Systems, II. Links Between Emprirical and Theoretical Models. Design Aspects of Variance, Bias, and Lack of Fit. Variance----Optimal Designs. Practical Choice of a Response Surface Design. Subject Index. Index.',\n",
       "  'authors': ['George E P Box ', ' Norman R Draper'],\n",
       "  'date': '1987',\n",
       "  'identifier': '2000836282',\n",
       "  'references': ['1968226729'],\n",
       "  'title': 'Empirical Model-Building and Response Surfaces'},\n",
       " {'abstract': 'We propose CornerNet, a new approach to object detection where we detect an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution neural network. By detecting objects as paired keypoints, we eliminate the need for designing a set of anchor boxes commonly used in prior single-stage detectors. In addition to our novel formulation, we introduce corner pooling, a new type of pooling layer that helps the network better localize corners. Experiments show that CornerNet achieves a 42.2% AP on MS COCO, outperforming all existing one-stage detectors.',\n",
       "  'authors': ['Hei Law ', ' Jia Deng'],\n",
       "  'date': '2020',\n",
       "  'identifier': '2886335102',\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '2962835968',\n",
       "   '639708223',\n",
       "   '1836465849',\n",
       "   '2102605133',\n",
       "   '2108598243',\n",
       "   '3106250896',\n",
       "   '1677182931',\n",
       "   '2963037989'],\n",
       "  'title': 'CornerNet: Detecting Objects as Paired Keypoints'},\n",
       " {'abstract': 'Quality inspection of food and agricultural produce are difficult and labor intensive. Simultaneously, with increased expectations for food products of high quality and safety standards, the need for accurate, fast and objective quality determination of these characteristics in food products continues to grow. However, these operations generally in India are manual which is costly as well as unreliable because human decision in identifying quality factors such as appearance, flavor, nutrient, texture, etc., is inconsistent, subjective and slow. Machine vision provides one alternative for an automated, non-destructive and cost-effective technique to accomplish these requirements. This inspection approach based on image analysis and processing has found a variety of different applications in the food industry. Considerable research has highlighted its potential for the inspection and grading of fruits and vegetables, grain quality and characteristic examination and quality evaluation of other food products like bakery products, pizza, cheese, and noodles etc. The objective of this paper is to provide in depth introduction of machine vision system, its components and recent work reported on food and agricultural produce.',\n",
       "  'authors': ['Krishna Kumar Patel 1',\n",
       "   ' A. Kar 1',\n",
       "   ' S. N. Jha 2',\n",
       "   ' M. A. Khan 3'],\n",
       "  'date': '2012',\n",
       "  'identifier': '2064782300',\n",
       "  'references': ['2145023731',\n",
       "   '2963623381',\n",
       "   '3111446822',\n",
       "   '2116044718',\n",
       "   '2740373864',\n",
       "   '1575739010',\n",
       "   '2145106362',\n",
       "   '2059432853',\n",
       "   '3100816363',\n",
       "   '2178432768'],\n",
       "  'title': 'Machine vision system: a tool for quality inspection of food and agricultural products'},\n",
       " {'abstract': 'Abstract For many years people have speculated that electroencephalographic activity or other electrophysiological measures of brain function might provide a new non-muscular channel for sending messages and commands to the external world – a brain–computer interface (BCI). Over the past 15 years, productive BCI research programs have arisen. Encouraged by new understanding of brain function, by the advent of powerful low-cost computer equipment, and by growing recognition of the needs and potentials of people with disabilities, these programs concentrate on developing new augmentative communication and control technology for those with severe neuromuscular disorders, such as amyotrophic lateral sclerosis, brainstem stroke, and spinal cord injury. The immediate goal is to provide these users, who may be completely paralyzed, or ‘locked in’, with basic communication capabilities so that they can express their wishes to caregivers or even operate word processing programs or neuroprostheses. Present-day BCIs determine the intent of the user from a variety of different electrophysiological signals. These signals include slow cortical potentials, P300 potentials, and mu or beta rhythms recorded from the scalp, and cortical neuronal activity recorded by implanted electrodes. They are translated in real-time into commands that operate a computer display or other device. Successful operation requires that the user encode commands in these signals and that the BCI derive the commands from the signals. Thus, the user and the BCI system need to adapt to each other both initially and continually so as to ensure stable performance. Current BCIs have maximum information transfer rates up to 10–25 bits/min. This limited capacity can be valuable for people whose severe disabilities prevent them from using conventional augmentative communication methods. At the same time, many possible applications of BCI technology, such as neuroprosthesis control, may require higher information transfer rates. Future progress will depend on: recognition that BCI research and development is an interdisciplinary problem, involving neurobiology, psychology, engineering, mathematics, and computer science; identification of those signals, whether evoked potentials, spontaneous rhythms, or neuronal firing rates, that users are best able to control independent of activity in conventional motor output pathways; development of training methods for helping users to gain and maintain that control; delineation of the best algorithms for translating these signals into device commands; attention to the identification and elimination of artifacts such as electromyographic and electro-oculographic activity; adoption of precise and objective procedures for evaluating BCI performance; recognition of the need for long-term as well as short-term assessment of BCI performance; identification of appropriate BCI applications and appropriate matching of applications and users; and attention to factors that affect user acceptance of augmentative technology, including ease of use, cosmesis, and provision of those communication and control capacities that are most important to the user. Development of BCI technology will also benefit from greater emphasis on peer-reviewed research publications and avoidance of the hyperbolic and often misleading media attention that tends to generate unrealistic expectations in the public and skepticism in other researchers. With adequate recognition and effective engagement of all these issues, BCI systems could eventually provide an important new communication and control option for those with motor disabilities and might also give those without disabilities a supplementary control channel or a control channel useful in special circumstances.',\n",
       "  'authors': ['Jonathan R Wolpaw 1',\n",
       "   ' 2',\n",
       "   ' Niels Birbaumer 3',\n",
       "   ' 4',\n",
       "   ' Dennis J McFarland 1',\n",
       "   ' Gert Pfurtscheller 5',\n",
       "   ' Theresa M Vaughan 1'],\n",
       "  'date': '2002',\n",
       "  'identifier': '2106006415',\n",
       "  'references': ['2132549764',\n",
       "   '1583327662',\n",
       "   '2116308679',\n",
       "   '2145302786',\n",
       "   '1543237449',\n",
       "   '2140413964',\n",
       "   '2990598036',\n",
       "   '2168689324',\n",
       "   '2023295798',\n",
       "   '2093534731'],\n",
       "  'title': 'Brain-computer interfaces for communication and control.'},\n",
       " {'abstract': \"This paper differs from previous examinations of organizational learning in that it is broader in scope and more evaluative of the literatures. Four constructs related to organizational learning knowledge acquisition, information distribution, information interpretation, and organizational memory are articulated, and the literatures related to each are described and critiqued. The literature on knowledge acquisition is voluminous and multi-faceted, and so the knowledge acquisition construct is portrayed here as consisting of five subconstructs or subprocesses: 1 drawing on knowledge available at the organization's birth, 2 learning from experience, 3 learning by observing other organizations, 4 grafting on to itself components that possess knowledge needed but not possessed by the organization, and 5 noticing or searching for information about the organization's environment and performance. Examination of the related literatures indicates that much has been learned about learning from experience, but also that there is a lack of cumulative work and a lack of integration of work from different research groups. Similarly, much has been learned about organizational search, but there is a lack of conceptual work, and there is a lack of both cumulative work and syntheses with which to create a more mature literature. Congenital learning, vicarious learning, and grafting are information acquisition subprocesses about which relatively little has been learned. The literature concerning information distribution is rich and mature, but an aspect of information distribution that is central to an organization's benefitting from its learning, namely how units that possess information and units that need this information can find each other quickly and with a high likelihood, is unexplored. Information interpretation, as an organizational process, rather than an individual process, requires empirical work for further advancement. Organizational memory is much in need of systematic investigation, particularly by those whose special concerns are improving organizational learning and decision making.\",\n",
       "  'authors': ['George P. Huber'],\n",
       "  'date': '1991',\n",
       "  'identifier': '2106919063',\n",
       "  'references': ['2045181804',\n",
       "   '2137358449',\n",
       "   '3021161106',\n",
       "   '2096452841',\n",
       "   '1965278510',\n",
       "   '1874939829',\n",
       "   '2083449279',\n",
       "   '2949851968',\n",
       "   '2724224207',\n",
       "   '1524086313'],\n",
       "  'title': 'Organizational Learning: The Contributing Processes and the Literatures'},\n",
       " {'abstract': '',\n",
       "  'authors': ['S. S. Stevens ', ' E. H. Galanter'],\n",
       "  'date': '1957',\n",
       "  'identifier': '2063252428',\n",
       "  'references': ['1984314602',\n",
       "   '2029654180',\n",
       "   '2783254024',\n",
       "   '1970846337',\n",
       "   '1965733974',\n",
       "   '2781567914',\n",
       "   '2057720927',\n",
       "   '2315997189',\n",
       "   '648465177',\n",
       "   '2104926117'],\n",
       "  'title': 'Ratio scales and category scales for a dozen perceptual continua.'},\n",
       " {'abstract': '1. Introduction to algebraic graph theory Part I. Linear Algebra in Graphic Thoery: 2. The spectrum of a graph 3. Regular graphs and line graphs 4. Cycles and cuts 5. Spanning trees and associated structures 6. The tree-number 7. Determinant expansions 8. Vertex-partitions and the spectrum Part II. Colouring Problems: 9. The chromatic polynomial 10. Subgraph expansions 11. The multiplicative expansion 12. The induced subgraph expansion 13. The Tutte polynomial 14. Chromatic polynomials and spanning trees Part III. Symmetry and Regularity: 15. Automorphisms of graphs 16. Vertex-transitive graphs 17. Symmetric graphs 18. Symmetric graphs of degree three 19. The covering graph construction 20. Distance-transitive graphs 21. Feasibility of intersection arrays 22. Imprimitivity 23. Minimal regular graphs with given girth References Index.',\n",
       "  'authors': ['Norman L. Biggs'],\n",
       "  'date': '1974',\n",
       "  'identifier': '1515707356',\n",
       "  'references': ['2008263650',\n",
       "   '2035098448',\n",
       "   '2045967420',\n",
       "   '2094078070',\n",
       "   '1963899800',\n",
       "   '2025318306',\n",
       "   '2118132754',\n",
       "   '2332424617',\n",
       "   '2032811360',\n",
       "   '2078370438'],\n",
       "  'title': 'Algebraic Graph Theory'},\n",
       " {'abstract': \"This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with SURF's application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline SURF's usefulness in a broad range of topics in computer vision.\",\n",
       "  'authors': ['Herbert Bay 1',\n",
       "   ' Andreas Ess 1',\n",
       "   ' Tinne Tuytelaars 2',\n",
       "   ' Luc Van Gool 1'],\n",
       "  'date': '2008',\n",
       "  'identifier': '2119605622',\n",
       "  'references': ['2151103935',\n",
       "   '2164598857',\n",
       "   '1677409904',\n",
       "   '2124386111',\n",
       "   '2177274842',\n",
       "   '2128017662',\n",
       "   '2154422044',\n",
       "   '2012778485',\n",
       "   '1625255723',\n",
       "   '1980911747'],\n",
       "  'title': 'Speeded-Up Robust Features (SURF)'},\n",
       " {'abstract': 'The movement parallax field due to the translation of an observer relative to a plane surface is studied in an infinitesimal neighborhood of a visual direction. The parallax field is decomposed into elementary transformations: a translation, a rigid rotation, a similarity, and a deformation. A topologically invariant classification based on critical-point analysis is also obtained. It is shown that the field is either that of a node or that of a saddle point. Numerical results for a general case are offered as illustration. We discuss the relevance of the local, as opposed to the global structure of the parallax field for visual perception and the visual space sense.',\n",
       "  'authors': ['J. J. Koenderink ', ' A. J. van Doorn'],\n",
       "  'date': '1976',\n",
       "  'identifier': '2061645173',\n",
       "  'references': ['1594551768', '2041880269', '2045631168', '2314185503'],\n",
       "  'title': 'Local structure of movement parallax of the plane'},\n",
       " {'abstract': 'AutoDock Vina, a new program for molecular docking and virtual screening, is presented. AutoDock Vina achieves an approximately two orders of magnitude speed-up compared with the molecular docking software previously developed in our lab (AutoDock 4), while also significantly improving the accuracy of the binding mode predictions, judging by our tests on the training set used in AutoDock 4 development. Further speed-up is achieved from parallelism, by using multithreading on multicore machines. AutoDock Vina automatically calculates the grid maps and clusters the results in a way transparent to the user.',\n",
       "  'authors': ['Oleg Trott ', ' Arthur J. Olson'],\n",
       "  'date': '2009',\n",
       "  'identifier': '2134967712',\n",
       "  'references': ['2152195021',\n",
       "   '3029645440',\n",
       "   '1497256448',\n",
       "   '2105668062',\n",
       "   '2581275558',\n",
       "   '2109364787',\n",
       "   '2158534713',\n",
       "   '2152964937',\n",
       "   '2092285329',\n",
       "   '2128332459'],\n",
       "  'title': 'AutoDock Vina: Improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading'},\n",
       " {'abstract': 'We are given a large database of customer transactions. Each transaction consists of items purchased by a customer in a visit. We present an efficient algorithm that generates all significant association rules between items in the database. The algorithm incorporates buffer management and novel estimation and pruning techniques. We also present results of applying this algorithm to sales data obtained from a large retailing company, which shows the effectiveness of the algorithm.',\n",
       "  'authors': ['Rakesh Agrawal 1', ' Tomasz Imieliński 2', ' Arun Swami 1'],\n",
       "  'date': '1993',\n",
       "  'identifier': '2166559705',\n",
       "  'references': ['3085162807',\n",
       "   '2149706766',\n",
       "   '1594031697',\n",
       "   '2100406636',\n",
       "   '1601529450',\n",
       "   '2019363670',\n",
       "   '1556507321',\n",
       "   '1487801850',\n",
       "   '2100176599',\n",
       "   '2323746689'],\n",
       "  'title': 'Mining association rules between sets of items in large databases'},\n",
       " {'abstract': 'A vector quantizer is a system for mapping a sequence of continuous or discrete vectors into a digital sequence suitable for communication over or storage in a digital channel. The goal of such a system is data compression: to reduce the bit rate so as to minimize communication channel capacity or digital storage memory requirements while maintaining the necessary fidelity of the data. The mapping for each vector may or may not have memory in the sense of depending on past actions of the coder, just as in well established scalar techniques such as PCM, which has no memory, and predictive quantization, which does. Even though information theory implies that one can always obtain better performance by coding vectors instead of scalars, scalar quantizers have remained by far the most common data compression system because of their simplicity and good performance when the communication rate is sufficiently large. In addition, relatively few design techniques have existed for vector quantizers. During the past few years several design algorithms have been developed for a variety of vector quantizers and the performance of these codes has been studied for speech waveforms, speech linear predictive parameter vectors, images, and several simulated random processes. It is the purpose of this article to survey some of these design techniques and their applications.',\n",
       "  'authors': ['R. Gray'],\n",
       "  'date': '1984',\n",
       "  'identifier': '2913399920',\n",
       "  'references': ['2134383396',\n",
       "   '2127218421',\n",
       "   '1995875735',\n",
       "   '2583466288',\n",
       "   '2142228262',\n",
       "   '2021760654',\n",
       "   '2164240509',\n",
       "   '2119352491',\n",
       "   '2040336387',\n",
       "   '2044002522'],\n",
       "  'title': 'Vector quantization'},\n",
       " {'abstract': 'By symmetry, P has eigenvalues 1 = I03 > I381 > ?> I 31xI- 1 2 -1. This paper develops methods for getting upper and lower bounds on 8i3 by comparison with a second reversible chain on the same state space. This extends the ideas introduced in Diaconis and Saloff-Coste (1993), where random walks on finite groups were considered. The bounds involve geometric properties such as the diameter and covering number of an associated graph along the lines of Diaconis and Stroock (1991). The main application gives a sharp upper bound on the second eigenvalue of the symmetric exclusion process. Thus, let S0 be a connected undirected graph with n vertices. For simplicity, we assume in this introduction that SW is regular. To start, r unlabelled particles are placed in an initial configuration, 1 < r < n. At each step, a particle is chosen at random; then one of the neighboring sites of this particle is chosen at random. If the neighboring site is unoccupied, the chosen particle is moved there; if the neighboring site is occupied, the system stays as it was. This is a reversible Markov chain on the r-sets of {1, 2, . . ., n} with uniform stationary distribution. Liggett (1985) gives background and motivation (he focuses on infinite systems). Fill (1991) gives bounds on the second eigenvalue of the labeled exclusion process on the finite circle ZZn 1 We study this chain by comparison with a second Markov chain on r-sets that proceeds by picking a particle at random, picking an unoccupied site at random (not necessarily a neighboring site) and moving the particle to the unoccupied site. This is a well studied chain (the Bernoulli-Laplace model for diffusion). Its eigenvalues are known. We show that the comparison techniques apply to give upper bounds on the eigenvalues of the exclusion',\n",
       "  'authors': ['Persi Diaconis ', ' Laurent Saloff-Coste'],\n",
       "  'date': '1993',\n",
       "  'identifier': '1976418263',\n",
       "  'references': ['2610857016',\n",
       "   '1521233381',\n",
       "   '1596709027',\n",
       "   '2751862591',\n",
       "   '2043694962',\n",
       "   '1515707356',\n",
       "   '2069092471',\n",
       "   '2065163799',\n",
       "   '2056389572',\n",
       "   '2086490380'],\n",
       "  'title': 'COMPARISON THEOREMS FOR REVERSIBLE MARKOV CHAINS'},\n",
       " {'abstract': 'Two direction finding algorithms are presented for nonGaussian signals, which are based on the fourth-order cumulants of the data received by the array. The first algorithm is similar to MUSIC, while the second is asymptotically minimum variance in a certain sense. The first algorithm requires singular value decomposition of the cumulant matrix, while the second is based on nonlinear minimization of a certain cost function. The performance of the minimum variance algorithm can be assessed by analytical means, at least for the case of discrete probability distributions of the source signals and spatially uncorrelated Gaussian noise. The numerical experiments performed seem to confirm the insensitivity of these algorithms to the (Gaussian) noise parameters. >',\n",
       "  'authors': ['B. Porat ', ' B. Friedlander'],\n",
       "  'date': '1991',\n",
       "  'identifier': '2140352766',\n",
       "  'references': ['2113638573',\n",
       "   '2149755721',\n",
       "   '2140728947',\n",
       "   '2106382018',\n",
       "   '2149817982',\n",
       "   '2091104692',\n",
       "   '2135629581',\n",
       "   '1863047762',\n",
       "   '2137977844',\n",
       "   '1855208394'],\n",
       "  'title': 'Direction finding algorithms based on high-order statistics'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Peter No'],\n",
       "  'date': '1986',\n",
       "  'identifier': '2186435531',\n",
       "  'references': ['2172073677',\n",
       "   '2053691921',\n",
       "   '2015370045',\n",
       "   '2171759622',\n",
       "   '2124718941',\n",
       "   '2167197102',\n",
       "   '2166223512',\n",
       "   '2053645446',\n",
       "   '2110964282',\n",
       "   '1511982185'],\n",
       "  'title': 'Digital Coding of Waveforms'},\n",
       " {'abstract': 'Several theories and much experimental research on relational tone in computer-mediated communication (CMC) points to the lack of nonverbal cues in this channel as a cause of impersonal and task-oriented messages. Field research in CMC often reports more positive relational behavior. This article examines the assumptions, methods, and findings of such research and suggests that negative relational effects are confined to narrow situational boundary conditions. Alternatively, it is suggested that communicators develop individuating impressions of others through accumulated CMC messages. Based upon these impressions, users may develop relationships and express multidimensional relational messages through verbal or textual cues. Predictions regarding these processes are suggested, and future research incorporating these points is urged.',\n",
       "  'authors': ['Joseph B. Walther'],\n",
       "  'date': '1992',\n",
       "  'identifier': '2098572648',\n",
       "  'references': ['2108752510',\n",
       "   '2038281539',\n",
       "   '1524086313',\n",
       "   '1595891490',\n",
       "   '1975273460',\n",
       "   '1552483424',\n",
       "   '2032633302',\n",
       "   '1578743203',\n",
       "   '2023380077',\n",
       "   '2012959741'],\n",
       "  'title': 'Interpersonal Effects in Computer-Mediated Interaction: A Relational Perspective'},\n",
       " {'abstract': 'Nature-inspired algorithms are among the most powerful algorithms for optimization. This paper intends to provide a detailed description of a new Firefly Algorithm (FA) for multimodal optimization applications. We will compare the proposed firefly algorithm with other metaheuristic algorithms such as particle swarm optimization (PSO). Simulations and results indicate that the proposed firefly algorithm is superior to existing metaheuristic algorithms. Finally we will discuss its applications and implications for further research.',\n",
       "  'authors': ['Xin-She Yang'],\n",
       "  'date': '2009',\n",
       "  'identifier': '1523741643',\n",
       "  'references': ['2152195021',\n",
       "   '1639032689',\n",
       "   '2904250082',\n",
       "   '2126554879',\n",
       "   '1596914020',\n",
       "   '2122122715',\n",
       "   '1480600971',\n",
       "   '2119899283',\n",
       "   '2001471353',\n",
       "   '2963434200'],\n",
       "  'title': 'Firefly algorithms for multimodal optimization'},\n",
       " {'abstract': 'Nearly all super-resolution algorithms are based on the fundamental constraints that the super-resolution image should generate low resolution input images when appropriately warped and down-sampled to model the image formation process. (These reconstruction constraints are normally combined with some form of smoothness prior to regularize their solution.) We derive a sequence of analytical results which show that the reconstruction constraints provide less and less useful information as the magnification factor increases. We also validate these results empirically and show that, for large enough magnification factors, any smoothness prior leads to overly smooth results with very little high-frequency content. Next, we propose a super-resolution algorithm that uses a different kind of constraint in addition to the reconstruction constraints. The algorithm attempts to recognize local features in the low-resolution images and then enhances their resolution in an appropriate manner. We call such a super-resolution algorithm a hallucination or reconstruction algorithm. We tried our hallucination algorithm on two different data sets, frontal images of faces and printed Roman text. We obtained significantly better results than existing reconstruction-based algorithms, both qualitatively and in terms of RMS pixel error.',\n",
       "  'authors': ['S. Baker ', ' T. Kanade'],\n",
       "  'date': '2002',\n",
       "  'identifier': '2140257560',\n",
       "  'references': ['2170120409',\n",
       "   '2313307644',\n",
       "   '2217896605',\n",
       "   '2149760002',\n",
       "   '2911709767',\n",
       "   '2103504761',\n",
       "   '2140257560',\n",
       "   '1991605728',\n",
       "   '1938714998',\n",
       "   '2042371054'],\n",
       "  'title': 'Limits on super-resolution and how to break them'},\n",
       " {'abstract': 'Probability * Introduction * Models in General * The Frequency Approach Rejected * The Single Event Model * Symmetry as the Measure of Probability * Independence * Subsets of a Sample Space * Conditional Probability * Randomness * Critique of the Model Some Mathematical Tools * Permutations * Combinations * The Binomial DistributionBernoulli Trials * Random Variables, Mean and the Expected Value * The Variance * The Generating Function * The Weak Law of Large Numbers * The Statistical Assignment of Probability * The Representation of Information Methods for Solving Problems * The Five Methods * The Total Sample Space and Fair Games * Enumeration * Historical Approach * Recursive Approach * Recursive Approach * The Method of Random Variables * Critique of the Notion of a Fair Game * Bernoulli Evaluation * Robustness * InclusionExclusion Principle Countably Infinite Sample Spaces * Introduction * Bernoulli Trials * On the Strategy to be Adopted * State Diagrams * Generating Functions of State Diagrams * Expanding a Rational Generating Function * Checking the Solution * Paradoxes Continuous Sample Spaces * A Philosophy of the Real Number System * Some First Examples * Some Paradoxes * The Normal Distribution * The Distribution of Numbers * Convergence to the Reciprocal Distribution * Random Times * Dead Times * Poisson Distribution in Time * Queing Theorem * Birth and Death Systems * Summary Uniform Probability Assignments Maximum Entropy * What is Entropy? * Shannons Entropy * Some Mathematical Properties of the Entropy Function * Some Simple Applications * The Maximum Entropy Principle Models of Probability * General Remarks * Maximum Likelihood in a Binary Choice * Von Mises Probability * The Mathematical Approach * The Statistical Approach * When The Mean Does Not Exist * Probability as an Extension of Logic * Di Finetti * Subjective Probability * Fuzzy Probability * Probability in Science * Complex Probability Some Limit Theorems * The Biomial Approximation for the case p=1/2 * Approximation by the Normal Distribution * Another Derivation of the Normal Distribution * Random Times * The Zipf Distribution * Summary An Essay on Simulation',\n",
       "  'authors': ['Richard W. Hamming'],\n",
       "  'date': '1991',\n",
       "  'identifier': '2044437392',\n",
       "  'references': ['1506281249',\n",
       "   '2096335861',\n",
       "   '2054847121',\n",
       "   '3088557317',\n",
       "   '2116056128',\n",
       "   '2051758472',\n",
       "   '2962838571',\n",
       "   '2560434059',\n",
       "   '2059343208',\n",
       "   '204027641'],\n",
       "  'title': 'The Art Of Probability'},\n",
       " {'abstract': 'Rough set plays vital role to overcome the complexities, vagueness, uncertainty, imprecision, and incomplete data during features analysis. Classification is tested on certain dataset that maintain an exact class and review process where key attributes decide the class positions. To assess efficient and automated learning, algorithms are used over training datasets. Generally, classification is supervised learning whereas clustering is unsupervised. Classifications under mathematical models deal with mining rules and machine learning. The Objective of this work is to establish a strong theoretical and manual analysis among three popular classifier namely K-nearest neighbor K-NN, Naive Bayes and Apriori algorithm. Hybridization with rough sets among these three classifiers enables enable to address larger datasets. Performances of three classifiers have tested in absence and presence of rough sets. This work is in the phase of implementation for DNA Deoxyribonucleic Acid datasets and it will design automated system to assess classifier under machine learning environment.',\n",
       "  'authors': ['Shamim H Ripon 1',\n",
       "   ' Sarwar Kamal 1',\n",
       "   ' Saddam Hossain 1',\n",
       "   ' Nilanjan Dey 2'],\n",
       "  'date': '2016',\n",
       "  'identifier': '2415856871',\n",
       "  'references': ['2130479394',\n",
       "   '2166559705',\n",
       "   '2340020088',\n",
       "   '2142183404',\n",
       "   '1970156673',\n",
       "   '1971403296',\n",
       "   '2151350595',\n",
       "   '2143451122',\n",
       "   '2002438422',\n",
       "   '2094613397'],\n",
       "  'title': 'Theoretical Analysis of Different Classifiers under Reduction Rough Data Set: A Brief Proposal'},\n",
       " {'abstract': \"The InterPro database (http://www.ebi.ac.uk/interpro/) integrates together predictive models or 'signatures' representing protein domains, families and functional sites from multiple, diverse source databases: Gene3D, PANTHER, Pfam, PIRSF, PRINTS, ProDom, PROSITE, SMART, SUPERFAMILY and TIGRFAMs. Integration is performed manually and approximately half of the total approximately 58,000 signatures available in the source databases belong to an InterPro entry. Recently, we have started to also display the remaining un-integrated signatures via our web interface. Other developments include the provision of non-signature data, such as structural data, in new XML files on our FTP site, as well as the inclusion of matchless UniProtKB proteins in the existing match XML files. The web interface has been extended and now links out to the ADAN predicted protein-protein interaction database and the SPICE and Dasty viewers. The latest public release (v18.0) covers 79.8% of UniProtKB (v14.1) and consists of 16 549 entries. InterPro data may be accessed either via the web address above, via web services, by downloading files by anonymous FTP or by using the InterProScan search software (http://www.ebi.ac.uk/Tools/InterProScan/).\",\n",
       "  'authors': ['Sarah Hunter 1',\n",
       "   ' Rolf Apweiler ',\n",
       "   ' Teresa K. Attwood 2',\n",
       "   ' Amos Bairoch ',\n",
       "   ' Alex Bateman ',\n",
       "   ' David Binns ',\n",
       "   ' Peer Bork ',\n",
       "   ' Ujjwal Das ',\n",
       "   ' Louise C. Daugherty ',\n",
       "   ' Lauranne Duquenne ',\n",
       "   ' Robert D. Finn ',\n",
       "   ' Julian Gough 2',\n",
       "   ' Daniel H. Haft ',\n",
       "   ' Nicolas Hulo ',\n",
       "   ' Daniel Kahn 3',\n",
       "   ' Elizabeth Kelly ',\n",
       "   ' Aurélie Laugraud ',\n",
       "   ' Ivica Letunic ',\n",
       "   ' David Lonsdale ',\n",
       "   ' Rodrigo Lopez ',\n",
       "   ' Martin Madera ',\n",
       "   ' John Maslen ',\n",
       "   ' Craig McAnulla ',\n",
       "   ' Jennifer McDowall ',\n",
       "   ' Jaina Mistry ',\n",
       "   ' Alex L. Mitchell ',\n",
       "   ' Nicola J. Mulder ',\n",
       "   ' Darren A. Natale ',\n",
       "   ' Christine A. Orengo ',\n",
       "   ' Antony F. Quinn ',\n",
       "   ' Jeremy D. Selengut ',\n",
       "   ' Christian J. A. Sigrist ',\n",
       "   ' Manjula Thimma ',\n",
       "   ' Paul D. Thomas ',\n",
       "   ' Franck Valentin ',\n",
       "   ' Derek Wilson ',\n",
       "   ' Cathy H. Wu ',\n",
       "   ' Corin Yeats'],\n",
       "  'date': '2009',\n",
       "  'identifier': '2116423958',\n",
       "  'references': ['2141885858',\n",
       "   '2161062388',\n",
       "   '1527927437',\n",
       "   '2091448229',\n",
       "   '2109872885',\n",
       "   '2097698606',\n",
       "   '2124285343',\n",
       "   '2097817417',\n",
       "   '2131736388',\n",
       "   '2100990314'],\n",
       "  'title': 'InterPro: the integrative protein signature database'},\n",
       " {'abstract': 'From the Publisher: A valuable reference for the novice as well as for the expert who needs a wider scope of coverage within the area of cryptography, this book provides easy and rapid access of information and includes more than 200 algorithms and protocols; more than 200 tables and figures; more than 1,000 numbered definitions, facts, examples, notes, and remarks; and over 1,250 significant references, including brief comments on each paper.',\n",
       "  'authors': ['Alfred J. Menezes ',\n",
       "   ' Scott A. Vanstone ',\n",
       "   ' Paul C. Van Oorschot'],\n",
       "  'date': '1996',\n",
       "  'identifier': '1660562555',\n",
       "  'references': ['2752885492',\n",
       "   '2011039300',\n",
       "   '2168676717',\n",
       "   '2052267638',\n",
       "   '2913419753',\n",
       "   '1606480398',\n",
       "   '2131300413',\n",
       "   '2010939995',\n",
       "   '2106539366',\n",
       "   '1496801598'],\n",
       "  'title': 'Handbook of Applied Cryptography'},\n",
       " {'abstract': 'Given a set of objects in a scene whose identifications are ambiguous, it is often possible to use relationships among the objects to reduce or eliminate the ambiguity. A striking example of this approach was given by Waltz [13]. This paper formulates the ambiguity-reduction process in terms of iterated parallel operations (i.e., relaxation operations) performed on an array of (object, identification) data. Several different models of the process are developed, convergence properties of these models are established, and simple examples are given.',\n",
       "  'authors': ['Azriel Rosenfeld ', ' Robert A. Hummel ', ' Steven W. Zucker'],\n",
       "  'date': '1976',\n",
       "  'identifier': '1979622972',\n",
       "  'references': ['2042119491', '2018646682', '2489172749', '2018030206'],\n",
       "  'title': 'Scene Labeling by Relaxation Operations'},\n",
       " {'abstract': 'Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.',\n",
       "  'authors': ['Douglas Bates ',\n",
       "   ' Martin Mächler ',\n",
       "   ' Benjamin M. Bolker ',\n",
       "   ' Steven C. Walker'],\n",
       "  'date': '2015',\n",
       "  'identifier': '1951724000',\n",
       "  'references': ['2582743722',\n",
       "   '2172199324',\n",
       "   '2045656233',\n",
       "   '1981457167',\n",
       "   '1587094587',\n",
       "   '2097879961',\n",
       "   '3086315876',\n",
       "   '2068047481',\n",
       "   '1520511539',\n",
       "   '2165693128'],\n",
       "  'title': 'Fitting Linear Mixed-Effects Models Using lme4'},\n",
       " {'abstract': 'Handprinted characters can be made more uniform in appearance than the as-written version if an appropriate linear transformation is performed on each input pattern. The transformation can be implemented electronically by programming a flying-spot raster-scanner to scan at specified angles rather than only along specified axes. Alternatively, curve-follower normalization can be achieved by transforming the coordinate waveforms in a linear combining network. Second-order moments of the pattern are convenient properties to use in specifying the transformation. By mapping the original pattern into one having a scalar moment matrix all linear pattern variations can be removed. Comparison experiments with three sets of handprinted numerals showed that error rates were reduced by integral factors if the patterns were normalized before scanning for recognition.',\n",
       "  'authors': ['R. G. Casey'],\n",
       "  'date': '1970',\n",
       "  'identifier': '2002448074',\n",
       "  'references': ['2028931872',\n",
       "   '2027299546',\n",
       "   '2056551493',\n",
       "   '2035609259',\n",
       "   '644374200',\n",
       "   '2104469847'],\n",
       "  'title': 'Moment normalization of handprinted characters'},\n",
       " {'abstract': 'We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.',\n",
       "  'authors': ['Nal Kalchbrenner ', ' Phil Blunsom'],\n",
       "  'date': '2013',\n",
       "  'identifier': '1753482797',\n",
       "  'references': ['2146502635',\n",
       "   '2117130368',\n",
       "   '179875071',\n",
       "   '2132339004',\n",
       "   '1889268436',\n",
       "   '2006969979',\n",
       "   '2171928131',\n",
       "   '2103305545',\n",
       "   '2251222643',\n",
       "   '196214544'],\n",
       "  'title': 'Recurrent Continuous Translation Models'},\n",
       " {'abstract': '',\n",
       "  'authors': ['A. Grossmann ', ' J. Morlet'],\n",
       "  'date': '1985',\n",
       "  'identifier': '42253355',\n",
       "  'references': ['2160642098',\n",
       "   '2063615912',\n",
       "   '2087377426',\n",
       "   '2014757225',\n",
       "   '2130047773',\n",
       "   '2010398978',\n",
       "   '2090231318',\n",
       "   '2074870412',\n",
       "   '2116200167',\n",
       "   '2133442910'],\n",
       "  'title': 'DECOMPOSITION OF FUNCTIONS INTO WAVELETS OF CONSTANT SHAPE, AND RELATED TRANSFORMS'},\n",
       " {'abstract': 'Boosting is one of the most important recent developments in classification methodology. Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced. For many classification algorithms, this simple strategy results in dramatic improvements in performance. We show that this seemingly mysterious phenomenon can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood. For the two-class problem, boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion. We develop more direct approximations and show that they exhibit nearly identical results to boosting. Direct multiclass generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multiclass generalizations of boosting in most situations, and far superior in some. We suggest a minor modification to boosting that can reduce computation, often by factors of 10 to 50. Finally, we apply these insights to produce an alternative formulation of boosting decision trees. This approach, based on best-first truncated tree induction, often leads to better performance, and can provide interpretable descriptions of the aggregate decision rule. It is also much faster computationally, making it more suitable to large-scale data mining applications.',\n",
       "  'authors': ['Jerome Friedman ', ' Trevor Hastie ', ' Robert Tibshirani'],\n",
       "  'date': '2000',\n",
       "  'identifier': '2024046085',\n",
       "  'references': ['1678356000',\n",
       "   '1540007258',\n",
       "   '2099968818',\n",
       "   '139959648',\n",
       "   '1881647329',\n",
       "   '2141518341'],\n",
       "  'title': 'Additive Logistic Regression : A Statistical View of Boosting'},\n",
       " {'abstract': 'We establish that the feedback capacity of the trapdoor channel is the logarithm of the golden ratio and provide a simple communication scheme that achieves capacity. As part of the analysis, we formulate a class of dynamic programs that characterize capacities of unifilar finite-state channels. The trapdoor channel is an instance that admits a simple closed-form solution.',\n",
       "  'authors': ['H. Permuter ', ' P. Cuff ', ' B. Van Roy ', ' T. Weissman'],\n",
       "  'date': '2008',\n",
       "  'identifier': '2163389339',\n",
       "  'references': ['2156216606',\n",
       "   '1558937370',\n",
       "   '2024886389',\n",
       "   '1579764623',\n",
       "   '2175624993',\n",
       "   '2947000318',\n",
       "   '2165397914',\n",
       "   '1967037758',\n",
       "   '1489262227',\n",
       "   '2118917401'],\n",
       "  'title': 'Capacity of the Trapdoor Channel With Feedback'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Dietmar Jannach 1',\n",
       "   ' Paul Resnick 2',\n",
       "   ' Alexander Tuzhilin 3',\n",
       "   ' Markus Zanker 4'],\n",
       "  'date': '2016',\n",
       "  'identifier': '2545577367',\n",
       "  'references': ['3093900339',\n",
       "   '2110325612',\n",
       "   '2155106456',\n",
       "   '3111571306',\n",
       "   '2124591829',\n",
       "   '1966553486',\n",
       "   '1560147776',\n",
       "   '2047756776',\n",
       "   '2030144199',\n",
       "   '1997136459'],\n",
       "  'title': 'Recommender systems — beyond matrix completion'},\n",
       " {'abstract': 'Bagging and boosting are methods that generate a diverse ensemble of classifiers by manipulating the training data given to a “base” learning algorithm. Breiman has pointed out that they rely for their effectiveness on the instability of the base learning algorithm. An alternative approach to generating an ensemble is to randomize the internal decisions made by the base algorithm. This general approach has been studied previously by Ali and Pazzani and by Dietterich and Kong. This paper compares the effectiveness of randomization, bagging, and boosting for improving the performance of the decision-tree algorithm C4.5. The experiments show that in situations with little or no classification noise, randomization is competitive with (and perhaps slightly superior to) bagging but not as accurate as boosting. In situations with substantial classification noise, bagging is much better than boosting, and sometimes better than randomization.',\n",
       "  'authors': ['Thomas G. Dietterich'],\n",
       "  'date': '2000',\n",
       "  'identifier': '1605688901',\n",
       "  'references': ['2912934387',\n",
       "   '2112076978',\n",
       "   '2152761983',\n",
       "   '2982720039',\n",
       "   '1966280301',\n",
       "   '2167277498',\n",
       "   '2073738917',\n",
       "   '1562197959',\n",
       "   '2976840617',\n",
       "   '1850527962'],\n",
       "  'title': 'An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization'},\n",
       " {'abstract': 'Abstract We study a representation formula of the form ƒ = ∑ Q 〈ƒ, ϑ Q 〉ψ Q for a distribution ƒ on R n. This formula is obtained by discretizing and localizing a standard Littlewood-Paley decomposition. The map taking ƒ to the sequence {〈ƒ, ϑ Q 〉} Q , with Q running over the dyadic cubes in R n, is called the ϑ-transform. The functions ϑQ and ψQ have a particularly simple form. Moreover, most of the familiar distribution spaces (Lp-spaces, 1 R n. We also consider pointwise multipliers. For the characteristic function of a domain, we obtain boundedness results for a general class of domains which properly includes Lipschitz domains. Several interpolation methods are easily analyzed via the sequence spaces. For real interpolation, we obtain, among other things, an extension to the case p = 0. This in turn gives a new approach to the traditional atomic decomposition of Hardy spaces.',\n",
       "  'authors': ['Michael Frazier 1', ' Björn Jawerth 2'],\n",
       "  'date': '1990',\n",
       "  'identifier': '2088277224',\n",
       "  'references': ['1488877410',\n",
       "   '2087377426',\n",
       "   '1594572592',\n",
       "   '2013987111',\n",
       "   '560409768',\n",
       "   '1548295926',\n",
       "   '2149461673',\n",
       "   '1526278895',\n",
       "   '2086510139',\n",
       "   '2083129101'],\n",
       "  'title': 'A discrete transform and decompositions of distribution spaces'},\n",
       " {'abstract': 'Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.',\n",
       "  'authors': ['Sebastian Bach 1',\n",
       "   ' Alexander Binder 2',\n",
       "   ' Grégoire Montavon 1',\n",
       "   ' Frederick Klauschen 3',\n",
       "   ' Klaus Robert Müller 1',\n",
       "   ' Wojciech Samek 1'],\n",
       "  'date': '2015',\n",
       "  'identifier': '1787224781',\n",
       "  'references': ['2618530766',\n",
       "   '2151103935',\n",
       "   '1849277567',\n",
       "   '2164598857',\n",
       "   '2963207607',\n",
       "   '2031489346',\n",
       "   '2964153729',\n",
       "   '1554663460',\n",
       "   '2097018403',\n",
       "   '2962851944'],\n",
       "  'title': 'On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation.'},\n",
       " {'abstract': 'Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.',\n",
       "  'authors': ['Ilya Sutskever 1',\n",
       "   ' James Martens 2',\n",
       "   ' George Dahl 2',\n",
       "   ' Geoffrey Hinton 2'],\n",
       "  'date': '2013',\n",
       "  'identifier': '104184427',\n",
       "  'references': ['2618530766',\n",
       "   '2136922672',\n",
       "   '2100495367',\n",
       "   '2064675550',\n",
       "   '1533861849',\n",
       "   '2147768505',\n",
       "   '2110798204',\n",
       "   '1993882792',\n",
       "   '2184045248',\n",
       "   '196761320'],\n",
       "  'title': 'On the importance of initialization and momentum in deep learning'},\n",
       " {'abstract': 'Abstract The Michigan Microwave Canopy Scattering model (MIMICS) is based on a first-order solution of the radiative-transfer equation for a tree canopy comprising a crown layer, a trunk layer and a rough-surface ground boundary. The crown layer is modelled in terms of distributions of dielectric cylinders (representing needles and/or branches) and discs (representing leaves), and the trunks are treated as dielectric cylinders of uniform diameter. This report describes MIMICS I, which pertains to tree canopies with horizontally continuous (closed) crowns. The model, which is intended for use in the 0·5-10GHz region at angles greater than 10° from normal incidence, is formulated in terms of a 4 × 4 Stokes-like transformation matrix from which the backscattering coefficient can be computed for any transmit/receive polarization configuration.',\n",
       "  'authors': ['Fawwaz T. Ulaby ',\n",
       "   ' Kamal Sarabandi ',\n",
       "   ' Kyle McDONALD ',\n",
       "   ' Michael Whitt ',\n",
       "   ' M. Craig Dobson'],\n",
       "  'date': '1990',\n",
       "  'identifier': '2141348340',\n",
       "  'references': ['2152577388',\n",
       "   '1555713064',\n",
       "   '1532691246',\n",
       "   '1588167619',\n",
       "   '2051986600',\n",
       "   '2069756757',\n",
       "   '2038222372',\n",
       "   '2084670992',\n",
       "   '2172147072',\n",
       "   '2094742158'],\n",
       "  'title': 'Michigan microwave canopy scattering model'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Paul W. Strike'],\n",
       "  'date': '1981',\n",
       "  'identifier': '587168508',\n",
       "  'references': ['2133750711',\n",
       "   '2317437900',\n",
       "   '2071686313',\n",
       "   '2323233727',\n",
       "   '2091715922',\n",
       "   '2152112501',\n",
       "   '2092390690',\n",
       "   '3013113166',\n",
       "   '2107965868',\n",
       "   '2037473016'],\n",
       "  'title': 'Medical Laboratory Statistics'},\n",
       " {'abstract': 'Seventy-three low birthweight babies were independently assessed for gestational age using the scoring system of Dubowitz et al. (1970) and 5 neurological reflexes described by Robinson (1966). The results obtained by the 5 reflexes were compared with those obtained by the scoring system and were found to be accurate estimations of gestational age. The 5 reflexes may be used for babies of gestational ages 29 to 37 weeks, but above 37 weeks the scoring system must be used.',\n",
       "  'authors': ['G L Serfontein ', ' A M Jaroszewicz'],\n",
       "  'date': '1978',\n",
       "  'identifier': '2132970644',\n",
       "  'references': ['2066239597',\n",
       "   '2000629819',\n",
       "   '2148242406',\n",
       "   '2047644986',\n",
       "   '2151553577',\n",
       "   '2015072911',\n",
       "   '2162904948',\n",
       "   '2105306735',\n",
       "   '2089490864',\n",
       "   '1991766777'],\n",
       "  'title': 'Estimation of gestational age at birth. Comparison of two methods'},\n",
       " {'abstract': 'We present a method to learn object class models from unlabeled and unsegmented cluttered scenes for the purpose of visual object recognition. We focus on a particular type of model where objects are represented as flexible constellations of rigid parts (features). The variability within a class is represented by a joint probability density function (pdf) on the shape of the constellation and the output of part detectors. In a first stage, the method automatically identifies distinctive parts in the training set by applying a clustering algorithm to patterns selected by an interest operator. It then learns the statistical shape model using expectation maximization. The method achieves very good classification results on human faces and rear views of cars.',\n",
       "  'authors': ['Markus Weber 1', ' Max Welling 1', ' Pietro Perona 1', ' 2'],\n",
       "  'date': '2000',\n",
       "  'identifier': '1949116567',\n",
       "  'references': ['2049633694',\n",
       "   '3017143921',\n",
       "   '1564419782',\n",
       "   '2095757522',\n",
       "   '1958762911',\n",
       "   '2124722975',\n",
       "   '2117138270',\n",
       "   '2125791971',\n",
       "   '2029727948',\n",
       "   '1628541567'],\n",
       "  'title': 'Unsupervised Learning of Models for Recognition'},\n",
       " {'abstract': 'This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints---that words tend to have one sense per discourse and one sense per collocation---exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%.',\n",
       "  'authors': ['David Yarowsky'],\n",
       "  'date': '1995',\n",
       "  'identifier': '2101210369',\n",
       "  'references': ['2049633694',\n",
       "   '2102381086',\n",
       "   '2099247782',\n",
       "   '2040004971',\n",
       "   '1977182536',\n",
       "   '1971220772',\n",
       "   '1554031433',\n",
       "   '2129139611',\n",
       "   '2428981601',\n",
       "   '2156202195'],\n",
       "  'title': 'UNSUPERVISED WORD SENSE DISAMBIGUATION RIVALING SUPERVISED METHODS'},\n",
       " {'abstract': 'A key question in radar polarimetry is the existence of a target dichotomy ie. the existence of more than one target decomposition theory. In this paper we show, by using a coherency matrix approach, that there is no target dichotomy and subsequently only one unique target decomposition. We illustrate its usefulness by giving two examples; the scattering of light by small anisotropic particles and rough surface scattering under physical optics.',\n",
       "  'authors': ['S. R. Cloude'],\n",
       "  'date': '1992',\n",
       "  'identifier': '2199513852',\n",
       "  'references': ['2152577388',\n",
       "   '2995136929',\n",
       "   '2004605483',\n",
       "   '2016187028',\n",
       "   '2077717067',\n",
       "   '2062928877',\n",
       "   '2519627623',\n",
       "   '101722527',\n",
       "   '2166202664',\n",
       "   '2027837850'],\n",
       "  'title': 'Uniqueness of Target Decomposition Theorems in Radar Polarimetry'},\n",
       " {'abstract': 'A new approach for black and white image compression is described, with which the eight CCITT test documents can be compressed in a lossless manner 20-30 percent better than with the best existing compression algorithms. The coding and the modeling aspects are treated separately. The key to these improvements is an efficient binary arithmetic code. The code is relatively simple to implement because it avoids the multiplication operation inherent in some earlier arithmetic codes. Arithmetic coding permits the compression of binary sequences where the statistics change on a bit-to-bit basis. Model statistics are studied from stationary, stationary adaptive, and nonstationary adaptive assumptions.',\n",
       "  'authors': ['G. Langdon ', ' J. Rissanen'],\n",
       "  'date': '1981',\n",
       "  'identifier': '2165564574',\n",
       "  'references': ['2142901448',\n",
       "   '2119047110',\n",
       "   '2911940095',\n",
       "   '2128066268',\n",
       "   '2046419776',\n",
       "   '1519253855',\n",
       "   '158805393',\n",
       "   '2024495675',\n",
       "   '1994606555',\n",
       "   '2027176594'],\n",
       "  'title': 'Compression of Black-White Images with Arithmetic Coding'},\n",
       " {'abstract': 'Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.',\n",
       "  'authors': ['Isabelle Guyon 1', ' André Elisseeff 2'],\n",
       "  'date': '2003',\n",
       "  'identifier': '2119479037',\n",
       "  'references': ['2148603752',\n",
       "   '2109363337',\n",
       "   '3023786531',\n",
       "   '2157795344',\n",
       "   '3085162807',\n",
       "   '2143426320',\n",
       "   '2087347434',\n",
       "   '2017337590',\n",
       "   '1594031697',\n",
       "   '2103333826'],\n",
       "  'title': 'An introduction to variable and feature selection'},\n",
       " {'abstract': \"We approach recognition in the framework of deformable shape matching, relying on a new algorithm for finding correspondences between feature points. This algorithm sets up correspondence as an integer quadratic programming problem, where the cost function has terms based on similarity of corresponding geometric blur point descriptors as well as the geometric distortion between pairs of corresponding feature points. The algorithm handles outliers, and thus enables matching of exemplars to query images in the presence of occlusion and clutter. Given the correspondences, we estimate an aligning transform, typically a regularized thin plate spline, resulting in a dense correspondence between the two shapes. Object recognition is then handled in a nearest neighbor framework where the distance between exemplar and query is the matching cost between corresponding points. We show results on two datasets. One is the Caltech 101 dataset (Fei-Fei, Fergus and Perona), an extremely challenging dataset with large intraclass variation. Our approach yields a 48% correct classification rate, compared to Fei-Fei et al 's 16%. We also show results for localizing frontal and profile faces that are comparable to special purpose approaches tuned to faces.\",\n",
       "  'authors': ['A.C. Berg ', ' T.L. Berg ', ' J. Malik'],\n",
       "  'date': '2005',\n",
       "  'identifier': '2168002178',\n",
       "  'references': ['2151103935',\n",
       "   '3097096317',\n",
       "   '2124386111',\n",
       "   '2177274842',\n",
       "   '2154422044',\n",
       "   '2124087378',\n",
       "   '2119823327',\n",
       "   '2155511848',\n",
       "   '2101522199',\n",
       "   '2160754664'],\n",
       "  'title': 'Shape matching and object recognition using low distortion correspondences'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Neeraj Tiwari ', ' Ila Pant ', ' Arun Kumar Nigam'],\n",
       "  'date': '2008',\n",
       "  'identifier': '1030489121',\n",
       "  'references': ['1988858799', '2014277562', '3019020937', '2081041209'],\n",
       "  'title': 'On an optimal controlled nearest proportional to size sampling scheme'},\n",
       " {'abstract': 'In this paper, we study the problem of sampling and reconstructing signals which are assumed to lie on or close to one of several subspaces of a Hilbert space. Importantly, we here consider a very general setting in which we allow infinitely many subspaces in infinite dimensional Hilbert spaces. This general approach allows us to unify many results derived recently in areas such as compressed sensing, affine rank minimization, analog compressed sensing and structured matrix decompositions.',\n",
       "  'authors': ['T Blumensath'],\n",
       "  'date': '2011',\n",
       "  'identifier': '2100875869',\n",
       "  'references': ['2296616510',\n",
       "   '2129638195',\n",
       "   '2145962650',\n",
       "   '2202343345',\n",
       "   '2053691921',\n",
       "   '2030449718',\n",
       "   '2125680629',\n",
       "   '2123629701',\n",
       "   '2963322354',\n",
       "   '2162409952'],\n",
       "  'title': 'Sampling and Reconstructing Signals From a Union of Linear Subspaces'},\n",
       " {'abstract': '',\n",
       "  'authors': ['L.G. DeMichiel'],\n",
       "  'date': '1989',\n",
       "  'identifier': '3110717095',\n",
       "  'references': ['2110411158',\n",
       "   '1521347133',\n",
       "   '2114760008',\n",
       "   '2145402478',\n",
       "   '2178579001',\n",
       "   '2108433101',\n",
       "   '2072392424',\n",
       "   '1983413912',\n",
       "   '1585503824',\n",
       "   '2583932101'],\n",
       "  'title': 'Resolving Database Incompatibility: An Approach to Performing Relational Operations over Mismatched Domains'},\n",
       " {'abstract': 'This book provides an up-to-date introduction to information theory. In addition to the classical topics discussed, it provides the first comprehensive treatment of the theory of I-Measure, network coding theory, Shannon and non-Shannon type information inequalities, and a relation between entropy and group theory. ITIP, a software package for proving information inequalities, is also included. With a large number of examples, illustrations, and original problems, this book is excellent as a textbook or reference book for a senior or graduate level course on the subject, as well as a reference for researchers in related fields.',\n",
       "  'authors': ['Raymond W. Yeung'],\n",
       "  'date': '2002',\n",
       "  'identifier': '1515040589',\n",
       "  'references': ['2105831729',\n",
       "   '2137813581',\n",
       "   '2106403318',\n",
       "   '2138928022',\n",
       "   '2049633694',\n",
       "   '2987657883',\n",
       "   '2135764410',\n",
       "   '1634005169',\n",
       "   '2120085609',\n",
       "   '1638203394'],\n",
       "  'title': 'A First Course in Information Theory'},\n",
       " {'abstract': 'Authorship attribution, the science of inferring characteristics of the author from the characteristics of documents written by that author, is a problem with a long history and a wide range of application. Recent work in \"non-traditional\" authorship attribution demonstrates the practicality of automatically analyzing documents based on authorial style, but the state of the art is confusing. Analyses are difficult to apply, little is known about type or rate of errors, and few \"best practices\" are available. In part because of this confusion, the field has perhaps had less uptake and general acceptance than is its due. This review surveys the history and present state of the discipline, presenting some comparative results when available. It shows, first, that the discipline is quite successful, even in difficult cases involving small documents in unfamiliar and less studied languages; it further analyzes the types of analysis and features used and tries to determine characteristics of well-performing systems, finally formulating these in a set of recommendations for best practices.',\n",
       "  'authors': ['Patrick Juola'],\n",
       "  'date': '2008',\n",
       "  'identifier': '2912780347',\n",
       "  'references': ['2156909104',\n",
       "   '2139212933',\n",
       "   '3097169496',\n",
       "   '2133671888',\n",
       "   '1638203394',\n",
       "   '2097333193',\n",
       "   '349770100',\n",
       "   '1604792744',\n",
       "   '1967461618',\n",
       "   '2091034860'],\n",
       "  'title': 'Authorship Attribution'},\n",
       " {'abstract': \"Networks of coupled dynamical systems have been used to model biological oscillators, Josephson junction arrays, excitable media, neural networks, spatial games, genetic control networks and many other self-organizing systems. Ordinarily, the connection topology is assumed to be either completely regular or completely random. But many biological, technological and social networks lie somewhere between these two extremes. Here we explore simple models of networks that can be tuned through this middle ground: regular networks 'rewired' to introduce increasing amounts of disorder. We find that these systems can be highly clustered, like regular lattices, yet have small characteristic path lengths, like random graphs. We call them 'small-world' networks, by analogy with the small-world phenomenon (popularly known as six degrees of separation. The neural network of the worm Caenorhabditis elegans, the power grid of the western United States, and the collaboration graph of film actors are shown to be small-world networks. Models of dynamical systems with small-world coupling display enhanced signal-propagation speed, computational power, and synchronizability. In particular, infectious diseases spread more easily in small-world networks than in regular lattices.\",\n",
       "  'authors': ['Duncan J. Watts ', ' Steven H. Strogatz'],\n",
       "  'date': '1998',\n",
       "  'identifier': '2112090702',\n",
       "  'references': ['2061901927',\n",
       "   '2905110430',\n",
       "   '2062663664',\n",
       "   '111157985',\n",
       "   '1500530370',\n",
       "   '2025490132',\n",
       "   '2079948225',\n",
       "   '3049667020',\n",
       "   '2026552514',\n",
       "   '2017444605'],\n",
       "  'title': 'Collective dynamics of small-world networks'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Ming-Hsuan Yang ', ' David J. Kriegman ', ' Narendra Ahuja'],\n",
       "  'date': '2002',\n",
       "  'identifier': '3111038685',\n",
       "  'references': ['2098693229',\n",
       "   '2997124351',\n",
       "   '2824947166',\n",
       "   '1965252517',\n",
       "   '2023244644',\n",
       "   '325114128',\n",
       "   '2975334175',\n",
       "   '2121718848',\n",
       "   '843191978',\n",
       "   '2332000102'],\n",
       "  'title': 'Detecting Faces in Images: A Survey'},\n",
       " {'abstract': 'From the Publisher: Dramatically updating and extending the first edition, published in 1995, the second edition of The Handbook of Brain Theory and Neural Networks presents the enormous progress made in recent years in the many subfields related to the two great questions: How does the brain work? and, How can we build intelligent machines? Once again, the heart of the book is a set of almost 300 articles covering the whole spectrum of topics in brain theory and neural networks. The first two parts of the book, prepared by Michael Arbib, are designed to help readers orient themselves in this wealth of material. Part I provides general background on brain modeling and on both biological and artificial neural networks. Part II consists of \"Road Maps\" to help readers steer through articles in part III on specific topics of interest. The articles in part III are written so as to be accessible to readers of diverse backgrounds. They are cross-referenced and provide lists of pointers to Road Maps, background material, and related reading. The second edition greatly increases the coverage of models of fundamental neurobiology, cognitive neuroscience, and neural network approaches to language. It contains 287 articles, compared to the 266 in the first edition. Articles on topics from the first edition have been updated by the original authors or written anew by new authors, and there are 106 articles on new topics.',\n",
       "  'authors': ['Michael A. Arbib'],\n",
       "  'date': '2007',\n",
       "  'identifier': '1536929369',\n",
       "  'references': ['2076063813',\n",
       "   '2070722739',\n",
       "   '2117812871',\n",
       "   '2164727176',\n",
       "   '2964265128',\n",
       "   '2165599843',\n",
       "   '2105464873',\n",
       "   '2963090522',\n",
       "   '2115979064',\n",
       "   '1689445748'],\n",
       "  'title': 'The Handbook of Brain Theory and Neural Networks'},\n",
       " {'abstract': \"Cambridge, Mass.: MIT Press, 1972. 2nd. ed. The book's aim is to seek general results from the close study of abstract version of devices known as perceptrons\",\n",
       "  'authors': ['Marvin Lee Minsky ', ' Seymour Papert'],\n",
       "  'date': '1969',\n",
       "  'identifier': '2086789740',\n",
       "  'references': ['2140190241',\n",
       "   '2017337590',\n",
       "   '607505555',\n",
       "   '1736726159',\n",
       "   '2293063825',\n",
       "   '2186428165',\n",
       "   '2962790689',\n",
       "   '2885825670',\n",
       "   '2077723394'],\n",
       "  'title': 'Perceptrons: An Introduction to Computational Geometry'},\n",
       " {'abstract': 'The earlier introduced arithmetic coding idea has been generalized to a very broad and flexible coding technique which includes virtually all known variable rate noiseless coding techniques as special cases. An outstanding feature of this technique is that alphabet extensions are not required. A complete decodability analysis is given. The relationship of arithmetic coding to other known nonblock codes is illuminated.',\n",
       "  'authors': ['J. Rissanen ', ' G. G. Langdon'],\n",
       "  'date': '1979',\n",
       "  'identifier': '2911940095',\n",
       "  'references': ['2752853835',\n",
       "   '2107745473',\n",
       "   '2122962290',\n",
       "   '1995875735',\n",
       "   '2038649859',\n",
       "   '2046419776',\n",
       "   '1519253855',\n",
       "   '2034323860',\n",
       "   '1992371956',\n",
       "   '158805393'],\n",
       "  'title': 'Arithmetic coding'},\n",
       " {'abstract': 'In this paper, we provide an overview of the WNUT-2020 shared task on the identification of informative COVID-19 English Tweets. We describe how we construct a corpus of 10K Tweets and organize the development and evaluation phases for this task. In addition, we also present a brief summary of results obtained from the final system evaluation submissions of 55 teams, finding that (i) many systems obtain very high performance, up to 0.91 F1 score, (ii) the majority of the submissions achieve substantially higher results than the baseline fastText (Joulin et al., 2017), and (iii) fine-tuning pre-trained language models on relevant language data followed by supervised training performs well in this task.',\n",
       "  'authors': ['Dat Quoc Nguyen 1',\n",
       "   ' Thanh Vu 2',\n",
       "   ' Afshin Rahimi 3',\n",
       "   ' Mai Hoang Dao ',\n",
       "   ' Long Doan'],\n",
       "  'date': '2020',\n",
       "  'identifier': '3099342932',\n",
       "  'references': ['2963341956',\n",
       "   '2965373594',\n",
       "   '2970597249',\n",
       "   '2963626623',\n",
       "   '2164777277',\n",
       "   '3024622987',\n",
       "   '1975879668',\n",
       "   '2125980212',\n",
       "   '3104186312',\n",
       "   '3105987139'],\n",
       "  'title': 'WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets'},\n",
       " {'abstract': 'A connection between universal codes and the problems of prediction and statistical estimation is established. A known lower bound for the mean length of universal codes is sharpened and generalized, and optimum universal codes constructed. The bound is defined to give the information in strings relative to the considered class of processes. The earlier derived minimum description length criterion for estimation of parameters, including their number, is given a fundamental information, theoretic justification by showing that its estimators achieve the information in the strings. It is also shown that one cannot do prediction in Gaussian autoregressive moving average (ARMA) processes below a bound, which is determined by the information in the data.',\n",
       "  'authors': ['J. Rissanen'],\n",
       "  'date': '1984',\n",
       "  'identifier': '2132119275',\n",
       "  'references': ['2142635246',\n",
       "   '2054658115',\n",
       "   '2106596127',\n",
       "   '2119047110',\n",
       "   '2082967074',\n",
       "   '2128777897',\n",
       "   '1651818244',\n",
       "   '1999120268',\n",
       "   '2030703096',\n",
       "   '2131024393'],\n",
       "  'title': 'Universal coding, information, prediction, and estimation'},\n",
       " {'abstract': 'We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.',\n",
       "  'authors': ['Tsung-Yi Lin 1',\n",
       "   ' Michael Maire 2',\n",
       "   ' Serge J. Belongie 1',\n",
       "   ' James Hays 3',\n",
       "   ' Pietro Perona 2',\n",
       "   ' Deva Ramanan 4',\n",
       "   ' Piotr Dollár 5',\n",
       "   ' C. Lawrence Zitnick 5'],\n",
       "  'date': '2014',\n",
       "  'identifier': '1861492603',\n",
       "  'references': ['2618530766',\n",
       "   '2102605133',\n",
       "   '2161969291',\n",
       "   '2108598243',\n",
       "   '2168356304',\n",
       "   '2963542991',\n",
       "   '3118608800',\n",
       "   '2031489346',\n",
       "   '2038721957',\n",
       "   '2110158442'],\n",
       "  'title': 'Microsoft COCO: Common Objects in Context'},\n",
       " {'abstract': 'A coding technique is described which improves error performance of synchronous data links without sacrificing data rate or requiring more bandwidth. This is achieved by channel coding with expanded sets of multilevel/phase signals in a manner which increases free Euclidean distance. Soft maximum--likelihood (ML) decoding using the Viterbi algorithm is assumed. Following a discussion of channel capacity, simple hand-designed trellis codes are presented for 8 phase-shift keying (PSK) and 16 quadrature amplitude-shift keying (QASK) modulation. These simple codes achieve coding gains in the order of 3-4 dB. It is then shown that the codes can be interpreted as binary convolutional codes with a mapping of coded bits into channel signals, which we call \"mapping by set partitioning.\" Based on a new distance measure between binary code sequences which efficiently lower-bounds the Euclidean distance between the corresponding channel signal sequences, a search procedure for more powerful codes is developed. Codes with coding gains up to 6 dB are obtained for a variety of multilevel/phase modulation schemes. Simulation results are presented and an example of carrier-phase tracking is discussed.',\n",
       "  'authors': ['G. Ungerboeck'],\n",
       "  'date': '1982',\n",
       "  'identifier': '2165205968',\n",
       "  'references': ['2142384583',\n",
       "   '2142901448',\n",
       "   '1577906631',\n",
       "   '2153810958',\n",
       "   '2113421553',\n",
       "   '2104919811',\n",
       "   '2015635521',\n",
       "   '2136168787',\n",
       "   '2139397562',\n",
       "   '2744859341'],\n",
       "  'title': 'Channel coding with multilevel/phase signals'},\n",
       " {'abstract': '',\n",
       "  'authors': ['William Feller'],\n",
       "  'date': '1950',\n",
       "  'identifier': '2751862591',\n",
       "  'references': ['1660562555',\n",
       "   '2099040451',\n",
       "   '1527262418',\n",
       "   '2106864314',\n",
       "   '2149959815',\n",
       "   '2180060173',\n",
       "   '2117756735',\n",
       "   '2147717514',\n",
       "   '2161160262'],\n",
       "  'title': 'An introduction to probability theory and its applications'},\n",
       " {'abstract': '',\n",
       "  'authors': ['W. N. Venables ', ' B. D. Ripley'],\n",
       "  'date': '2002',\n",
       "  'identifier': '2289748525',\n",
       "  'references': ['2156909104',\n",
       "   '2148603752',\n",
       "   '2331432542',\n",
       "   '1554944419',\n",
       "   '1554663460',\n",
       "   '1480376833',\n",
       "   '2119821739',\n",
       "   '1548802052',\n",
       "   '1563088657',\n",
       "   '1679913846'],\n",
       "  'title': 'Modern Applied Statistics with S Fourth edition'},\n",
       " {'abstract': \"Text-based passwords remain the dominant authentication method in computer systems, despite significant advancement in attackers' capabilities to perform password cracking. In response to this threat, password composition policies have grown increasingly complex. However, there is insufficient research defining metrics to characterize password strength and using them to evaluate password-composition policies. In this paper, we analyze 12,000 passwords collected under seven composition policies via an online study. We develop an efficient distributed method for calculating how effectively several heuristic password-guessing algorithms guess passwords. Leveraging this method, we investigate (a) the resistance of passwords created under different conditions to guessing, (b) the performance of guessing algorithms under different training sets, (c) the relationship between passwords explicitly created under a given composition policy and other passwords that happen to meet the same requirements, and (d) the relationship between guess ability, as measured with password-cracking algorithms, and entropy estimates. Our findings advance understanding of both password-composition policies and metrics for quantifying password security.\",\n",
       "  'authors': ['P. G. Kelley ',\n",
       "   ' S. Komanduri ',\n",
       "   ' M. L. Mazurek ',\n",
       "   ' R. Shay ',\n",
       "   ' T. Vidas ',\n",
       "   ' L. Bauer ',\n",
       "   ' N. Christin ',\n",
       "   ' L. F. Cranor ',\n",
       "   ' J. Lopez'],\n",
       "  'date': '2012',\n",
       "  'identifier': '2054626033',\n",
       "  'references': ['2173213060',\n",
       "   '2141708418',\n",
       "   '1598064945',\n",
       "   '2151401338',\n",
       "   '2114269021',\n",
       "   '2171920515',\n",
       "   '1566273181',\n",
       "   '1577841485',\n",
       "   '1503108337',\n",
       "   '2135359429'],\n",
       "  'title': 'Guess Again (and Again and Again): Measuring Password Strength by Simulating Password-Cracking Algorithms'},\n",
       " {'abstract': 'This paper describes the development of a computer model GANET that involves the application of an area of evolutionary computing, better known as genetic algorithms, to the problem of least-cost design of water distribution networks. Genetic algorithms represent an efficient search method for nonlinear optimization problems; this method is gaining acceptance among water resources managers/planners. These algorithms share the favorable attributes of Monte Carlo techniques over local optimization methods in that they do not require linearizing assumptions nor the calculation of partial derivatives, and they avoid numerical instabilities associated with matrix inversion. In addition, their sampling is global, rather than local, thus reducing the tendency to become entrapped in local minima and avoiding dependency on a starting point. Genetic algorithms are introduced in their original form followed by different improvements that were found to be necessary for their effective implementation in the optimization of water distribution networks. An example taken from the literature illustrates the approach used for the formulation of the problem. To illustrate the capability of GANET to efficiently identify good designs, three previously published problems have been solved. This led to the discovery of inconsistencies in predictions of network performance caused by different interpretations of the widely adopted Hazen-Williams pipe flow equation in the past studies. As well as being very efficient for network optimization, GANET is also easy to use, having almost the same input requirements as hydraulic simulation models. The only additional data requirements are a few genetic algorithm parameters that take values recommended in the literature. Two network examples, one of a new network design and one of parallel network expansion, illustrate the potential of GANET as a tool for water distribution network planning and management.',\n",
       "  'authors': ['Dragan A. Savic ', ' Godfrey A. Walters'],\n",
       "  'date': '1997',\n",
       "  'identifier': '1997621454',\n",
       "  'references': ['1639032689',\n",
       "   '1497256448',\n",
       "   '2156728410',\n",
       "   '2042704421',\n",
       "   '2037801859',\n",
       "   '2028017445',\n",
       "   '2011703887',\n",
       "   '1496027058',\n",
       "   '1963627948',\n",
       "   '2066368882'],\n",
       "  'title': 'Genetic Algorithms for Least-Cost Design of Water Distribution Networks'},\n",
       " {'abstract': 'Abstract Subjects were requested to choose between gambles, where the outcome of one gamble depended on a single elementary event, and the other depended on an event compounded of a series of such elementary events. The data supported the hypothesis that the subjective probability of a compound event is systematically biased in the direction of the probability of its components resulting in overestimation of conjunctive events and underestimation of disjunctive events. Studies pertaining to this topic are discussed.',\n",
       "  'authors': ['Maya Bar-Hillel'],\n",
       "  'date': '1973',\n",
       "  'identifier': '1965740984',\n",
       "  'references': ['2035782089',\n",
       "   '2016377072',\n",
       "   '2079199322',\n",
       "   '2085773866',\n",
       "   '2074854537',\n",
       "   '2085744109',\n",
       "   '1983846617',\n",
       "   '63828031'],\n",
       "  'title': 'On the subjective probability of compound events'},\n",
       " {'abstract': 'We study an energy functional for computing optical flow that combines three assumptions: a brightness constancy assumption, a gradient constancy assumption, and a discontinuity-preserving spatio-temporal smoothness constraint. In order to allow for large displacements, linearisations in the two data terms are strictly avoided. We present a consistent numerical scheme based on two nested fixed point iterations. By proving that this scheme implements a coarse-to-fine warping strategy, we give a theoretical foundation for warping which has been used on a mainly experimental basis so far. Our evaluation demonstrates that the novel method gives significantly smaller angular errors than previous techniques for optical flow estimation. We show that it is fairly insensitive to parameter variations, and we demonstrate its excellent robustness under noise.',\n",
       "  'authors': ['Thomas Brox ',\n",
       "   ' Andr ´ es Bruhn ',\n",
       "   ' Nils Papenberg ',\n",
       "   ' Joachim Weickert'],\n",
       "  'date': '2004',\n",
       "  'identifier': '1867429401',\n",
       "  'references': ['2103559027',\n",
       "   '3003662786',\n",
       "   '2620619910',\n",
       "   '2118877769',\n",
       "   '2154504070',\n",
       "   '2100315781',\n",
       "   '2130657708',\n",
       "   '1538585633',\n",
       "   '1552681844',\n",
       "   '1961031432'],\n",
       "  'title': 'High Accuracy Optical Flow Estimation Based on a Theory for Warping'},\n",
       " {'abstract': 'A class of neurons specifically related to hand movements was studied in the posterior parietal cortex while the monkeys manipulated different types of objects. We examined the neuronal activity during manipulation of objects by the hand in the light and in the dark. Fiftyfive neurons were active during manipulation in the dark and were classified as “hand-movement-related” neurons. Of these, 38/55 (69%) cells were also influenced by the visual stimulus. Most of the hand-movement-related neurons were selective in the type of objects manipulated. Moreover, some of these cells were selective in the axis of orientation of the object. These results suggest that the hand-movement-related neurons of the parietal cortex are concerned with the visual guidance of the hand movement, especially in matching the pattern of movement with the spatial characteristics of the object to be manipulated.',\n",
       "  'authors': ['M. Taira ',\n",
       "   ' S. Mine ',\n",
       "   ' A. P. Georgopoulos ',\n",
       "   ' A. Murata ',\n",
       "   ' H. Sakata'],\n",
       "  'date': '1990',\n",
       "  'identifier': '2044539750',\n",
       "  'references': ['2883524685',\n",
       "   '1566353072',\n",
       "   '1904024272',\n",
       "   '2093642963',\n",
       "   '2177960617',\n",
       "   '1984679454',\n",
       "   '2406002377',\n",
       "   '2058949896',\n",
       "   '1895588836',\n",
       "   '1587405368'],\n",
       "  'title': 'Parietal cortex neurons of the monkey related to the visual guidance of hand movement'},\n",
       " {'abstract': '1. Introduction. 2. Fundamentals of Digital Speech Processing. 3. Digital Models for the Speech Signal. 4. Time-Domain Models for Speech Processing. 5. Digital Representation of the Speech Waveform. 6. Short-Time Fourier Analysis. 7. Homomorphic Speech Processing. 8. Linear Predictive Coding of Speech. 9. Digital Speech Processing for Man-Machine Communication by Voice.',\n",
       "  'authors': ['Lawrence R. Rabiner ', ' Ronald W. Schafer'],\n",
       "  'date': '1978',\n",
       "  'identifier': '2069501481',\n",
       "  'references': ['2139356617',\n",
       "   '2159390040',\n",
       "   '2074788634',\n",
       "   '1755563775',\n",
       "   '2014683958',\n",
       "   '2078528584',\n",
       "   '2129244720',\n",
       "   '353562331',\n",
       "   '2048508162',\n",
       "   '1553004968'],\n",
       "  'title': 'Digital Processing of Speech Signals'},\n",
       " {'abstract': 'Long Short-Term Memory (LSTM) is a specific recurrent neural network (RNN) architecture that was designed to model temporal sequences and their long-range dependencies more accurately than conventional RNNs. In this paper, we explore LSTM RNN architectures for large scale acoustic modeling in speech recognition. We recently showed that LSTM RNNs are more effective than DNNs and conventional RNNs for acoustic modeling, considering moderately-sized models trained on a single machine. Here, we introduce the first distributed training of LSTM RNNs using asynchronous stochastic gradient descent optimization on a large cluster of machines. We show that a two-layer deep LSTM RNN where each LSTM layer has a linear recurrent projection layer can exceed state-of-the-art speech recognition performance. This architecture makes more effective use of model parameters than the others considered, converges quickly, and outperforms a deep feed forward neural network having an order of magnitude more parameters. Index Terms: Long Short-Term Memory, LSTM, recurrent neural network, RNN, speech recognition, acoustic modeling.',\n",
       "  'authors': ['Hasim Sak ', ' Andrew W. Senior ', ' Françoise Beaufays'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2293634267',\n",
       "  'references': ['2064675550',\n",
       "   '2168231600',\n",
       "   '2143612262',\n",
       "   '179875071',\n",
       "   '2147768505',\n",
       "   '1993882792',\n",
       "   '2184045248',\n",
       "   '2005708641',\n",
       "   '2122585011',\n",
       "   '2402268235'],\n",
       "  'title': 'Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling'},\n",
       " {'abstract': 'This monograph provides an overview of general deep learning methodology and its applications to a variety of signal and information processing tasks. The application areas are chosen with the following three criteria in mind: (1) expertise or knowledge of the authors; (2) the application areas that have already been transformed by the successful use of deep learning technology, such as speech recognition and computer vision; and (3) the application areas that have the potential to be impacted significantly by deep learning and that have been experiencing research growth, including natural language and text processing, information retrieval, and multimodal information processing empowered by multi-task deep learning.',\n",
       "  'authors': ['Li Deng ', ' Dong Yu'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2150341604',\n",
       "  'references': ['2618530766',\n",
       "   '2102605133',\n",
       "   '2153579005',\n",
       "   '1614298861',\n",
       "   '2136922672',\n",
       "   '1849277567',\n",
       "   '2100495367',\n",
       "   '2310919327',\n",
       "   '1904365287',\n",
       "   '2158899491'],\n",
       "  'title': 'Deep Learning: Methods and Applications'},\n",
       " {'abstract': 'In this paper, we compare the performance of descriptors computed for local interest regions, as, for example, extracted by the Harris-Affine detector [Mikolajczyk, K and Schmid, C, 2004]. Many different descriptors have been proposed in the literature. It is unclear which descriptors are more appropriate and how their performance depends on the interest region detector. The descriptors should be distinctive and at the same time robust to changes in viewing conditions as well as to errors of the detector. Our evaluation uses as criterion recall with respect to precision and is carried out for different image transformations. We compare shape context [Belongie, S, et al., April 2002], steerable filters [Freeman, W and Adelson, E, Setp. 1991], PCA-SIFT [Ke, Y and Sukthankar, R, 2004], differential invariants [Koenderink, J and van Doorn, A, 1987], spin images [Lazebnik, S, et al., 2003], SIFT [Lowe, D. G., 1999], complex filters [Schaffalitzky, F and Zisserman, A, 2002], moment invariants [Van Gool, L, et al., 1996], and cross-correlation for different types of interest regions. We also propose an extension of the SIFT descriptor and show that it outperforms the original method. Furthermore, we observe that the ranking of the descriptors is mostly independent of the interest region detector and that the SIFT-based descriptors perform best. Moments and steerable filters show the best performance among the low dimensional descriptors.',\n",
       "  'authors': ['K. Mikolajczyk 1', ' C. Schmid 2'],\n",
       "  'date': '2005',\n",
       "  'identifier': '2177274842',\n",
       "  'references': ['2151103935',\n",
       "   '2033819227',\n",
       "   '2124386111',\n",
       "   '2163352848',\n",
       "   '2131846894',\n",
       "   '2057175746',\n",
       "   '2154422044',\n",
       "   '2145023731',\n",
       "   '1980911747',\n",
       "   '2145072179'],\n",
       "  'title': 'A performance evaluation of local descriptors'},\n",
       " {'abstract': 'In this chapter, we discuss a statistical generative model called independent component analysis. It is basically a proper probabilistic formulation of the ideas underpinning sparse coding. It shows how sparse coding can be interpreted as providing a Bayesian prior, and answers some questions which were not properly answered in the sparse coding framework.',\n",
       "  'authors': ['Aapo Hyvarinen ', ' Juha Karhunen ', ' Erkki Oja'],\n",
       "  'date': '2001',\n",
       "  'identifier': '1548802052',\n",
       "  'references': ['2076063813',\n",
       "   '2163922914',\n",
       "   '2072128103',\n",
       "   '2154053567',\n",
       "   '2123649031',\n",
       "   '2132549764',\n",
       "   '3110653090',\n",
       "   '2115706991',\n",
       "   '2098535678'],\n",
       "  'title': 'Independent Component Analysis'},\n",
       " {'abstract': 'The problems arising in the modeling and coding of strings for compression purposes are discussed. The notion of an information source that simplifies and sharpens the traditional one is axiomatized, and adaptive and nonadaptive models are defined. With a measure of complexity assigned to the models, a fundamental theorem is proved which states that models that use any kind of alphabet extension are inferior to the best models using no alphabet extensions at all. A general class of so-called first-in first-out (FIFO) arithmetic codes is described which require no alphabet extension devices and which therefore can be used in conjunction with the best models. Because the coding parameters are the probabilities that define the model, their design is easy, and the application of the code is straightforward even with adaptively changing source models.',\n",
       "  'authors': ['J. Rissanen ', ' G. Langdon'],\n",
       "  'date': '1981',\n",
       "  'identifier': '2119047110',\n",
       "  'references': ['2911940095',\n",
       "   '2046419776',\n",
       "   '1519253855',\n",
       "   '2034323860',\n",
       "   '2005097301',\n",
       "   '2006384477',\n",
       "   '158805393',\n",
       "   '2103206811',\n",
       "   '2143603891',\n",
       "   '2061192159'],\n",
       "  'title': 'Universal modeling and coding'},\n",
       " {'abstract': 'The capacity of a class of deterministic relay channels with transmitter input X, receiver output Y, relay output Y1 = f(X, Y), and separate noiseless communication link of capacity R0 from the relay to the receiver, is shown to be C(R0) = sup min {I(X;Y) + R0, I(X;Y,Y,)}. P(x) Roughly speaking, every bit from the relay is worth one bit to the receiver until saturation at capacity.',\n",
       "  'authors': ['Young-Han Kim'],\n",
       "  'date': '2008',\n",
       "  'identifier': '2110266514',\n",
       "  'references': ['2098567664',\n",
       "   '2167447263',\n",
       "   '2099213070',\n",
       "   '2150412388',\n",
       "   '2153829573',\n",
       "   '2146818257',\n",
       "   '1973237629',\n",
       "   '1971536606',\n",
       "   '2034323860',\n",
       "   '2155150064'],\n",
       "  'title': 'Capacity of a Class of Deterministic Relay Channels'},\n",
       " {'abstract': 'Gaussian mixture models are currently the dominant technique for modeling the emission distribution of hidden Markov models for speech recognition. We show that better phone recognition on the TIMIT dataset can be achieved by replacing Gaussian mixture models by deep neural networks that contain many layers of features and a very large number of parameters. These networks are first pre-trained as a multi-layer generative model of a window of spectral feature vectors without making use of any discriminative information. Once the generative pre-training has designed the features, we perform discriminative fine-tuning using backpropagation to adjust the features slightly to make them better at predicting a probability distribution over the states of monophone hidden Markov models.',\n",
       "  'authors': ['A. Mohamed ', ' G. E. Dahl ', ' G. Hinton'],\n",
       "  'date': '2012',\n",
       "  'identifier': '1993882792',\n",
       "  'references': ['2136922672',\n",
       "   '3118608800',\n",
       "   '2100495367',\n",
       "   '2116064496',\n",
       "   '2147768505',\n",
       "   '2159080219',\n",
       "   '44815768',\n",
       "   '1994197834',\n",
       "   '2913932916',\n",
       "   '2103359087'],\n",
       "  'title': 'Acoustic Modeling Using Deep Belief Networks'},\n",
       " {'abstract': 'A database management system provides the ideal support for electronic mail applications. The Walnut mail system built at the Xerox Palo Alto Research Center was recently redesigned to take better advantage of its underlying database facilities. The ability to pose ad-hoc queries with a \"fill-in-the-form\" browser allows people to browse their mail quickly and effectively, while database access paths guarantee fast retrieval of stored information. Careful consideration of the systems\\' usage was reflected in both the database schema representation and the user-interface for browsing mail.',\n",
       "  'authors': ['Jack Kent ', ' Douglas B. Terry ', ' Willie-Sue Orr'],\n",
       "  'date': '1988',\n",
       "  'identifier': '1516049114',\n",
       "  'references': ['2037717074',\n",
       "   '2945526390',\n",
       "   '2996909114',\n",
       "   '2043671329',\n",
       "   '2095396650',\n",
       "   '2087747577',\n",
       "   '38814960',\n",
       "   '2065694966',\n",
       "   '2021100126'],\n",
       "  'title': 'Browsing Electronic Mail: Experiences Interfacing a Mail System to a DBMS'},\n",
       " {'abstract': 'The systemic comparison of every newly determined amino acid sequence with all other known sequences may allow a complete reconstruction of the evolutionary events leading to contemporary proteins. But sometimes the surviving similarities are so vague that even computer-based sequence comparisons procedures are unable to validate relationships. In other cases similar sequences may appear in totally alien proteins as a result of mere chance or, occasionally, by the convergent evolution of sequences with special properties.',\n",
       "  'authors': ['Russell F. Doolittle'],\n",
       "  'date': '1981',\n",
       "  'identifier': '2005313213',\n",
       "  'references': ['2074231493',\n",
       "   '1977618577',\n",
       "   '2135326109',\n",
       "   '1983519005',\n",
       "   '2007647936',\n",
       "   '1967627390',\n",
       "   '2044966609',\n",
       "   '2065707216',\n",
       "   '1994528559',\n",
       "   '2054931113'],\n",
       "  'title': 'Similar Amino Acid Sequences: Chance or Common Ancestry?'},\n",
       " {'abstract': \"Abstract In recent years there have been several hundred studies within the rather narrowly-defined topic of information utilization in judgment and decision making. Much of this work has been accomplished within two basic schools of research, which we have labeled the “regression” and the “Bayesian” approaches. Each has its characteristic tasks and characteristic information that must be processed to accomplish these tasks. For the most part, researchers have tended to work strictly within a single approach and there has been minimal communication between the resultant subgroups of workers. Our objective here is to present a review and comparative analysis of these two approaches. Within each, we examine (a) the models that have been developed for describing and prescribing the use of information in decision making; (b) the major experimental paradigms, including the types of judgment, prediction, and decision tasks and the kinds of information that have been available to the decision maker in these tasks; (c) the key independent variables that have been manipulated in experimental studies; and (d) the major empirical results and conclusions. In comparing these approaches, we seek the answers to two basic questions. First, do the specific models and methods characteristic of different paradigms direct the researcher's attention to certain problems and cause him to neglect others that may be equally important? Second, can a researcher studying a particular substantive problem increase his understanding by employing diverse models and diverse experimental methods?\",\n",
       "  'authors': ['Paul Slovic ', ' Sarah Lichtenstein'],\n",
       "  'date': '1971',\n",
       "  'identifier': '2079199322',\n",
       "  'references': ['1483679835',\n",
       "   '2091579301',\n",
       "   '1784695092',\n",
       "   '1984314602',\n",
       "   '2016377072',\n",
       "   '1976624377',\n",
       "   '2068984932',\n",
       "   '2795495912',\n",
       "   '2034637506',\n",
       "   '2313563931'],\n",
       "  'title': 'Comparison of Bayesian and Regression Approaches to the Study of Information Processing in Judgment.'},\n",
       " {'abstract': 'We present an approach for automatically learning to solve algebra word problems. Our algorithm reasons across sentence boundaries to construct and solve a system of linear equations, while simultaneously recovering an alignment of the variables and numbers in these equations to the problem text. The learning algorithm uses varied supervision, including either full equations or just the final answers. We evaluate performance on a newly gathered corpus of algebra word problems, demonstrating that the system can correctly answer almost 70% of the questions in the dataset. This is, to our knowledge, the first learning result for this task.',\n",
       "  'authors': ['Nate Kushman 1',\n",
       "   ' Yoav Artzi 2',\n",
       "   ' Luke Zettlemoyer 2',\n",
       "   ' Regina Barzilay 1'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2251349042',\n",
       "  'references': ['2252136820',\n",
       "   '1508977358',\n",
       "   '2163561827',\n",
       "   '2123661878',\n",
       "   '1496189301',\n",
       "   '2251673953',\n",
       "   '2189089430',\n",
       "   '2118781169',\n",
       "   '1923162067',\n",
       "   '1559723967'],\n",
       "  'title': 'Learning to Automatically Solve Algebra Word Problems'},\n",
       " {'abstract': \"Herminia Ibarra Harvard University This paper argues that two network mechanisms operate to create and reinforce gender inequalities in the organizational distribution of power: sex differences in homophily (i.e., tendency to form same-sex network relationships) and in the ability to convert individual attributes and positional resources into network advantages. These arguments were tested in a network analytic study of men's and women's interaction patterns in an advertising firm. Men were more likely to form homophilous ties across multiple networks and to have stronger homophilous ties, while women evidenced a differentiated network pattern in which they obtained social support and friendship from women and instrumental access through network ties to men. Although centrality in organization-wide networks did not vary by sex once controls were instituted, relative to women, men appeared to reap greater network returns from similar individual and positional resources, as well as from homophilous relationships.'\",\n",
       "  'authors': ['Herminia Ibarra'],\n",
       "  'date': '1992',\n",
       "  'identifier': '1997052091',\n",
       "  'references': ['2109469951',\n",
       "   '2056944867',\n",
       "   '2106446203',\n",
       "   '2058718434',\n",
       "   '2084148163',\n",
       "   '9381690',\n",
       "   '2316809388',\n",
       "   '2006766908'],\n",
       "  'title': 'Homophily and differential returns: Sex differences in network structure and access in an advertising firm.'},\n",
       " {'abstract': '',\n",
       "  'authors': ['C. Helstrom'],\n",
       "  'date': '1966',\n",
       "  'identifier': '2001301726',\n",
       "  'references': ['2228259595', '2154455356', '1985335267'],\n",
       "  'title': 'An expansion of a signal in Gaussian elementary signals (Corresp.)'},\n",
       " {'abstract': '1 Introduction.- 2 The digitized image and its properties.- 3 Data structures for image analysis.- 4 Image pre-processing.- 5 Segmentation.- 6 Shape representation and description.- 7 Object recognition.- 8 Image understanding.- 9 3D Vision.- 10 Mathematical morphology.- 11 Linear discrete image transforms.- 12 Image data compression.- 13 Texture.- 14 Motion analysis.',\n",
       "  'authors': ['Milan Sonka ', ' Vaclav Hlavac ', ' Roger Boyle'],\n",
       "  'date': '1993',\n",
       "  'identifier': '2116044718',\n",
       "  'references': ['2185396915',\n",
       "   '2605445762',\n",
       "   '2106706098',\n",
       "   '2565516711',\n",
       "   '2108556791',\n",
       "   '2168005337',\n",
       "   '2103706175',\n",
       "   '2911956715',\n",
       "   '1973178306',\n",
       "   '1974954013'],\n",
       "  'title': 'Image Processing: Analysis and Machine Vision'},\n",
       " {'abstract': \"Differential Evolution (DE) has recently proven to be an efficient method for optimizing real-valued multi-modal objective functions. Besides its good convergence properties and suitability for parallelization, DE's main assets are its conceptual simplicity and ease of use. This paper describes two variants of DE which were used to minimize the real test functions of the ICEC'96 contest.\",\n",
       "  'authors': ['R. Storn ', ' K. Price'],\n",
       "  'date': '1996',\n",
       "  'identifier': '1626834557',\n",
       "  'references': ['2089419496'],\n",
       "  'title': \"Minimizing the real functions of the ICEC'96 contest by differential evolution\"},\n",
       " {'abstract': 'We introduce a new class of problems called network information flow which is inspired by computer network applications. Consider a point-to-point communication network on which a number of information sources are to be multicast to certain sets of destinations. We assume that the information sources are mutually independent. The problem is to characterize the admissible coding rate region. This model subsumes all previously studied models along the same line. We study the problem with one information source, and we have obtained a simple characterization of the admissible coding rate region. Our result can be regarded as the max-flow min-cut theorem for network information flow. Contrary to one\\'s intuition, our work reveals that it is in general not optimal to regard the information to be multicast as a \"fluid\" which can simply be routed or replicated. Rather, by employing coding at the nodes, which we refer to as network coding, bandwidth can in general be saved. This finding may have significant impact on future design of switching systems.',\n",
       "  'authors': ['R. Ahlswede 1',\n",
       "   ' Ning Cai 2',\n",
       "   ' S.-Y.R. Li 2',\n",
       "   ' R.W. Yeung 2'],\n",
       "  'date': '2000',\n",
       "  'identifier': '2105831729',\n",
       "  'references': ['2137813581',\n",
       "   '2106403318',\n",
       "   '2120085609',\n",
       "   '2101526130',\n",
       "   '2170184107',\n",
       "   '2142511915',\n",
       "   '2099213070',\n",
       "   '1515701089',\n",
       "   '2123095296',\n",
       "   '2050687505'],\n",
       "  'title': 'Network information flow'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Elizabeth C. Hirschman ',\n",
       "   ' Daniel Kahneman ',\n",
       "   ' Paul Slovic ',\n",
       "   ' Amos Tversky'],\n",
       "  'date': '1983',\n",
       "  'identifier': '2724224207',\n",
       "  'references': ['2760977209',\n",
       "   '2133328722',\n",
       "   '2166982098',\n",
       "   '2084335597',\n",
       "   '1562463929',\n",
       "   '2343630645',\n",
       "   '2095561982',\n",
       "   '2106919063',\n",
       "   '1496684753',\n",
       "   '2115411156'],\n",
       "  'title': 'Judgement under Uncertainty: Heuristics and Biases'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Chris Birchenhall ',\n",
       "   ' William H. Press ',\n",
       "   ' Saul A. Teukolsky ',\n",
       "   ' William T. Vetterling ',\n",
       "   ' Brian P. Flannery'],\n",
       "  'date': '1994',\n",
       "  'identifier': '2313307644',\n",
       "  'references': ['2139212933',\n",
       "   '2217896605',\n",
       "   '2132103241',\n",
       "   '2000042664',\n",
       "   '2147555557',\n",
       "   '2135029798',\n",
       "   '2127774996',\n",
       "   '2107551484',\n",
       "   '2186428165',\n",
       "   '2034432063'],\n",
       "  'title': 'Numerical Recipes in C: The Art of Scientific Computing'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Loren Terveen 1',\n",
       "   ' Will Hill 1',\n",
       "   ' Brian Amento 2',\n",
       "   ' David McDonald 3',\n",
       "   ' Josh Creter 4'],\n",
       "  'date': '1997',\n",
       "  'identifier': '2066590388',\n",
       "  'references': ['2155106456',\n",
       "   '2124591829',\n",
       "   '2030144199',\n",
       "   '2013594690',\n",
       "   '1560991565'],\n",
       "  'title': 'PHOAKS: a system for sharing recommendations'},\n",
       " {'abstract': 'Abstract A new measure of event-related brain dynamics, the event-related spectral perturbation (ERSP), is introduced to study event-related dynamics of the EEG spectrum induced by, but not phase-locked to, the onset of the auditory stimuli. The ERSP reveals aspects of event-related brain dynamics not contained in the ERP average of the same response epochs. Twenty-eight subjects participated in daily auditory evoked response experiments during a 4 day study of the effects of 24 h free-field exposure to intermittent trains of 89 dB low frequency tones. During evoked response testing, the same tones were presented through headphones in random order at 5 sec intervals. No significant changes in behavioral thresholds occurred during or after free-field exposure. ERSPs induced by target pips presented in some inter-tone intervals were larger than, but shared common features with, ERSPs induced by the tones, most prominently a ridge of augmented EEG amplitude from 11 to 18 Hz, peaking 1–1.5 sec after stimulus onset. Following 3–11 h of free-field exposure, this feature was significantly smaller in tone-induced ERSPs; target-induced ERSPs were not similarly affected. These results, therefore, document systematic effects of exposure to intermittent tones on EEG brain dynamics even in the absence of changes in auditory thresholds.',\n",
       "  'authors': ['Scott Makeig'],\n",
       "  'date': '1993',\n",
       "  'identifier': '2100741386',\n",
       "  'references': ['2057213277',\n",
       "   '2079173805',\n",
       "   '1990973912',\n",
       "   '2045103094',\n",
       "   '2126666959',\n",
       "   '2164470519',\n",
       "   '2094688616',\n",
       "   '2085497225',\n",
       "   '181468950',\n",
       "   '2083537734'],\n",
       "  'title': 'Auditory Event-Related Dynamics of the EEG Spectrum and Effects of Exposure to Tones'},\n",
       " {'abstract': 'This paper briefly describes the major features of the DRAGON speech understanding system. DRAGON makes systematic use of a general abstract model to represent each of the knowledge sources necessary for automatic recognition of continuous speech. The model--that of a probabilistic function of a Markov process--is very flexible and leads to features which allow DRAGON to function despite high error rates from individual knowledge sources. Repeated use of a simple abstract model produces a system which is simple in structure, but powerful in capabilities.',\n",
       "  'authors': ['J. Baker'],\n",
       "  'date': '1975',\n",
       "  'identifier': '2163929346',\n",
       "  'references': ['2341171179',\n",
       "   '2077574412',\n",
       "   '2110693321',\n",
       "   '2138250146',\n",
       "   '303664091'],\n",
       "  'title': 'The DRAGON system--An overview'},\n",
       " {'abstract': 'Abstract A new method for nonparametric multiple regression is presented. The procedure models the regression surface as a sum of general smooth functions of linear combinations of the predictor variables in an iterative manner. It is more general than standard stepwise and stagewise regression procedures, does not require the definition of a metric in the predictor space, and lends itself to graphical interpretation.',\n",
       "  'authors': ['Jerome H. Friedman ', ' Werner Stuetzle'],\n",
       "  'date': '1981',\n",
       "  'identifier': '2091886411',\n",
       "  'references': ['2319794630',\n",
       "   '2024081693',\n",
       "   '2059507684',\n",
       "   '2026258334',\n",
       "   '2082612735',\n",
       "   '1979519992',\n",
       "   '2478937241',\n",
       "   '168150807',\n",
       "   '2146434287',\n",
       "   '1994753884'],\n",
       "  'title': 'Projection Pursuit Regression'},\n",
       " {'abstract': 'In this paper we show how wavelets can be used for data segmentation. The basic idea is to split the data into smooth segments that can be compressed separately. A fast algorithm that uses wavelets on closed sets and wavelet probing is presented.© (1993) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.',\n",
       "  'authors': ['Baiqiao Deng ',\n",
       "   ' Bjorn D. Jawerth ',\n",
       "   ' Gunnar Peters ',\n",
       "   ' Wim Sweldens'],\n",
       "  'date': '1993',\n",
       "  'identifier': '2017366455',\n",
       "  'references': ['1485280399',\n",
       "   '2020919250',\n",
       "   '1556281046',\n",
       "   '2106016501',\n",
       "   '2143386239',\n",
       "   '2023797760',\n",
       "   '2076506581',\n",
       "   '2144793402',\n",
       "   '1596818115',\n",
       "   '2138593203'],\n",
       "  'title': 'Wavelet probing for compression-based segmentation'},\n",
       " {'abstract': 'Background: This study presents estimates of lifetime and 12-month prevalence of 14 DSM-III-R psychiatric disorders from the National Comorbidity Survey, the first survey to administer a structured psychiatric interview to a national probability sample in the United States. Methods: The DSM-III-R psychiatric disorders among persons aged 15 to 54 years in the noninstitutionalized civilian population of the United States were assessed with data collected by lay interviewers using a revised version of the Composite International Diagnostic Interview. Results: Nearly 50% of respondents reported at least one lifetime disorder, and close to 30% reported at least one 12-month disorder. The most common disorders were major depressive episode, alcohol dependence, social phobia, and simple phobia. More than half of all lifetime disorders occurred in the 14% of the population who had a history of three or more comorbid disorders. These highly comorbid people also included the vast majority of people with severe disorders. Less than 40% of those with a lifetime disorder had ever received professional treatment, and less than 20% of those with a recent disorder had been in treatment during the past 12 months. Consistent with previous risk factor research, it was found that women had elevated rates of affective disorders and anxiety disorders, that men had elevated rates of substance use disorders and antisocial personality disorder, and that most disorders declined with age and with higher socioeconomic status. Conclusions: The prevalence of psychiatric disorders is greater than previously thought to be the case. Furthermore, this morbidity is more highly concentrated than previously recognized in roughly one sixth of the population who have a history of three or more comorbid disorders. This suggests that the causes and consequences of high comorbidity should be the focus of research attention. The majority of people with psychiatric disorders fail to obtain professional treatment. Even among people with a lifetime history of three or more comorbid disorders, the proportion who ever obtain specialty sector mental health treatment is less than 50%. These results argue for the importance of more outreach and more research on barriers to professional help-seeking.',\n",
       "  'authors': ['Ronald C. Kessler 1',\n",
       "   ' Katherine A. McGonagle 2',\n",
       "   ' Shanyang Zhao 2',\n",
       "   ' Christopher B. Nelson 2',\n",
       "   ' Michael Hughes 2',\n",
       "   ' Suzann Eshleman 2',\n",
       "   ' Hans-Ulrich Wittchen 2',\n",
       "   ' Kenneth S. Kendler 2'],\n",
       "  'date': '1994',\n",
       "  'identifier': '2159061031',\n",
       "  'references': ['1966460145',\n",
       "   '2060711552',\n",
       "   '2050768782',\n",
       "   '2022496892',\n",
       "   '2963108147',\n",
       "   '2114334028',\n",
       "   '2060200544',\n",
       "   '2148412563',\n",
       "   '2138202604',\n",
       "   '2107182280'],\n",
       "  'title': 'Lifetime and 12-Month Prevalence of DSM-III-R Psychiatric Disorders in the United States: Results From the National Comorbidity Survey'},\n",
       " {'abstract': 'A new class of convolutional codes called turbo-codes, whose performances in terms of bit error rate (BER) are close to the Shannon limit, is discussed. The turbo-code encoder is built using a parallel concatenation of two recursive systematic convolutional codes, and the associated decoder, using a feedback decoding rule, is implemented as P pipelined identical elementary decoders. >',\n",
       "  'authors': ['C. Berrou ', ' A. Glavieux ', ' P. Thitimajshima'],\n",
       "  'date': '1993',\n",
       "  'identifier': '2121606987',\n",
       "  'references': ['2154185977', '1562979145', '2045407304'],\n",
       "  'title': 'Near Shannon limit error-correcting coding and decoding: Turbo-codes. 1'},\n",
       " {'abstract': \"The information revolution has made for a radically more fluid knowledge environment, and the growth of venture capital has created inexorable pressure towards fast commercialisation of existing technologies Companies that don't use the technologies they develop are likely to lose them. Key features Over the past several years, Hank Chesbrough has done excellent research and writing on the commercialisation of technology and the changing role and context for R&D. This book represents a powerful synthesis of that work in the form of a new paradigm for managing corporate research and bringing new technologies to market Chesbrough impressively articulates his ideas and how they connect to each other, weaving several disparate areas of work R&D, corporate venturing, spinoffs, licensing and intellectual property into a single coherent framework.\",\n",
       "  'authors': ['Henry William Chesbrough'],\n",
       "  'date': '2003',\n",
       "  'identifier': '1561725974',\n",
       "  'references': ['2169667133',\n",
       "   '2136071264',\n",
       "   '2132789787',\n",
       "   '1553746973',\n",
       "   '167368901',\n",
       "   '2100967449',\n",
       "   '2018915326',\n",
       "   '1986342936',\n",
       "   '1964487809',\n",
       "   '1979461265'],\n",
       "  'title': 'Open Innovation: The New Imperative for Creating and Profiting from Technology'},\n",
       " {'abstract': 'This paper deals with two experiments with a large vocabulary isolated word recognizer. The first compares word error rates for 1) meaningful sentences belonging to actual documents and 2) random word lists from the same vocabulary. The error rate is considerably lower for random word lists. The second experiment investigates the performance of the recognition system on sentences containing words outside the vocabulary of the recognizer. Sentences from a 5000 word vocabulary task are recognized with a recognizer limited to a 2000 word subvocabulary. The error rate is only slightly higher than it would be if recognition of the full 5000 word vocabulary was allowed.',\n",
       "  'authors': ['L. Bahl ',\n",
       "   ' S. Das ',\n",
       "   ' P. de Souza ',\n",
       "   ' F. Jelinek ',\n",
       "   ' S. Katz ',\n",
       "   ' R. Mercer ',\n",
       "   ' M. Picheny'],\n",
       "  'date': '1984',\n",
       "  'identifier': '2099074650',\n",
       "  'references': ['1966812932',\n",
       "   '2583466288',\n",
       "   '1575431606',\n",
       "   '1847689439',\n",
       "   '1579558060',\n",
       "   '2137309058',\n",
       "   '1597694191',\n",
       "   '2099735873'],\n",
       "  'title': 'Some experiments with large-vocabulary isolated-word sentence recognition'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Marko Balabanović ', ' Yoav Shoham'],\n",
       "  'date': '1997',\n",
       "  'identifier': '2043403353',\n",
       "  'references': ['2155106456',\n",
       "   '2124591829',\n",
       "   '2030144199',\n",
       "   '1493526108',\n",
       "   '2137719099',\n",
       "   '2085419413',\n",
       "   '2069564398',\n",
       "   '2031459112'],\n",
       "  'title': 'Fab: content-based, collaborative recommendation'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Ronald Aylmer Fisher'],\n",
       "  'date': '1936',\n",
       "  'identifier': '2001619934',\n",
       "  'references': ['2027987815'],\n",
       "  'title': 'THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS'},\n",
       " {'abstract': '',\n",
       "  'authors': ['D. Gabor'],\n",
       "  'date': '1947',\n",
       "  'identifier': '2154455356',\n",
       "  'references': ['2177274842',\n",
       "   '2078204800',\n",
       "   '2000982976',\n",
       "   '2141983208',\n",
       "   '2107554012',\n",
       "   '2144982973',\n",
       "   '2046079134',\n",
       "   '1996021349'],\n",
       "  'title': 'Theory of communication'},\n",
       " {'abstract': 'This paper is motivated by the need for fundamental understanding of ultimate limits of bandwidth efficient delivery of higher bit-rates in digital wireless communications and to also begin to look into how these limits might be approached. We examine exploitation of multi-element array (MEA) technology, that is processing the spatial dimension (not just the time dimension) to improve wireless capacities in certain applications. Specifically, we present some basic information theory results that promise great advantages of using MEAs in wireless LANs and building to building wireless communication links. We explore the important case when the channel characteristic is not available at the transmitter but the receiver knows (tracks) the characteristic which is subject to Rayleigh fading. Fixing the overall transmitted power, we express the capacity offered by MEA technology and we see how the capacity scales with increasing SNR for a large but practical number, n, of antenna elements at both transmitter and receiver. We investigate the case of independent Rayleigh faded paths between antenna elements and find that with high probability extraordinary capacity is available. Compared to the baseline n = 1 case, which by Shannon‘s classical formula scales as one more bit/cycle for every 3 dB of signal-to-noise ratio (SNR) increase, remarkably with MEAs, the scaling is almost like n more bits/cycle for each 3 dB increase in SNR. To illustrate how great this capacity is, even for small n, take the cases n = 2, 4 and 16 at an average received SNR of 21 dB. For over 99% of the channels the capacity is about 7, 19 and 88 bits/cycle respectively, while if n = 1 there is only about 1.2 bit/cycle at the 99% level. For say a symbol rate equal to the channel bandwith, since it is the bits/symbol/dimension that is relevant for signal constellations, these higher capacities are not unreasonable. The 19 bits/cycle for n = 4 amounts to 4.75 bits/symbol/dimension while 88 bits/cycle for n = 16 amounts to 5.5 bits/symbol/dimension. Standard approaches such as selection and optimum combining are seen to be deficient when compared to what will ultimately be possible. New codecs need to be invented to realize a hefty portion of the great capacity promised.',\n",
       "  'authors': ['G. J. Foschini ', ' M. J. Gans'],\n",
       "  'date': '1998',\n",
       "  'identifier': '1667950888',\n",
       "  'references': ['2121606987',\n",
       "   '1549664537',\n",
       "   '2128978199',\n",
       "   '1596939795',\n",
       "   '2021573106',\n",
       "   '2165205968',\n",
       "   '2139593345',\n",
       "   '2142901448',\n",
       "   '2099096090',\n",
       "   '1974755392'],\n",
       "  'title': 'On Limits of Wireless Communications in a Fading Environment when UsingMultiple Antennas'},\n",
       " {'abstract': \"Abstract We describe segmented multiresolution analyses of [0, 1]. Such multiresolution analyses lead to segmented wavelet bases which are adapted to discontinuities, cusps, etc., at a given location τ ∈ [0, 1]. Our approach emphasizes the idea of average-interpolation — synthesizing a smooth function on the line having prescribed boxcar averages. This particular approach leads to methods with subpixel resolution and to wavelet transforms with the advantage that, for a signal of length n , all n pixel-level segmented wavelet transforms can be computed simultaneously in a total time and space which are both O ( n log( n )). We consider the search for a segmented wavelet basis which, among all such segmented bases, minimizes the “entropy” of the resulting coefficients. Fast access to all segmentations enables fast search for a best segmentation. When the “entropy” is Stein's Unbiased Risk Estimate, one obtains a new method of edge-preserving de-noising. When the “entropy” is the l 2 -energy, one obtains a new multi-resolution edge detector, which works not only for step discontinuities but also for cusp and higher-order discontinuities, and in a near-optimal fashion in the presence of noise. We describe an iterative approach, Segmentation Pursuit , for identifying edges by the fast segmentation algorithm and removing them from the data.\",\n",
       "  'authors': ['David L. Donoho'],\n",
       "  'date': '1994',\n",
       "  'identifier': '1556281046',\n",
       "  'references': ['2151693816',\n",
       "   '1489213177',\n",
       "   '2079724595',\n",
       "   '2148593155',\n",
       "   '2156447271',\n",
       "   '3005363104',\n",
       "   '191129667',\n",
       "   '2094585768',\n",
       "   '2166087152',\n",
       "   '2050880896'],\n",
       "  'title': 'On Minimum Entropy Segmentation'},\n",
       " {'abstract': 'In an earlier paper, we introduced a new \"boosting\" algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that con- sistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a \"pseudo-loss\" which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman\\'s \"bagging\" method when used to aggregate various classifiers (including decision trees and single attribute- value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem.',\n",
       "  'authors': ['Yoav Freund ', ' Robert E. Schapire'],\n",
       "  'date': '1996',\n",
       "  'identifier': '2112076978',\n",
       "  'references': ['1988790447',\n",
       "   '2912934387',\n",
       "   '2125055259',\n",
       "   '1504694836',\n",
       "   '1670263352',\n",
       "   '1966280301',\n",
       "   '2093717447',\n",
       "   '2132166479',\n",
       "   '2070534370',\n",
       "   '2137291015'],\n",
       "  'title': 'Experiments with a new boosting algorithm'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Philip Clarkson ', ' Ronald Rosenfeld'],\n",
       "  'date': '1997',\n",
       "  'identifier': '1549285799',\n",
       "  'references': ['2153653739',\n",
       "   '1631260214',\n",
       "   '1916559533',\n",
       "   '2143017621',\n",
       "   '2142069714',\n",
       "   '2158195707',\n",
       "   '2134800885',\n",
       "   '2159981908',\n",
       "   '2116316001',\n",
       "   '2161792612'],\n",
       "  'title': 'Statistical Language Modeling using the CMU-Cambridge Toolkit'},\n",
       " {'abstract': 'Cluster analysis is the automated search for groups of related observations in a dataset. Most clustering done in practice is based largely on heuristic but intuitively reasonable procedures, and most clustering methods available in commercial software are also of this type. However, there is little systematic guidance associated with these methods for solving important practical questions that arise in cluster analysis, such as how many clusters are there, which clustering method should be used, and how should outliers be handled. We review a general methodology for model-based clustering that provides a principled statistical approach to these issues. We also show that this can be useful for other problems in multivariate analysis, such as discriminant analysis and multivariate density estimation. We give examples from medical diagnosis, minefield detection, cluster recovery from noisy data, and spatial density estimation. Finally, we mention limitations of the methodology and discuss recent development...',\n",
       "  'authors': ['Chris Fraley ', ' Adrian E Raftery'],\n",
       "  'date': '2002',\n",
       "  'identifier': '2011832962',\n",
       "  'references': ['2150926065',\n",
       "   '2109363337',\n",
       "   '2117812871',\n",
       "   '2147246240',\n",
       "   '2049633694',\n",
       "   '2059334100',\n",
       "   '2087684630',\n",
       "   '2109970232',\n",
       "   '2135187880',\n",
       "   '2117853077'],\n",
       "  'title': 'Model-Based Clustering, Discriminant Analysis, and Density Estimation'},\n",
       " {'abstract': 'Motivation: Chimeric DNA sequences often form during polymerase chain reaction amplification, especially when sequencing single regions (e.g. 16S rRNA or fungal Internal Transcribed Spacer) to assess diversity or compare populations. Undetected chimeras may be misinterpreted as novel species, causing inflated estimates of diversity and spurious inferences of differences between populations. Detection and removal of chimeras is therefore of critical importance in such experiments. Results: We describe UCHIME, a new program that detects chimeric sequences with two or more segments. UCHIME either uses a database of chimera-free sequences or detects chimeras de novo by exploiting abundance data. UCHIME has better sensitivity than ChimeraSlayer (previously the most sensitive database method), especially with short, noisy sequences. In testing on artificial bacterial communities with known composition, UCHIME de novo sensitivity is shown to be comparable to Perseus. UCHIME is >100× faster than Perseus and >1000× faster than ChimeraSlayer. Contact: [email protected] Availability: Source, binaries and data: http://drive5.com/uchime. Supplementary information:Supplementary data are available at Bioinformatics online.',\n",
       "  'authors': ['Robert C. Edgar ',\n",
       "   ' Brian J. Haas ',\n",
       "   ' Jose C. Clemente ',\n",
       "   ' Christopher Quince ',\n",
       "   ' Rob Knight'],\n",
       "  'date': '2011',\n",
       "  'identifier': '2136879569',\n",
       "  'references': ['2158714788',\n",
       "   '2124351063',\n",
       "   '2155066423',\n",
       "   '2101120467',\n",
       "   '2119362018',\n",
       "   '2106308757',\n",
       "   '2132598047',\n",
       "   '2890040444',\n",
       "   '2158421363',\n",
       "   '2170052798'],\n",
       "  'title': 'UCHIME improves sensitivity and speed of chimera detection'},\n",
       " {'abstract': 'Full Factorial Designs.- to Full Factorial Designs with Two-Level Factors.- Analysis of Full Factorial Experiments.- Common Randomization Restrictions.- More Full Factorial Design Examples.- Fractional Factorial Designs.- Fractional Factorial Designs: The Basics.- Fractional Factorial Designs for Estimating Main Effects.- Designs for Estimating Main Effects and Some Two-Factor Interactions.- Resolution V Fractional Factorial Designs.- Augmenting Fractional Factorial Designs.- Fractional Factorial Designs with Randomization Restrictions.- More Fractional Factorial Design Examples.- Additional Topics.- Response Surface Methods and Second-Order Designs.- Special Topics Regarding the Design.- Special Topics Regarding the Analysis.- Appendices and Tables.- Upper Percentiles of t Distributions, t.- Upper Percentiles of F Distributions, F.- Upper Percentiles for Lenth t Statistics, and.- Computing Upper Percentiles for Maximum Studentized Residual.- Orthogonal Blocking for Full 2 Factorial Designs.- Column Labels of Generators for Regular Fractional Factorial Designs.- Tables of Minimum Aberration Regular Fractional Factorial Designs.- Minimum Aberration Blocking Schemes for Fractional Factorial Designs.- Alias Matrix Derivation.- Distinguishing Among Fractional Factorial Designs.',\n",
       "  'authors': ['Robert W. Mee'],\n",
       "  'date': '2009',\n",
       "  'identifier': '611766052',\n",
       "  'references': ['651150315',\n",
       "   '570925313',\n",
       "   '1566093225',\n",
       "   '2078495506',\n",
       "   '2120637011',\n",
       "   '2068928060',\n",
       "   '2054085683',\n",
       "   '2111499032',\n",
       "   '2023039737',\n",
       "   '2107673074'],\n",
       "  'title': 'A Comprehensive Guide to Factorial Two-Level Experimentation'},\n",
       " {'abstract': '',\n",
       "  'authors': ['James A. Russell'],\n",
       "  'date': '1980',\n",
       "  'identifier': '2149628368',\n",
       "  'references': ['650744393',\n",
       "   '199743680',\n",
       "   '2082550766',\n",
       "   '1777859530',\n",
       "   '2093862925',\n",
       "   '1986936900',\n",
       "   '2056418346',\n",
       "   '2046220546',\n",
       "   '2046677541',\n",
       "   '2169371330'],\n",
       "  'title': 'A circumplex model of affect.'},\n",
       " {'abstract': 'We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition. We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output. The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error. We illustrate the key components of our model, describe the procedure for applying CD-DNN-HMMs to LVSR, and analyze the effects of various modeling choices on performance. Experiments on a challenging business search dataset demonstrate that CD-DNN-HMMs can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs, with an absolute sentence accuracy improvement of 5.8% and 9.2% (or relative error reduction of 16.0% and 23.2%) over the CD-GMM-HMMs trained using the minimum phone error rate (MPE) and maximum-likelihood (ML) criteria, respectively.',\n",
       "  'authors': ['G. E. Dahl 1', ' Dong Yu 2', ' Li Deng 2', ' A. Acero 2'],\n",
       "  'date': '2012',\n",
       "  'identifier': '2147768505',\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2072128103',\n",
       "   '1533861849',\n",
       "   '2116064496',\n",
       "   '2117130368',\n",
       "   '2546302380',\n",
       "   '2025768430',\n",
       "   '1993882792',\n",
       "   '1498436455'],\n",
       "  'title': 'Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition'},\n",
       " {'abstract': \"Learning by observation involves automatic creation of categories that summarize experience. In this paper we present UNIMEM, an artificial intelligence system that learns by observation. UNIMEM is a robust program that can be run on many domains with real-world problem characteristics such as uncertainty, incompleteness, and large numbers of examples. We give an overview of the program that illustrates several key elements, including the automatic creation of non-disjoint concept hierarchies that are evaluated over time. We then describe several experiments that we have carried out with UNIMEM, including tests on different domains (universities, Congressional voting records, and terrorist events) and an examination of the effect of varying UNIMEM's parameters on the resulting concept hierarchies. Finally we discuss future directions for our work with the program.\",\n",
       "  'authors': ['Michael Lebowitz'],\n",
       "  'date': '1987',\n",
       "  'identifier': '2024868117',\n",
       "  'references': ['2073308541',\n",
       "   '1596324102',\n",
       "   '2121773050',\n",
       "   '2180885055',\n",
       "   '2118587067',\n",
       "   '2009207944',\n",
       "   '2101602574',\n",
       "   '1525136198',\n",
       "   '1970185999',\n",
       "   '1488252886'],\n",
       "  'title': 'Experiments with Incremental Concept Formation: UNIMEM'},\n",
       " {'abstract': 'Suppose you are given some data set drawn from an underlying probability distribution P and you want to estimate a \"simple\" subset S of input space such that the probability that a test point drawn from P lies outside of S equals some a priori specified value between 0 and 1. We propose a method to approach this problem by trying to estimate a function f that is positive on S and negative on the complement. The functional form of f is given by a kernel expansion in terms of a potentially small subset of the training data; it is regularized by controlling the length of the weight vector in an associated feature space. The expansion coefficients are found by solving a quadratic programming problem, which we do by carrying out sequential optimization over pairs of input patterns. We also provide a theoretical analysis of the statistical performance of our algorithm. The algorithm is a natural extension of the support vector algorithm to the case of unlabeled data.',\n",
       "  'authors': ['Bernhard Schölkopf 1',\n",
       "   ' John C. Platt 1',\n",
       "   ' John C. Shawe-Taylor 2',\n",
       "   ' Alex J. Smola 3',\n",
       "   ' Robert C. Williamson 3'],\n",
       "  'date': '2001',\n",
       "  'identifier': '2132870739',\n",
       "  'references': ['2156909104',\n",
       "   '2148603752',\n",
       "   '2099111195',\n",
       "   '2798766386',\n",
       "   '3021469268',\n",
       "   '1512098439',\n",
       "   '2087347434',\n",
       "   '1604938182',\n",
       "   '2108995755',\n",
       "   '1576520375'],\n",
       "  'title': 'Estimating the Support of a High-Dimensional Distribution'},\n",
       " {'abstract': 'We present a technique for automatically assigning a neuroanatomical label to each location on a cortical surface model based on probabilistic information estimated from a manually labeled training set. This procedure incorporates both geometric information derived from the cortical model, and neuroanatomical convention, as found in the training set. The result is a complete labeling of cortical sulci and gyri. Examples are given from two different training sets generated using different neuroanatomical conventions, illustrating the flexibility of the algorithm. The technique is shown to be comparable in accuracy to manual labeling.',\n",
       "  'authors': ['Bruce Fischl ',\n",
       "   ' André van der Kouwe ',\n",
       "   ' Christophe Destrieux ',\n",
       "   ' Eric Halgren ',\n",
       "   ' Florent Ségonne ',\n",
       "   ' David H. Salat ',\n",
       "   ' Evelina Busa ',\n",
       "   ' Larry J. Seidman ',\n",
       "   ' Jill Goldstein ',\n",
       "   ' David Kennedy ',\n",
       "   ' Verne Caviness ',\n",
       "   ' Nikos Makris ',\n",
       "   ' Bruce Rosen ',\n",
       "   ' Anders M. Dale'],\n",
       "  'date': '2004',\n",
       "  'identifier': '2151130155',\n",
       "  'references': ['1997063559',\n",
       "   '2319937903',\n",
       "   '2136573752',\n",
       "   '2151721316',\n",
       "   '2004293194',\n",
       "   '2113319997',\n",
       "   '2110208125',\n",
       "   '2141796362',\n",
       "   '2128409098',\n",
       "   '2051691068'],\n",
       "  'title': 'Automatically Parcellating the Human Cerebral Cortex'},\n",
       " {'abstract': \"The relative efficiency of any particular image-coding scheme should be defined only in relation to the class of images that the code is likely to encounter. To understand the representation of images by the mammalian visual system, it might therefore be useful to consider the statistics of images from the natural environment (i.e., images with trees, rocks, bushes, etc). In this study, various coding schemes are compared in relation to how they represent the information in such natural images. The coefficients of such codes are represented by arrays of mechanisms that respond to local regions of space, spatial frequency, and orientation (Gabor-like transforms). For many classes of image, such codes will not be an efficient means of representing information. However, the results obtained with six natural images suggest that the orientation and the spatial-frequency tuning of mammalian simple cells are well suited for coding the information in such images if the goal of the code is to convert higher-order redundancy (e.g., correlation between the intensities of neighboring pixels) into first-order redundancy (i.e., the response distribution of the coefficients). Such coding produces a relatively high signal-to-noise ratio and permits information to be transmitted with only a subset of the total number of cells. These results support Barlow's theory that the goal of natural vision is to represent the information in the natural environment with minimal redundancy.\",\n",
       "  'authors': ['David J. Field'],\n",
       "  'date': '1987',\n",
       "  'identifier': '2167034998',\n",
       "  'references': ['2003370853',\n",
       "   '2006500012',\n",
       "   '1995875735',\n",
       "   '2078498116',\n",
       "   '1499486838',\n",
       "   '2116360511',\n",
       "   '2135587681',\n",
       "   '2138100172',\n",
       "   '1999908130',\n",
       "   '1964415410'],\n",
       "  'title': 'Relations between the statistics of natural images and the response properties of cortical cells.'},\n",
       " {'abstract': 'This book will be most useful to applied mathematicians, communication engineers, signal processors, statisticians, and time series researchers, both applied and theoretical. Readers should have some background in complex function theory and matrix algebra and should have successfully completed the equivalent of an upper division course in statistics.',\n",
       "  'authors': ['David R. Brillinger'],\n",
       "  'date': '1981',\n",
       "  'identifier': '2098301339',\n",
       "  'references': ['1660562555',\n",
       "   '2099741732',\n",
       "   '1525535255',\n",
       "   '210359992',\n",
       "   '2158336491',\n",
       "   '2153675946',\n",
       "   '1582484699',\n",
       "   '2295124130',\n",
       "   '2014165366',\n",
       "   '1543237449'],\n",
       "  'title': 'Time Series: Data Analysis and Theory'},\n",
       " {'abstract': '',\n",
       "  'authors': ['David G. Luenberger ', ' Yinyu Ye'],\n",
       "  'date': '2008',\n",
       "  'identifier': '3023695801',\n",
       "  'references': ['2159687282',\n",
       "   '2171048418',\n",
       "   '1883186006',\n",
       "   '2146000945',\n",
       "   '2020833085',\n",
       "   '2580753685',\n",
       "   '88520345',\n",
       "   '2157988812',\n",
       "   '1602182271',\n",
       "   '3102961917'],\n",
       "  'title': 'Linear and Nonlinear Programming'},\n",
       " {'abstract': 'The problem we are addressing in Alvey Project MMI149 is that of using computer vision to understand the unconstrained 3D world, in which the viewed scenes will in general contain too wide a diversity of objects for topdown recognition techniques to work. For example, we desire to obtain an understanding of natural scenes, containing roads, buildings, trees, bushes, etc., as typified by the two frames from a sequence illustrated in Figure 1. The solution to this problem that we are pursuing is to use a computer vision system based upon motion analysis of a monocular image sequence from a mobile camera. By extraction and tracking of image features, representations of the 3D analogues of these features can be constructed.',\n",
       "  'authors': ['Christopher G. Harris ', ' Mike Stephens'],\n",
       "  'date': '1988',\n",
       "  'identifier': '2111308925',\n",
       "  'references': ['1639227073',\n",
       "   '1756736144',\n",
       "   '2063599328',\n",
       "   '2048192053',\n",
       "   '2039106392',\n",
       "   '2997169974'],\n",
       "  'title': 'A COMBINED CORNER AND EDGE DETECTOR'},\n",
       " {'abstract': 'This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 % recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/~uijlings/SelectiveSearch.html ).',\n",
       "  'authors': ['J. R. Uijlings 1',\n",
       "   ' K. E. Sande 2',\n",
       "   ' T. Gevers 2',\n",
       "   ' A. W. Smeulders 2'],\n",
       "  'date': '2013',\n",
       "  'identifier': '2088049833',\n",
       "  'references': ['2151103935',\n",
       "   '2161969291',\n",
       "   '2168356304',\n",
       "   '2164598857',\n",
       "   '2031489346',\n",
       "   '3097096317',\n",
       "   '2162915993',\n",
       "   '2163352848',\n",
       "   '2067191022',\n",
       "   '2121947440'],\n",
       "  'title': 'Selective Search for Object Recognition'},\n",
       " {'abstract': 'This work was supported in part the Royal Society of the UK, the National Natural Science Foundation of China under Grants 61329301, 61374010, and 61403319, and the Alexander von Humboldt Foundation of Germany.',\n",
       "  'authors': ['Weibo Liu 1',\n",
       "   ' Zidong Wang 1',\n",
       "   ' Xiaohui Liu 1',\n",
       "   ' Nianyin Zeng 2',\n",
       "   ' Yurong Liu 3',\n",
       "   ' 4',\n",
       "   ' Fuad E. Alsaadi 4'],\n",
       "  'date': '2017',\n",
       "  'identifier': '2565516711',\n",
       "  'references': ['2618530766',\n",
       "   '2919115771',\n",
       "   '2108598243',\n",
       "   '1663973292',\n",
       "   '2168356304',\n",
       "   '2136922672',\n",
       "   '2257979135',\n",
       "   '2100495367',\n",
       "   '2310919327',\n",
       "   '2076063813'],\n",
       "  'title': 'A survey of deep neural network architectures and their applications'},\n",
       " {'abstract': \"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability. The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.\",\n",
       "  'authors': ['R.S. Sutton ', ' A.G. Barto'],\n",
       "  'date': '1988',\n",
       "  'identifier': '2121863487',\n",
       "  'references': ['1639032689',\n",
       "   '2154642048',\n",
       "   '3017143921',\n",
       "   '2100677568',\n",
       "   '1535810436',\n",
       "   '1603765807',\n",
       "   '3011120880',\n",
       "   '1569320505',\n",
       "   '94523489',\n",
       "   '1540723801'],\n",
       "  'title': 'Reinforcement Learning: An Introduction'},\n",
       " {'abstract': 'This paper presents a review of techniques used for digital encoding of picture material. Statistical models of picture signals and elements of psychophysics relevant to picture coding are covered first, followed by a description of the coding techniques. Detailed examples of three typical systems, which combine some of the coding principles, are given. A bright future for new systems is forecasted based on emerging new concepts, technology of integrated circuits and the need to digitize in a variety of contexts.',\n",
       "  'authors': ['A.N. Netravali ', ' J.O. Limb'],\n",
       "  'date': '1980',\n",
       "  'identifier': '2078498116',\n",
       "  'references': ['2031614119',\n",
       "   '2105815873',\n",
       "   '3023802253',\n",
       "   '2067265395',\n",
       "   '2118274709',\n",
       "   '1999908130',\n",
       "   '2150912583',\n",
       "   '2109808436',\n",
       "   '2134809980',\n",
       "   '2015157402'],\n",
       "  'title': 'Picture coding: A review'},\n",
       " {'abstract': 'The problem of quantitatively comparing the performance of different broad-coverage grammars of English has to date resisted solution. Prima facie, known English grammars appear to disagree strongly with each other as to the elements of even the simplest sentences. For instance, the grammars of Steve Abney (Bellcore), Ezra Black (IBM), Dan Flickinger (Hewlett Packard), Claudia Gdaniec (Logos), Ralph Grishman and Tomek Strzalkowski (NYU), Phil Harrison (Boeing), Don Hindle (AT&T), Bob Ingria (BBN), and Mitch Marcus (U. of Pennsylvania) recognize in common only the following constituents, when each grammarian provides the single parse which he/she would ideally want his/her grammar to specify for three sample Brown Corpus sentences:The famed Yankee Clipper, now retired, has been assisting (as (a batting coach)).One of those capital-gains ventures, in fact, has saddled him (with Gore Court).He said this constituted a (very serious) misuse (of the (Criminal court) processes).',\n",
       "  'authors': ['S. Abney ',\n",
       "   ' S. Flickenger ',\n",
       "   ' C. Gdaniec ',\n",
       "   ' C. Grishman ',\n",
       "   ' P. Harrison ',\n",
       "   ' D. Hindle ',\n",
       "   ' R. Ingria ',\n",
       "   ' F. Jelinek ',\n",
       "   ' J. Klavans ',\n",
       "   ' M. Liberman ',\n",
       "   ' M. Marcus ',\n",
       "   ' S. Roukos ',\n",
       "   ' B. Santorini ',\n",
       "   ' T. Strzalkowski ',\n",
       "   ' E. Black'],\n",
       "  'date': '1991',\n",
       "  'identifier': '2087165009',\n",
       "  'references': ['2092654472',\n",
       "   '2027979924',\n",
       "   '2963532001',\n",
       "   '73274768',\n",
       "   '1986543644',\n",
       "   '2110882317',\n",
       "   '2094061585',\n",
       "   '2153439141'],\n",
       "  'title': 'Procedure for quantitatively comparing the syntactic coverage of English grammars'},\n",
       " {'abstract': 'This is the first in a planned series of 10 volumes that will attempt to \"summarize epidemiological knowledge about all major conditions and most risk factors;...generate assessments of numbers of deaths by cause that are consistent with the total numbers of deaths by age sex and region provided by demographers;...provide methodologies for and assessments of aggregate disease burden that combine--into the Disability-Adjusted Life Year or DALY measure--burden from premature mortality with that from living with disability; and...use historical trends in main determinants to project mortality and disease burden forward to 2020.\" This first volume includes chapters summarizing results from the project as a whole. (EXCERPT)',\n",
       "  'authors': ['Christopher J. L. Murray ', ' Alan D. Lopez'],\n",
       "  'date': '1996',\n",
       "  'identifier': '1583327662',\n",
       "  'references': ['2125065061',\n",
       "   '2127951128',\n",
       "   '2110052313',\n",
       "   '2097950056',\n",
       "   '2108344016',\n",
       "   '2155121555',\n",
       "   '2054931962',\n",
       "   '2106006415',\n",
       "   '2614986146',\n",
       "   '1984615306'],\n",
       "  'title': 'The global burden of disease: a comprehensive assessment of mortality and disability from diseases injuries and risk factors in 1990 and projected to 2020.'},\n",
       " {'abstract': 'This paper reports the results of a Monte Carlo study on the relative effectiveness of two internal indices in estimating the true number of clusters in multivariate data. The two indices are the Davies and Bouldin(1) index and a new modification of the Hubert Γ statistic(2). Data in d dimensions are clustered to create sequences of partitions. Estimates are based on plots of the indices as functions of the number of recovered clusters. Neither index uses a-priori information. The effects of sample size, dimensionality, cluster spread, number of true clusters, and sampling window are examined. Clustered data are generated to assure a given number of distinct clusters. The degree of clustering in the data is verified by a separate Monte Carlo study based on the Jaccard and corrected Rand indices that exhibits the importance of correcting external indices for chance. The modified Hubert index, proposed here for the first time, is shown to perform better than the Davies-Bouldin index under all experimental conditions. Recovery of the true number of clusters gets better as the number of true clusters decreases and as the number of dimensions increases. No effect occurs due to sampling window. The complete link clustering method and a square error clustering method recognize the true number of clusters consistently better than the single link method. This study demonstrates the difficulty inherent in estimating the number of clusters.',\n",
       "  'authors': ['Richard C. Dubes'],\n",
       "  'date': '1987',\n",
       "  'identifier': '2072533619',\n",
       "  'references': ['3017143921',\n",
       "   '1975152892',\n",
       "   '103650626',\n",
       "   '2118587067',\n",
       "   '2051224630',\n",
       "   '2112348586',\n",
       "   '2033403400',\n",
       "   '2018388286',\n",
       "   '2051974258',\n",
       "   '2118570622'],\n",
       "  'title': 'How many clusters are best?—an experiment'},\n",
       " {'abstract': 'This paper compares five methods for pruning decision trees, developed from sets of examples. When used with uncertain rather than deterministic data, decision-tree induction involves three main stages—creating a complete tree able to classify all the training examples, pruning this tree to give statistical reliability, and processing the pruned tree to improve understandability. This paper concerns the second stage—pruning. It presents empirical comparisons of the five methods across several domains. The results show that three methods—critical value, error complexity and reduced error—perform well, while the other two may cause problems. They also show that there is no significant interaction between the creation and pruning methods.',\n",
       "  'authors': ['John Mingers'],\n",
       "  'date': '1989',\n",
       "  'identifier': '1604329830',\n",
       "  'references': ['3085162807',\n",
       "   '2149706766',\n",
       "   '1594031697',\n",
       "   '2128420091',\n",
       "   '1596324102',\n",
       "   '2159047538',\n",
       "   '2777019853',\n",
       "   '1999011285',\n",
       "   '177590838',\n",
       "   '3021257214'],\n",
       "  'title': 'An Empirical Comparison of Pruning Methods for Decision Tree Induction'},\n",
       " {'abstract': 'Computer vision researchers are developing new approaches to object recognition and detection that are based almost directly on images and avoid the use of intermediate three-dimensional models. Many of these techniques depend on a representation of images that induce a linear vector space structure and in principle requires dense feature correspondence. This image representation allows the use of learning techniques for the analysis of images (for computer vision) as well as for the synthesis of images (for computer graphics).',\n",
       "  'authors': ['David Beymer ', ' Tomaso Poggio'],\n",
       "  'date': '1996',\n",
       "  'identifier': '2070320140',\n",
       "  'references': ['2138451337',\n",
       "   '2123977795',\n",
       "   '2095757522',\n",
       "   '2138835141',\n",
       "   '2135463994',\n",
       "   '2142912032',\n",
       "   '94523489',\n",
       "   '1981025032',\n",
       "   '3110825305',\n",
       "   '1531060698'],\n",
       "  'title': 'Image representations for visual learning.'},\n",
       " {'abstract': 'A Study of Thinking is a pioneering account of how human beings achieve a measure of rationality in spite of the constraints imposed by bias, limited attention and memory, and the risks of error imposed by pressures of time and ignorance. First published in 1956 and hailed at its appearance as a groundbreaking study, it is still read three decades later as a major contribution to our understanding of the mind. In their insightful new introduction, the authors relate the book to the cognitive revolution and its handmaiden, artificial intelligence. The central theme of the work is that the scientific study of human thinking must concentrate upon meaning and its achievement rather than upon the behaviorists\\' stimuli and responses and the presumed connections between them. The book\\'s point of departure is how human beings group the world of particulars into ordered classes and categories-concepts-in order to impose a coherent and manageable order upon that world. But rather than relying principally on philosophical speculation to make its point, A Study of Thinking reports dozens of experiments to elucidate the strategies that people use in penetrating to the deep structure of the information they encounter. This seminal study was a major event in the cognitive revolution of the 1950s. Reviewing it at the time, J. Robert Oppenheimer said it \"has in many ways the flavor of conviction which makes it point to the future.\"',\n",
       "  'authors': ['Bruner 1', ' Jacqueline J Goodnow 2', ' George A Austin'],\n",
       "  'date': '1956',\n",
       "  'identifier': '1778685146',\n",
       "  'references': ['2099141989',\n",
       "   '2075585362',\n",
       "   '1526441817',\n",
       "   '2116967411',\n",
       "   '2108860923',\n",
       "   '1515466786',\n",
       "   '2050731102',\n",
       "   '2580252561',\n",
       "   '1709698377',\n",
       "   '2016429292'],\n",
       "  'title': 'A study of thinking'},\n",
       " {'abstract': 'This monograph describes and analyzes some practical methods for finding approximate zeros and minima of functions.',\n",
       "  'authors': ['Richard P Brent'],\n",
       "  'date': '1972',\n",
       "  'identifier': '2078950386',\n",
       "  'references': ['2046941888', '1597888531'],\n",
       "  'title': 'Algorithms for Minimization Without Derivatives'},\n",
       " {'abstract': 'Parameter estimation for multivariate functions of Markov chains, a class of versatile statistical models for vector random processes, is discussed. The model regards an ordered sequence of vectors as noisy multivariate observations of a Markov chain. Mixture distributions are a special case. The foundations of the theory presented here were established by Baum, Petrie, Soules, and Weiss. A powerful representation theorem by Fan is employed to generalize the analysis of Baum, {\\\\em et al.} to a larger class of distributions.',\n",
       "  'authors': ['L. Liporace'],\n",
       "  'date': '1982',\n",
       "  'identifier': '1980800561',\n",
       "  'references': ['2142384583',\n",
       "   '1562979145',\n",
       "   '2045407304',\n",
       "   '1575431606',\n",
       "   '2086699924',\n",
       "   '2077574412',\n",
       "   '1965555277',\n",
       "   '2166851988',\n",
       "   '2142485381',\n",
       "   '2034541289'],\n",
       "  'title': 'Maximum likelihood estimation for multivariate observations of Markov sources'},\n",
       " {'abstract': '',\n",
       "  'authors': ['W. J. Youden ',\n",
       "   ' Oscar Kempthorne ',\n",
       "   ' John W. Tukey ',\n",
       "   ' G. E. P. Box ',\n",
       "   ' J. S. Hunter'],\n",
       "  'date': '1959',\n",
       "  'identifier': '2010374567',\n",
       "  'references': ['2038669746',\n",
       "   '999207820',\n",
       "   '2078495506',\n",
       "   '2789635418',\n",
       "   '2224044578',\n",
       "   '2084721986',\n",
       "   '2085993106',\n",
       "   '2071465274',\n",
       "   '1988522639',\n",
       "   '1973810641'],\n",
       "  'title': 'Discussion of the Papers of Messrs. Satterthwaite and Budne'},\n",
       " {'abstract': 'Particle tracking is of key importance for quantitative analysis of intracellular dynamic processes from time-lapse microscopy image data. Because manually detecting and following large numbers of individual particles is not feasible, automated computational methods have been developed for these tasks by many groups. Aiming to perform an objective comparison of methods, we gathered the community and organized an open competition in which participating teams applied their own methods independently to a commonly defined data set including diverse scenarios. Performance was assessed using commonly defined measures. Although no single method performed best across all scenarios, the results revealed clear differences between the various approaches, leading to notable practical conclusions for users and developers.',\n",
       "  'authors': ['Nicolas Chenouard 1',\n",
       "   ' Ihor Smal 1',\n",
       "   ' Fabrice de Chaumont 2',\n",
       "   ' Martin Maška 3',\n",
       "   ' 4',\n",
       "   ' Ivo F Sbalzarini 1',\n",
       "   ' Yuanhao Gong 1',\n",
       "   ' Janick Cardinale 1',\n",
       "   ' Craig Carthel 1',\n",
       "   ' Stefano Coraluppi 1',\n",
       "   ' Mark Winter 1',\n",
       "   ' Andrew R Cohen 5',\n",
       "   ' William J Godinez 1',\n",
       "   ' Karl Rohr 1',\n",
       "   ' Yannis Kalaidzidis 1',\n",
       "   ' Liang Liang 1',\n",
       "   ' James Duncan 1',\n",
       "   ' Hongying Shen 1',\n",
       "   ' Yingke Xu 6',\n",
       "   ' Klas E G Magnusson 7',\n",
       "   ' Joakim Jaldén 7',\n",
       "   ' Helen M Blau 1',\n",
       "   ' Perrine Paul-Gilloteaux 8',\n",
       "   ' Philippe Roudot 9',\n",
       "   ' Charles Kervrann 9',\n",
       "   ' François Waharte 8',\n",
       "   ' Jean-Yves Tinevez 10',\n",
       "   ' Spencer L Shorte 10',\n",
       "   ' Joost Willemse 11',\n",
       "   ' Katherine Celler 11',\n",
       "   ' Gilles P van Wezel 11',\n",
       "   ' Han-Wei Dan 12',\n",
       "   ' Yuh-Show Tsai 12',\n",
       "   ' Carlos Ortiz de Solórzano 4',\n",
       "   ' Jean-Christophe Olivo-Marin 1',\n",
       "   ' Erik Meijering 1'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2103706175',\n",
       "  'references': ['2151103935',\n",
       "   '2167279371',\n",
       "   '1568122762',\n",
       "   '2037947871',\n",
       "   '2116044718',\n",
       "   '2129249398',\n",
       "   '1974642741',\n",
       "   '2143094150',\n",
       "   '2062552264',\n",
       "   '2105916176'],\n",
       "  'title': 'Objective comparison of particle tracking methods'},\n",
       " {'abstract': \"This monograph is about a class of optimization algorithms called proximal algorithms. Much like Newton's method is a standard tool for solving unconstrained smooth optimization problems of modest size, proximal algorithms can be viewed as an analogous tool for nonsmooth, constrained, large-scale, or distributed versions of these problems. They are very generally applicable, but are especially well-suited to problems of substantial recent interest involving large or high-dimensional datasets. Proximal methods sit at a higher level of abstraction than classical algorithms like Newton's method: the base operation is evaluating the proximal operator of a function, which itself involves solving a small convex optimization problem. These subproblems, which generalize the problem of projecting a point onto a convex set, often admit closed-form solutions or can be solved very quickly with standard or simple specialized methods. Here, we discuss the many different interpretations of proximal operators and algorithms, describe their connections to many other topics in optimization and applied mathematics, survey some popular algorithms, and provide a large number of examples of proximal operators that commonly arise in practice.\",\n",
       "  'authors': ['Neal Parikh ', ' Stephen Boyd'],\n",
       "  'date': '2013',\n",
       "  'identifier': '2913535645',\n",
       "  'references': ['2296319761',\n",
       "   '2164278908',\n",
       "   '3029645440',\n",
       "   '2100556411',\n",
       "   '2135046866',\n",
       "   '2122825543',\n",
       "   '2078204800',\n",
       "   '2103972604',\n",
       "   '2146842127',\n",
       "   '2138019504'],\n",
       "  'title': 'Proximal Algorithms'},\n",
       " {'abstract': 'This paper presents an in-depth computational comparison of the basic solution algorithms for solving transportation problems. The comparison is performed using \"state of the art\" computer codes for the dual simplex transportation method, the out-of-kilter method, and the primal simplex transportation method (often referred to as the Row-Column Sum Method or M O D I method). In addition, these codes are compared against a state of the art large scale LP code, O P H E L I E/LP. The study discloses that the most efficient solution procedure arises by coupling a primal transportation algorithm (embodying recently developed methods for accelerating the determination of basis trees and dual evaluators) with a version of the Row Minimum start rule and a \"modified row first negative evaluator\" rule. The resulting method has been found to be at least 100 times faster than OPHELIE, and 9 times faster than a streamlined version of the SHARE out-of-kilter code. The method\\'s median solution time for solving 1000 \\\\times 1000 transportation problems on a CDC 6600 computer is 17 seconds with a range of 14 to 22 seconds. Some of the unique characteristics of this study are (1) all of the fundamental solution techniques are tested on the same machine and the same problems, (2) a broad spectrum of problem sizes are examined, varying from 10 \\\\times 10 to 1000 \\\\times 1000; (3) a broad profile of nondense problems are examined ranging from 100 percent to 1 percent dense; and (4) additional tests using the best of the codes have been made on three other machines (IBM 360/65, UNIVAC 1108, and CDC 6400), providing surprising insights into conclusions based on comparing times on different machines and compilers.',\n",
       "  'authors': ['Fred Glover ', ' D. Karney ', ' D. Klingman ', ' A. Napier'],\n",
       "  'date': '1974',\n",
       "  'identifier': '2031763823',\n",
       "  'references': ['1559582792',\n",
       "   '2095478116',\n",
       "   '2074145107',\n",
       "   '2053044043',\n",
       "   '2018638861',\n",
       "   '2047903283',\n",
       "   '2119098384',\n",
       "   '1983516812',\n",
       "   '1964540128',\n",
       "   '2164613142'],\n",
       "  'title': 'A Computation Study on Start Procedures, Basis Change Criteria, and Solution Algorithms for Transportation Problems'},\n",
       " {'abstract': \"OBJECTIVES Prediction, Explanation, Elimination or What? How Many Variables in the Prediction Formula? Alternatives to Using Subsets 'Black Box' Use of Best-Subsets Techniques LEAST-SQUARES COMPUTATIONS Using Sums of Squares and Products Matrices Orthogonal Reduction Methods Gauss-Jordan v. Orthogonal Reduction Methods Interpretation of Projections Appendix A: Operation Counts for All-Subsets Regression FINDING SUBSETS WHICH FIT WELL Objectives and Limitations of this Chapter Forward Selection Efroymson's Algorithm Backward Elimination Sequential Replacement Algorithm Replacing Two Variables at a Time Generating All Subsets Using Branch-and-Bound Techniques Grouping Variables Ridge Regression and Other Alternatives The Non-Negative Garrote and the Lasso Some Examples Conclusions and Recommendations HYPOTHESIS TESTING Is There any Information in the Remaining Variables? Is One Subset Better than Another? Appendix A: Spjftvoll's Method - Detailed Description WHEN TO STOP? What Criterion Should We Use? Prediction Criteria Cross-Validation and the PRESS Statistic Bootstrapping Likelihood and Information-Based Stopping Rules Appendix A. Approximate Equivalence of Stopping Rules ESTIMATION OF REGRESSION COEFFICIENTS Selection Bias Choice Between Two Variables Selection Bias in the General Case, and its Reduction Conditional Likelihood Estimation Estimation of Population Means Estimating Least-Squares Projections Appendix A: Changing Projections to Equate Sums of Squares BAYESIAN METHODS Bayesian Introduction 'Spike and Slab' Prior Normal prior for Regression Coefficients Model Averaging Picking the Best Model CONCLUSIONS AND SOME RECOMMENDATIONS REFERENCES INDEX\",\n",
       "  'authors': ['Alan J. Miller'],\n",
       "  'date': '2002',\n",
       "  'identifier': '1969423031',\n",
       "  'references': ['2296616510',\n",
       "   '2127271355',\n",
       "   '2017761965',\n",
       "   '2158940042',\n",
       "   '2109449402',\n",
       "   '2017337590',\n",
       "   '2103519107',\n",
       "   '2126607811'],\n",
       "  'title': 'Subset Selection in Regression'},\n",
       " {'abstract': 'The problem of handwritten digit recognition is dealt with by multilayer feedforward neural networks with different types of neuronal activation functions. Three types of activation functions are adopted in the network, namely, the traditional sigmoid function, sinusoidal function and a periodic function that can be considered as a combination of the first two functions. To speed up the learning, as well as to reduce the network size, an extended Kalman filter algorithm with the pruning method is used to train the network. Simulation results show that periodic activation functions perform better than monotonic ones in solving multi-cluster classification problems such as handwritten digit recognition.',\n",
       "  'authors': ['Kwok-wo Wong 1', ' Chi-sing Leung 1', ' Sheng-jiang Chang 2'],\n",
       "  'date': '2002',\n",
       "  'identifier': '2142075376',\n",
       "  'references': ['2147800946',\n",
       "   '3036751298',\n",
       "   '2145085734',\n",
       "   '2169415433',\n",
       "   '2256679588',\n",
       "   '2062803420',\n",
       "   '1970680382',\n",
       "   '2116640950',\n",
       "   '1998536535',\n",
       "   '1969190118'],\n",
       "  'title': 'Handwritten digit recognition using multilayer feedforward neural networks with periodic and monotonic activation functions'},\n",
       " {'abstract': 'We propose a new class of algorithms for linear cost network flow problems with and without gains. These algorithms are based on iterative improvement of a dual cost and operate in a manner that is reminiscent of coordinate ascent and Gauss-Seidel relaxation methods. We compare our coded implementations of these methods with mature state-of-the-art primal simplex and primal-dual codes, and find them to be several times faster on standard benchmark problems, and faster by an order of magnitude on large, randomly generated problems. Our experiments indicate that the speedup factor increases with problem dimension.',\n",
       "  'authors': ['Dimitri P. Bertsekas ', ' Paul Tseng'],\n",
       "  'date': '1988',\n",
       "  'identifier': '1964540128',\n",
       "  'references': ['2053913299',\n",
       "   '2090359754',\n",
       "   '2090963365',\n",
       "   '1964119857',\n",
       "   '2078667951',\n",
       "   '2009070933',\n",
       "   '1561880747',\n",
       "   '1481747975',\n",
       "   '2109567578',\n",
       "   '2155370486'],\n",
       "  'title': 'Relaxation methods for minimum cost ordinary and generalized network flow problems'},\n",
       " {'abstract': 'Simple sets of parallel operations are described which can be used to detect texture edges, \"spots,\" and \"streaks\" in digitized pictures. It is shown that, by comparing the outputs of the operations corresponding to (e.g.,) edges of different sizes, one can construct a composite output in which edges between differently textured regions are detected, and isolated objects are also detected, but the objects composing the textures are ignored. Relationships between this class of picture processing operations and the Gestalt psychologists\\' laws of pictorial pattern organization are also discussed.',\n",
       "  'authors': ['A. Rosenfeld 1', ' M. Thurston 2'],\n",
       "  'date': '1971',\n",
       "  'identifier': '1968245656',\n",
       "  'references': ['2987654510',\n",
       "   '2087517326',\n",
       "   '2108245145',\n",
       "   '2295345027',\n",
       "   '2755218350',\n",
       "   '1544096716',\n",
       "   '2334615115',\n",
       "   '2017537431'],\n",
       "  'title': 'Edge and Curve Detection for Visual Scene Analysis'},\n",
       " {'abstract': 'This paper surveys locally weighted learning, a form of lazy learning and memory-based learning, and focuses on locally weighted linear regression. The survey discusses distance functions, smoothing parameters, weighting functions, local model structures, regularization of the estimates and bias, assessing predictions, handling noisy data and outliers, improving the quality of predictions by tuning fit parameters, interference between old and new data, implementing locally weighted learning efficiently, and applications of locally weighted learning. A companion paper surveys how locally weighted learning can be used in robot learning and control.',\n",
       "  'authors': ['Christopher G. Atkeson 1',\n",
       "   ' Andrew W. Moore 2',\n",
       "   ' Stefan Schaal 1'],\n",
       "  'date': '1997',\n",
       "  'identifier': '1689445748',\n",
       "  'references': ['2170120409',\n",
       "   '1536929369',\n",
       "   '1770825568',\n",
       "   '2074429597',\n",
       "   '2149723649',\n",
       "   '204885769',\n",
       "   '1969423031',\n",
       "   '2319794630',\n",
       "   '2796837256',\n",
       "   '2069888879'],\n",
       "  'title': 'Locally Weighted Learning'},\n",
       " {'abstract': 'New techniques are described for model-based recognition of the objects in 3-D space. The recognition is performed from single gray-scale images taken from unknown viewpoints. The objects in the scene may be overlapping and partially occluded. An efficient matching algorithm, which assumes affine approximation to the prospective viewing transformation, is proposed. The algorithm has an offline model preprocessing (shape representation) phase which is independent of the scene information and a recognition phase based on efficient indexing. It has a straightforward parallel implementation. The algorithm was successfully tested in recognition of industrial objects appearing in composite occluded scenes. >',\n",
       "  'authors': ['Y. Lamdan 1', ' J.T. Schwartz 1', ' H.J. Wolfson 2'],\n",
       "  'date': '1990',\n",
       "  'identifier': '2159709546',\n",
       "  'references': ['2798909945',\n",
       "   '2911709767',\n",
       "   '1996773532',\n",
       "   '2154204736',\n",
       "   '1532977286',\n",
       "   '2026311529',\n",
       "   '1513966746',\n",
       "   '2068204893',\n",
       "   '2000048778',\n",
       "   '2136113379'],\n",
       "  'title': 'Affine invariant model-based object recognition'},\n",
       " {'abstract': 'The need to make default assumptions is frequently encountered in reasoning about incompletely specified worlds. Inferences sanctioned by default are best viewed as beliefs which may well be modified or rejected by subsequent observations. It is this property which leads to the non-monotonicity of any logic of defaults. In this paper we propose a logic for default reasoning. We then specialize our treatment to a very large class of commonly occuring defaults. For this class we develop a complete proof theory and show how to interface it with a top down resolution theorem prover. Finally, we provide criteria under which the revision of derived beliefs must be effected.',\n",
       "  'authors': ['Raymond Reiter'],\n",
       "  'date': '1987',\n",
       "  'identifier': '2155322595',\n",
       "  'references': ['2138162238',\n",
       "   '1766332311',\n",
       "   '2121773050',\n",
       "   '1996347293',\n",
       "   '2105486835',\n",
       "   '2212036697',\n",
       "   '2100738443',\n",
       "   '2530006810',\n",
       "   '2157368609',\n",
       "   '1541540802'],\n",
       "  'title': 'A logic for default reasoning'},\n",
       " {'abstract': \"We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people play the game they help determine the contents of images by providing meaningful labels for them. If the game is played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don't work well enough, we encourage people to do the work by taking advantage of their desire to be entertained.\",\n",
       "  'authors': ['Luis von Ahn ', ' Laura Dabbish'],\n",
       "  'date': '2004',\n",
       "  'identifier': '2141282920',\n",
       "  'references': ['1666447063',\n",
       "   '1934863104',\n",
       "   '2166770390',\n",
       "   '1587328194',\n",
       "   '2293605478',\n",
       "   '2055225264',\n",
       "   '2050457084',\n",
       "   '181417509',\n",
       "   '2612148268'],\n",
       "  'title': 'Labeling images with a computer game'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Franz Josef Och ', ' Christoph Tillmann ', ' Hermann Ney'],\n",
       "  'date': '1999',\n",
       "  'identifier': '1517947178',\n",
       "  'references': ['2006969979',\n",
       "   '2038698865',\n",
       "   '2107551411',\n",
       "   '2113106066',\n",
       "   '2196555355',\n",
       "   '2294072136',\n",
       "   '2158164089'],\n",
       "  'title': 'Improved Alignment Models for Statistical Machine Translation'},\n",
       " {'abstract': 'Class-tested and coherent, this groundbreaking new textbook teaches web-era information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. Written from a computer science perspective by three leading experts in the field, it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Although originally designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the book will also create a buzz for researchers and professionals alike.',\n",
       "  'authors': ['Christopher D. Manning 1',\n",
       "   ' Prabhakar Raghavan 2',\n",
       "   ' Hinrich Schütze 3'],\n",
       "  'date': '2005',\n",
       "  'identifier': '1532325895',\n",
       "  'references': ['1888005072',\n",
       "   '2962965405',\n",
       "   '1662133657',\n",
       "   '2164019165',\n",
       "   '1521626219',\n",
       "   '2122369144',\n",
       "   '1736726159',\n",
       "   '2248026759',\n",
       "   '2150341604'],\n",
       "  'title': 'Introduction to Information Retrieval'},\n",
       " {'abstract': 'Most research related to the reliability and validity of marketing measures has focused on multi-item quantitative scales. In contrast, little attention has been given to the quality of nominal sca...',\n",
       "  'authors': ['William D. Perreault 1', ' Laurence E. Leigh 2'],\n",
       "  'date': '1989',\n",
       "  'identifier': '2069585723',\n",
       "  'references': ['2164777277',\n",
       "   '2053154970',\n",
       "   '2037789405',\n",
       "   '1975879668',\n",
       "   '1974930980',\n",
       "   '1979773093',\n",
       "   '1977880816',\n",
       "   '2149490731',\n",
       "   '2334548368',\n",
       "   '2028548753'],\n",
       "  'title': 'Reliability of Nominal Data Based on Qualitative Judgments'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Alexander H. Miller 1',\n",
       "   ' Adam Fisch 1',\n",
       "   ' Jesse Dodge 2',\n",
       "   ' Amir-Hossein Karimi 3',\n",
       "   ' Antoine Bordes 1',\n",
       "   ' Jason Weston 1'],\n",
       "  'date': '2016',\n",
       "  'identifier': '2963448850',\n",
       "  'references': ['2964308564',\n",
       "   '2158899491',\n",
       "   '1532325895',\n",
       "   '2123442489',\n",
       "   '102708294',\n",
       "   '1793121960',\n",
       "   '2094728533',\n",
       "   '2252136820',\n",
       "   '2584341106',\n",
       "   '1512387364'],\n",
       "  'title': 'Key-Value Memory Networks for Directly Reading Documents'},\n",
       " {'abstract': \"In this paper we applied Cloude's decomposition to imaging radar polarimetry. We show in detail how the decomposition results can guide the interpretation of scattering from vegetated area. For multi-frequency polarimetric radar measurements of a clearcut area, the decomposition leads us to conclude that the vegetation is probably thin compared to even the C-band radar wavelength of 6 cm. For a forested area, we notice an increased amount of even number of reflection scattering at P-band and L-band, probably the result of penetration through the coniferous canopy resulting in trunk-ground double reflection scattering. The scattering for the forested area is still dominated by scattering from randomly oriented cylinders, however. It is found that these cylinders are thicker than in the case of clearcut areas, leading us to conclude that scattering from the branches probably dominate in this case.\",\n",
       "  'authors': ['Jakob J. Vanzyl'],\n",
       "  'date': '1993',\n",
       "  'identifier': '2044781968',\n",
       "  'references': ['2078985447',\n",
       "   '2133989913',\n",
       "   '2097272115',\n",
       "   '2131035246',\n",
       "   '2154830216',\n",
       "   '2170481611',\n",
       "   '2052190325',\n",
       "   '2007695941',\n",
       "   '2096893798',\n",
       "   '2116319373'],\n",
       "  'title': \"Application of Cloude's target decomposition theorem to polarimetric imaging radar data\"},\n",
       " {'abstract': 'The performance of current speech recognition systems is far below that of humans. Neural nets offer the potential of providing massive parallelism, adaptation, and new algorithmic approaches to problems in speech recognition. Initial studies have demonstrated that multilayer networks with time delays can provide excellent discrimination between small sets of pre-segmented difficult-to-discriminate words, consonants, and vowels. Performance for these small vocabularies has often exceeded that of more conventional approaches. Physiological front ends have provided improved recognition accuracy in noise and a cochlea filter-bank that could be used in these front ends has been implemented using micro-power analog VLSI techniques. Techniques have been developed to scale networks up in size to handle larger vocabularies, to reduce training time, and to train nets with recurrent connections. Multilayer perceptron classifiers are being integrated into conventional continuous-speech recognizers. Neural net archit...',\n",
       "  'authors': ['Richard P. Lippmann'],\n",
       "  'date': '1989',\n",
       "  'identifier': '2169415433',\n",
       "  'references': ['2581275558',\n",
       "   '2154642048',\n",
       "   '2042264548',\n",
       "   '2293063825',\n",
       "   '2110485445',\n",
       "   '1991848143',\n",
       "   '3017143921',\n",
       "   '2173629880',\n",
       "   '2105594594',\n",
       "   '94523489'],\n",
       "  'title': 'Review of neural networks for speech recognition'},\n",
       " {'abstract': 'The term word association is used in a very particular sense in the psycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor. ) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose an objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words.',\n",
       "  'authors': ['Kenneth Ward Church 1', ' Patrick Hanks 2'],\n",
       "  'date': '1990',\n",
       "  'identifier': '1593045043',\n",
       "  'references': ['2099247782',\n",
       "   '2017580301',\n",
       "   '1548013757',\n",
       "   '2155818555',\n",
       "   '1990438144',\n",
       "   '2158652440',\n",
       "   '2082092506',\n",
       "   '2066873261',\n",
       "   '2979401726'],\n",
       "  'title': 'Word association norms, mutual information, and lexicography'},\n",
       " {'abstract': 'This paper concerns relationships among focus of attention, choice of referring expression, and perceived coherence of utterances within a discourse segment. It presents a framework and initial theory of centering intended to model the local component of attentional state. The paper examines interactions between local coherence and choice of referring expressions; it argues that differences in coherence correspond in part to the inference demands made by different types of referring expressions, given a particular attentional state. It demonstrates that the attentional state properties modeled by centering can account for these differences.',\n",
       "  'authors': ['Barbara J. Grosz 1',\n",
       "   ' Scott Weinstein 2',\n",
       "   ' Aravind K. Joshi 2'],\n",
       "  'date': '1995',\n",
       "  'identifier': '2124741472',\n",
       "  'references': ['2167702024',\n",
       "   '2001722583',\n",
       "   '2040025108',\n",
       "   '2139243841',\n",
       "   '1700273416',\n",
       "   '2170809941',\n",
       "   '180090742',\n",
       "   '2146213370',\n",
       "   '1582784770',\n",
       "   '1506254219'],\n",
       "  'title': 'Centering: a framework for modeling the local coherence of discourse'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Pierre Gilles Lemarié-Rieusset ', ' Yves Meyer'],\n",
       "  'date': '1986',\n",
       "  'identifier': '2013987111',\n",
       "  'references': ['2132984323',\n",
       "   '2146842127',\n",
       "   '2098914003',\n",
       "   '1996021349',\n",
       "   '2148593155',\n",
       "   '2069912449',\n",
       "   '1970352604',\n",
       "   '2163398148',\n",
       "   '1980149518',\n",
       "   '2166982406'],\n",
       "  'title': 'Ondelettes et bases hilbertiennes.'},\n",
       " {'abstract': \"Visual tracking, in essence, deals with non-stationary image streams that change over time. While most existing algorithms are able to track objects well in controlled environments, they usually fail in the presence of significant variation of the object's appearance or surrounding illumination. One reason for such failures is that many algorithms employ fixed appearance models of the target. Such models are trained using only appearance data available before tracking begins, which in practice limits the range of appearances that are modeled, and ignores the large volume of information (such as shape changes or specific lighting conditions) that becomes available during tracking. In this paper, we present a tracking method that incrementally learns a low-dimensional subspace representation, efficiently adapting online to changes in the appearance of the target. The model update, based on incremental algorithms for principal component analysis, includes two important features: a method for correctly updating the sample mean, and a forgetting factor to ensure less modeling power is expended fitting older observations. Both of these features contribute measurably to improving overall tracking performance. Numerous experiments demonstrate the effectiveness of the proposed tracking algorithm in indoor and outdoor environments where the target objects undergo large changes in pose, scale, and illumination.\",\n",
       "  'authors': ['David A. Ross 1',\n",
       "   ' Jongwoo Lim 2',\n",
       "   ' Ruei-Sung Lin 3',\n",
       "   ' Ming-Hsuan Yang 2'],\n",
       "  'date': '2008',\n",
       "  'identifier': '2139047213',\n",
       "  'references': ['2148694408',\n",
       "   '2132103241',\n",
       "   '2798909945',\n",
       "   '3111950349',\n",
       "   '2159128898',\n",
       "   '2123977795',\n",
       "   '2125027820',\n",
       "   '1481420047',\n",
       "   '2613779721',\n",
       "   '2118877769'],\n",
       "  'title': 'Incremental Learning for Robust Visual Tracking'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Wolfgang Hahn'],\n",
       "  'date': '1963',\n",
       "  'identifier': '1967618559',\n",
       "  'references': ['2442162137',\n",
       "   '2135176411',\n",
       "   '1507624518',\n",
       "   '1989051419',\n",
       "   '2039669896',\n",
       "   '2155465398',\n",
       "   '2145795099',\n",
       "   '2400094359',\n",
       "   '1489956020',\n",
       "   '1980212624'],\n",
       "  'title': \"Theory and Application of Liapunov's Direct Method\"},\n",
       " {'abstract': 'LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets.',\n",
       "  'authors': ['Rong-En Fan ',\n",
       "   ' Kai-Wei Chang ',\n",
       "   ' Cho-Jui Hsieh ',\n",
       "   ' Xiang-Rui Wang ',\n",
       "   ' Chih-Jen Lin'],\n",
       "  'date': '2008',\n",
       "  'identifier': '2118585731',\n",
       "  'references': ['2153635508',\n",
       "   '2097360283',\n",
       "   '2109943925',\n",
       "   '2087347434',\n",
       "   '2035720976',\n",
       "   '2150102617',\n",
       "   '2165966284',\n",
       "   '2142623206',\n",
       "   '2118286367',\n",
       "   '2039050532'],\n",
       "  'title': 'LIBLINEAR: A Library for Large Linear Classification'},\n",
       " {'abstract': 'We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.',\n",
       "  'authors': ['Yoon Kim'],\n",
       "  'date': '2014',\n",
       "  'identifier': '1832693441',\n",
       "  'references': ['2618530766',\n",
       "   '2153579005',\n",
       "   '2146502635',\n",
       "   '2310919327',\n",
       "   '1904365287',\n",
       "   '2158899491',\n",
       "   '2143612262',\n",
       "   '2251939518',\n",
       "   '2062118960',\n",
       "   '2160660844'],\n",
       "  'title': 'Convolutional Neural Networks for Sentence Classification'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Patrick J. Grother'],\n",
       "  'date': '2004',\n",
       "  'identifier': '138943044',\n",
       "  'references': ['2341283081',\n",
       "   '2155511848',\n",
       "   '2137385871',\n",
       "   '97107628',\n",
       "   '102626146',\n",
       "   '2112530785',\n",
       "   '2104060706',\n",
       "   '1502189209',\n",
       "   '1498645202'],\n",
       "  'title': 'Face Recognition Vendor Test 2002 Supplemental Report | NIST'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Leonard E. Baum ',\n",
       "   ' Ted Petrie ',\n",
       "   ' George Soules ',\n",
       "   ' Norman Weiss'],\n",
       "  'date': '1970',\n",
       "  'identifier': '2086699924',\n",
       "  'references': ['2007321142', '2077574412', '2018751505'],\n",
       "  'title': 'A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains'},\n",
       " {'abstract': 'This paper discusses how local measurements of positions and surface normals may be used to identify and locate overlapping objects. The objects are modeled as polyhedra (or polygons) having up to six degrees of positional freedom relative to the sensors. The approach operates by examining all hypotheses about pairings between sensed data and object surfaces and efficiently discarding inconsistent ones by using local constraints on: distances between faces, angles between face normals, and angles (relative to the surface normals) of vectors between sensed points. The method described here is an extension of a method for recognition and localization of nonoverlapping parts previously described in [18] and [15].',\n",
       "  'authors': ['W. Eric L. Grimson ', ' Tomas Lozano-Perez'],\n",
       "  'date': '1987',\n",
       "  'identifier': '2068204893',\n",
       "  'references': ['2740373864',\n",
       "   '2003370853',\n",
       "   '2131806657',\n",
       "   '1996773532',\n",
       "   '1532977286',\n",
       "   '2000048778',\n",
       "   '2135432705',\n",
       "   '2002478775',\n",
       "   '2069683773',\n",
       "   '121511052'],\n",
       "  'title': 'Localizing Overlapping Parts by Searching the Interpretation Tree'},\n",
       " {'abstract': 'Human action recognition is an important yet challenging task. The recently developed commodity depth sensors open up new possibilities of dealing with this problem but also present some unique challenges. The depth maps captured by the depth cameras are very noisy and the 3D positions of the tracked joints may be completely wrong if serious occlusions occur, which increases the intra-class variations in the actions. In this paper, an actionlet ensemble model is learnt to represent each action and to capture the intra-class variance. In addition, novel features that are suitable for depth data are proposed. They are robust to noise, invariant to translational and temporal misalignments, and capable of characterizing both the human motion and the human-object interactions. The proposed approach is evaluated on two challenging action recognition datasets captured by commodity depth cameras, and another dataset captured by a MoCap system. The experimental evaluations show that the proposed approach achieves superior performance to the state of the art algorithms.',\n",
       "  'authors': ['Jiang Wang 1',\n",
       "   ' Zicheng Liu 2',\n",
       "   ' Ying Wu 1',\n",
       "   ' Junsong Yuan 3'],\n",
       "  'date': '2012',\n",
       "  'identifier': '2143267104',\n",
       "  'references': [],\n",
       "  'title': 'Mining actionlet ensemble for action recognition with depth cameras'},\n",
       " {'abstract': 'The successful integration of data from autonomous and heterogeneous systems calls for the resolution of semantic conflicts that may be present. Such conflicts are often reflected by discrepancies in attribute values of the same data object. In this paper, we describe a recently developed prototype system, DIscovering and REconciling ConflicTs (DIRECT). The system mines data value conversion rules in the process of integrating business data from multiple sources. The system architecture and functional modules are described. The process of discovering conversion rules from sales data of a trading company is presented as an illustrative example.',\n",
       "  'authors': ['Weiguo Fan 1',\n",
       "   ' Hongjun Lu 2',\n",
       "   ' Stuart E. Madnick 3',\n",
       "   ' David Cheung 4'],\n",
       "  'date': '2002',\n",
       "  'identifier': '2114760008',\n",
       "  'references': [],\n",
       "  'title': 'DIRECT: a system for mining data value conversion rules from disparate data sources'},\n",
       " {'abstract': \"The character recognition problem, usually resulting from characters being corrupted by printing deterioration and/or inherent noise of the devices, is considered from the viewpoint of statistical decision theory. The optimization consists of minimizing the expected risk for a weight function which is preassigned to measure the consequences of system decisions As an alternative minimization of the error rate for a given rejection rate is used as the critenon. The optimum recogition is thus obtained. The optimum system consists of a conditional-probability densisities computer; character channels, one for each character; a rejection channel; and a comparison network. Its precise structure and and ultimate performance depend essentially upon the signals and noise structure. Explicit examples for an additive Gaussian noise and a ``cosine'' noise are presented. Finally, an error-free recognition system and a possible criterion to measure the character style and deteriortation are presented.\",\n",
       "  'authors': ['C. K. Chow'],\n",
       "  'date': '1957',\n",
       "  'identifier': '2147947791',\n",
       "  'references': [],\n",
       "  'title': 'An optimum character recognition system using decision functions'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Carl Hewitt ', ' Peter Bishop ', ' Richard Steiger'],\n",
       "  'date': '1973',\n",
       "  'identifier': '2131062488',\n",
       "  'references': [],\n",
       "  'title': 'A universal modular ACTOR formalism for artificial intelligence'},\n",
       " {'abstract': \"The proposal of G. Cottrell et al. (1987) that their image compression network might be used to extract image features for pattern recognition automatically, is tested by training a neural network to compress 64 face images, spanning 11 subjects, and 13 nonface images. Features extracted in this manner (the output of the hidden units) are given as input to a one-layer network trained to distinguish faces from nonfaces and to attach a name and sex to the face images. The network successfully recognizes new images of familiar faces, categorizes novel images as to their `faceness' and, to a great extent, gender, and exhibits continued accuracy over a considerable range of partial or shifted input\",\n",
       "  'authors': ['M.K. Fleming ', ' G.W. Cottrell'],\n",
       "  'date': '1990',\n",
       "  'identifier': '2125999363',\n",
       "  'references': ['1498436455',\n",
       "   '1573503290',\n",
       "   '2006852055',\n",
       "   '2565808444',\n",
       "   '2169718527',\n",
       "   '2045817341'],\n",
       "  'title': 'Categorization of faces using unsupervised feature extraction'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Walter E. Schneider ', ' Richard M. Shiffrin'],\n",
       "  'date': '1977',\n",
       "  'identifier': '1535782774',\n",
       "  'references': [],\n",
       "  'title': 'Controlled and automatic human information processing: I'},\n",
       " {'abstract': 'In this paper a theory of two-dimensional moment invariants for planar geometric figures is presented. A fundamental theorem is established to relate such moment invariants to the well-known algebraic invariants. Complete systems of moment invariants under translation, similitude and orthogonal transformations are derived. Some moment invariants under general two-dimensional linear transformations are also included. Both theoretical formulation and practical models of visual pattern recognition based upon these moment invariants are discussed. A simple simulation program together with its performance are also presented. It is shown that recognition of geometrical patterns and alphabetical characters independently of position, size and orientation can be accomplished. It is also indicated that generalization is possible to include invariance with parallel projection.',\n",
       "  'authors': ['Ming-Kuei Hu'],\n",
       "  'date': '1962',\n",
       "  'identifier': '2159498975',\n",
       "  'references': ['118296737',\n",
       "   '205043379',\n",
       "   '2165514071',\n",
       "   '2073869024',\n",
       "   '2027875806'],\n",
       "  'title': 'Visual pattern recognition by moment invariants'},\n",
       " {'abstract': 'We present the results of a survey that collected information about the demographics of participants on Amazon Mechanical Turk, together with information about their level of activity and motivation for working on Amazon Mechanical Turk. We find that approximately 50% of the workers come from the United States and 40% come from India. Country of origin tends to change the motivating reasons for workers to participate in the marketplace. Significantly more workers from India participate on Mechanical Turk because the online marketplace is a primary source of income, while in the US most workers consider Mechanical Turk a secondary source of income. While money is a primary motivating reason for workers to participate in the marketplace, workers also cite a variety of other motivating reasons, including entertainment and education.',\n",
       "  'authors': ['Panagiotis G. Ipeirotis'],\n",
       "  'date': '2010',\n",
       "  'identifier': '1577841485',\n",
       "  'references': ['2106568252',\n",
       "   '2126302787',\n",
       "   '2103240268',\n",
       "   '2164976104',\n",
       "   '1975450585',\n",
       "   '2054626033',\n",
       "   '2059105030',\n",
       "   '2147603330',\n",
       "   '2124503193',\n",
       "   '2086676398'],\n",
       "  'title': 'Demographics of Mechanical Turk'},\n",
       " {'abstract': 'Hands-free operation of a cursor can be achieved by a few neurons in the motor cortex. The activity of motor cortex (MI) neurons conveys movement intent sufficiently well to be used as a control signal to operate artificial devices1,2,3, but until now this has called for extensive training or has been confined to a limited movement repertoire2,3. Here we show how activity from a few (7–30) MI neurons can be decoded into a signal that a monkey is able to use immediately to move a computer cursor to any new position in its workspace (14° × 14° visual angle). Our results, which are based on recordings made by an electrode array that is suitable for human use4,5, indicate that neurally based control of movement may eventually be feasible in paralysed humans.',\n",
       "  'authors': ['Mijail D. Serruya 1',\n",
       "   ' Nicholas G. Hatsopoulos 1',\n",
       "   ' 2',\n",
       "   ' Liam Paninski 1',\n",
       "   ' 3',\n",
       "   ' Matthew R. Fellows 1',\n",
       "   ' John P. Donoghue 1'],\n",
       "  'date': '2002',\n",
       "  'identifier': '2023295798',\n",
       "  'references': ['1543237449',\n",
       "   '1483156930',\n",
       "   '2043721804',\n",
       "   '2020769260',\n",
       "   '2130060244'],\n",
       "  'title': 'Instant neural control of a movement signal'},\n",
       " {'abstract': 'OceanStore is a utility infrastructure designed to span the globe and provide continuous access to persistent information. Since this infrastructure is comprised of untrusted servers, data is protected through redundancy and cryptographic techniques. To improve performance, data is allowed to be cached anywhere, anytime. Additionally, monitoring of usage patterns allows adaptation to regional outages and denial of service attacks; monitoring also enhances performance through pro-active movement of data. A prototype implementation is currently under development.',\n",
       "  'authors': ['John Kubiatowicz ',\n",
       "   ' David Bindel ',\n",
       "   ' Yan Chen ',\n",
       "   ' Steven Czerwinski ',\n",
       "   ' Patrick Eaton ',\n",
       "   ' Dennis Geels ',\n",
       "   ' Ramakrishan Gummadi ',\n",
       "   ' Sean Rhea ',\n",
       "   ' Hakim Weatherspoon ',\n",
       "   ' Westley Weimer ',\n",
       "   ' Chris Wells ',\n",
       "   ' Ben Zhao'],\n",
       "  'date': '2000',\n",
       "  'identifier': '2104210894',\n",
       "  'references': ['2104532741',\n",
       "   '2126087831',\n",
       "   '2126540423',\n",
       "   '2170496240',\n",
       "   '2000876023',\n",
       "   '2115599946',\n",
       "   '1539259921',\n",
       "   '2138683923',\n",
       "   '2005373714',\n",
       "   '2104112849'],\n",
       "  'title': 'OceanStore: an architecture for global-scale persistent storage'},\n",
       " {'abstract': 'Abstract A dependent variable is some unknown function of independent variables plus an error component. If the magnitude of the error could be estimated with minimal assumptions about the underlying functional dependence, then this could be used to judge goodness-of-fit and as a means of selecting a subset of the independent variables which best determine the dependent variable. We propose a procedure for this purpose which is based on a data-directed partitioning of the space into subregions and a fitting of the function in each subregion. The behavior of the procedure is heuristically discussed and illustrated by some simulation examples.',\n",
       "  'authors': ['L. Breiman ', ' W. S. Meisel'],\n",
       "  'date': '1976',\n",
       "  'identifier': '1994753884',\n",
       "  'references': ['2120062331', '2797195457', '2133501628'],\n",
       "  'title': 'General Estimates of the Intrinsic Variability of Data in Nonlinear Regression Models'},\n",
       " {'abstract': '',\n",
       "  'authors': ['John Wilder Tukey'],\n",
       "  'date': '1977',\n",
       "  'identifier': '2319794630',\n",
       "  'references': ['2011430131',\n",
       "   '2144182447',\n",
       "   '2109972881',\n",
       "   '1501500081',\n",
       "   '2099046646',\n",
       "   '2127875809',\n",
       "   '2102148524',\n",
       "   '2151498684'],\n",
       "  'title': 'Exploratory Data Analysis'},\n",
       " {'abstract': 'Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.',\n",
       "  'authors': ['Yann LeCun 1',\n",
       "   ' 2',\n",
       "   ' Yoshua Bengio 3',\n",
       "   ' Geoffrey Hinton 4',\n",
       "   ' 5'],\n",
       "  'date': '2015',\n",
       "  'identifier': '2919115771',\n",
       "  'references': ['2145339207',\n",
       "   '2136922672',\n",
       "   '2100495367',\n",
       "   '2310919327',\n",
       "   '2064675550',\n",
       "   '2163922914',\n",
       "   '2160815625',\n",
       "   '2022508996',\n",
       "   '1993882792',\n",
       "   '2108069432'],\n",
       "  'title': 'Deep learning'},\n",
       " {'abstract': \"Wideband analog signals push contemporary analog-to-digital conversion (ADC) systems to their performance limits. In many applications, however, sampling at the Nyquist rate is inefficient because the signals of interest contain only a small number of significant frequencies relative to the band limit, although the locations of the frequencies may not be known a priori. For this type of sparse signal, other sampling strategies are possible. This paper describes a new type of data acquisition system, called a random demodulator, that is constructed from robust, readily available components. Let K denote the total number of frequencies in the signal, and let W denote its band limit in hertz. Simulations suggest that the random demodulator requires just O(K log(W/K)) samples per second to stably reconstruct the signal. This sampling rate is exponentially lower than the Nyquist rate of W hertz. In contrast to Nyquist sampling, one must use nonlinear methods, such as convex programming, to recover the signal from the samples taken by the random demodulator. This paper provides a detailed theoretical analysis of the system's performance that supports the empirical observations.\",\n",
       "  'authors': ['J.A. Tropp 1',\n",
       "   ' J.N. Laska 2',\n",
       "   ' M.F. Duarte 2',\n",
       "   ' J.K. Romberg 3',\n",
       "   ' R.G. Baraniuk 2'],\n",
       "  'date': '2010',\n",
       "  'identifier': '2141116650',\n",
       "  'references': ['2296616510',\n",
       "   '2145096794',\n",
       "   '2115755118',\n",
       "   '2129638195',\n",
       "   '2127271355',\n",
       "   '2164452299',\n",
       "   '2078204800',\n",
       "   '2017761965',\n",
       "   '2170524206',\n",
       "   '1766888123'],\n",
       "  'title': 'Beyond Nyquist: Efficient Sampling of Sparse Bandlimited Signals'},\n",
       " {'abstract': \"Jaynes's principle of maximum entropy and Kullbacks principle of minimum cross-entropy (minimum directed divergence) are shown to be uniquely correct methods for inductive inference when new information is given in the form of expected values. Previous justifications use intuitive arguments and rely on the properties of entropy and cross-entropy as information measures. The approach here assumes that reasonable methods of inductive inference should lead to consistent results when there are different ways of taking the same information into account (for example, in different coordinate system). This requirement is formalized as four consistency axioms. These are stated in terms of an abstract information operator and make no reference to information measures. It is proved that the principle of maximum entropy is correct in the following sense: maximizing any function but entropy will lead to inconsistency unless that function and entropy have identical maxima. In other words given information in the form of constraints on expected values, there is only one (distribution satisfying the constraints that can be chosen by a procedure that satisfies the consistency axioms; this unique distribution can be obtained by maximizing entropy. This result is established both directly and as a special case (uniform priors) of an analogous result for the principle of minimum cross-entropy. Results are obtained both for continuous probability densities and for discrete distributions.\",\n",
       "  'authors': ['J. Shore ', ' R. Johnson'],\n",
       "  'date': '1980',\n",
       "  'identifier': '2052958516',\n",
       "  'references': ['2025130480',\n",
       "   '1995875735',\n",
       "   '2123838014',\n",
       "   '2132930716',\n",
       "   '2160709761',\n",
       "   '2032558547',\n",
       "   '2294604761',\n",
       "   '3043143654',\n",
       "   '630957616',\n",
       "   '2089611415'],\n",
       "  'title': 'Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy'},\n",
       " {'abstract': \"Some people's faces are easier to recognize than others, but it is not obvious what subject-specific factors make individual faces easy or difficult to recognize. This study considers 11 factors that might make recognition easy or difficult for 1,072 human subjects in the FERET dataset. The specific factors are: race (white, Asian, African-American, or other), gender, age (young or old), glasses (present or absent), facial hair (present or absent), bangs (present or absent), mouth (closed or other), eyes (open or other), complexion (clear or other), makeup (present or absent), and expression (neutral or other). An ANOVA is used to determine the relationship between these subject covariates and the distance between pairs of images of the same subject in a standard Eigenfaces subspace. Some results are not terribly surprising. For example, the distance between pairs of images of the same subject increases for people who change their appearance, e.g., open and close their eyes, open and close their mouth or change expression. Thus changing appearance makes recognition harder. Other findings are surprising. Distance between pairs of images for subjects decreases for people who consistently wear glasses, so wearing glasses makes subjects more recognizable. Pairwise distance also decreases for people who are either Asian or African-American rather than white. A possible shortcoming of our analysis is that minority classifications such as African-Americans and wearers-of-glasses are underrepresented in training. Followup experiments with balanced training addresses this concern and corroborates the original findings. Another possible shortcoming of this analysis is the novel use of pairwise distance between images of a single person as the predictor of recognition difficulty. A separate experiment confirms that larger distances between pairs of subject images implies a larger recognition rank for that same pair of images, thus confirming that the subject is harder to recognize.\",\n",
       "  'authors': ['Geof Givens ',\n",
       "   ' J Ross Beveridge ',\n",
       "   ' Bruce A. Draper ',\n",
       "   ' David Bolme'],\n",
       "  'date': '2003',\n",
       "  'identifier': '2112530785',\n",
       "  'references': ['1989702938',\n",
       "   '2033419168',\n",
       "   '3015463134',\n",
       "   '1528905581',\n",
       "   '2098693229',\n",
       "   '2115689562',\n",
       "   '1598106187',\n",
       "   '2014102379',\n",
       "   '2483194642',\n",
       "   '2139085189'],\n",
       "  'title': 'A Statistical Assessment of Subject Factors in the PCA Recognition of Human Faces'},\n",
       " {'abstract': 'Abstract Test collections have traditionally been used by information retrieval researchers to improve their retrieval strategies. To be viable as a laboratory tool, a collection must reliably rank different retrieval variants according to their true effectiveness. In particular, the relative effectiveness of two retrieval strategies should be insensitive to modest changes in the relevant document set since individual relevance assessments are known to vary widely. The test collections developed in the TREC workshops have become the collections of choice in the retrieval research community. To verify their reliability, NIST investigated the effect changes in the relevance assessments have on the evaluation of retrieval results. Very high correlations were found among the rankings of systems produced using different relevance judgment sets. The high correlations indicate that the comparative evaluation of retrieval performance is stable despite substantial differences in relevance judgments, and thus reaffirm the use of the TREC collections as laboratory tools.',\n",
       "  'authors': ['Ellen M. Voorhees'],\n",
       "  'date': '2000',\n",
       "  'identifier': '2120308175',\n",
       "  'references': ['2106365165',\n",
       "   '1550436572',\n",
       "   '2151664495',\n",
       "   '1557757161',\n",
       "   '2069820131',\n",
       "   '2056319610',\n",
       "   '1988344511',\n",
       "   '36405575',\n",
       "   '2916674408',\n",
       "   '2077324407'],\n",
       "  'title': 'Variations in relevance judgments and the measurement of retrieval effectiveness'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Tzay Y. Young ', ' Thomas W. Calvert'],\n",
       "  'date': '1974',\n",
       "  'identifier': '1536628706',\n",
       "  'references': ['1554544485',\n",
       "   '2003454866',\n",
       "   '2126631147',\n",
       "   '2066789935',\n",
       "   '2095301394',\n",
       "   '2045010718',\n",
       "   '2123060977',\n",
       "   '2107415846',\n",
       "   '1518603657',\n",
       "   '2130995959'],\n",
       "  'title': 'Classification, estimation, and pattern recognition'},\n",
       " {'abstract': 'This paper focuses on the problem of identifying influential users of micro-blogging services. Twitter, one of the most notable micro-blogging services, employs a social-networking model called \"following\", in which each user can choose who she wants to \"follow\" to receive tweets from without requiring the latter to give permission first. In a dataset prepared for this study, it is observed that (1) 72.4% of the users in Twitter follow more than 80% of their followers, and (2) 80.5% of the users have 80% of users they are following follow them back. Our study reveals that the presence of \"reciprocity\" can be explained by phenomenon of homophily. Based on this finding, TwitterRank, an extension of PageRank algorithm, is proposed to measure the influence of users in Twitter. TwitterRank measures the influence taking both the topical similarity between users and the link structure into account. Experimental results show that TwitterRank outperforms the one Twitter currently uses and other related algorithms, including the original PageRank and Topic-sensitive PageRank.',\n",
       "  'authors': ['Jianshu Weng 1', ' Ee-Peng Lim 1', ' Jing Jiang 1', ' Qi He 2'],\n",
       "  'date': '2010',\n",
       "  'identifier': '2076219102',\n",
       "  'references': ['1880262756',\n",
       "   '3013264884',\n",
       "   '2138621811',\n",
       "   '2402962589',\n",
       "   '2046804949',\n",
       "   '2001082470',\n",
       "   '2130354913',\n",
       "   '2170344111',\n",
       "   '2334889010',\n",
       "   '1654194294'],\n",
       "  'title': 'TwitterRank: finding topic-sensitive influential twitterers'},\n",
       " {'abstract': 'This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy.',\n",
       "  'authors': ['Michael John Collins'],\n",
       "  'date': '1996',\n",
       "  'identifier': '2110882317',\n",
       "  'references': ['1632114991',\n",
       "   '1773803948',\n",
       "   '2099247782',\n",
       "   '1623072288',\n",
       "   '2153439141',\n",
       "   '2441154163',\n",
       "   '1859173823',\n",
       "   '2439178139',\n",
       "   '2087165009',\n",
       "   '2069912724'],\n",
       "  'title': 'A New Statistical Parser Based on Bigram Lexical Dependencies'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Richard Snodgrass'],\n",
       "  'date': '1989',\n",
       "  'identifier': '113037826',\n",
       "  'references': ['2002110561',\n",
       "   '2054609588',\n",
       "   '2079814255',\n",
       "   '2132381663',\n",
       "   '2132627975',\n",
       "   '2048715330',\n",
       "   '2271345878',\n",
       "   '1554228447',\n",
       "   '1720324328',\n",
       "   '2126413127'],\n",
       "  'title': 'The temporal query language TQuel'},\n",
       " {'abstract': 'Much previous research has established that perceived ease of use is an important factor influencing user acceptance and usage behavior of information technologies. However, very little research has been conducted to understand how that perception forms and changes over time. The current work presents and tests an anchoring and adjustment-based theoretical model of the determinants of system-specific perceived ease of use. The model proposes control (internal and external--conceptualized as computer self-efficacy and facilitating conditions, respectively), intrinsic motivation (conceptualized as computer playfulness), and emotion (conceptualized as computer anxiety) as anchors that determine early perceptions about the ease of use of a new system. With increasing experience, it is expected that system-specific perceived ease of use, while still anchored to the general beliefs regarding computers and computer use, will adjust to reflect objective usability, perceptions of external control specific to the new system environment, and system-specific perceived enjoyment. The proposed model was tested in three different organizations among 246 employees using three measurements taken over a three-month period. The proposed model was strongly supported at all points of measurement, and explained up to 60% of the variance in system-specific perceived ease of use, which is twice as much as our current understanding. Important theoretical and practical implications of these findings are discussed.',\n",
       "  'authors': ['Viswanath Venkatesh'],\n",
       "  'date': '2000',\n",
       "  'identifier': '2166982098',\n",
       "  'references': ['2099697766',\n",
       "   '1791587663',\n",
       "   '2168569455',\n",
       "   '2888190061',\n",
       "   '2033943395',\n",
       "   '1987198869',\n",
       "   '2098685541',\n",
       "   '1491644571',\n",
       "   '1517229207',\n",
       "   '2036389121'],\n",
       "  'title': 'Determinants of Perceived Ease of Use: Integrating Control, Intrinsic Motivation, and Emotion into the Technology Acceptance Model'},\n",
       " {'abstract': 'The study of the web as a graph is not only fascinating in its own right, but also yields valuable insight into web algorithms for crawling, searching and community discovery, and the sociological phenomena which characterize its evolution. We report on experiments on local and global properties of the web graph using two Altavista crawls each with over 200 million pages and 1.5 billion links. Our study indicates that the macroscopic structure of the web is considerably more intricate than suggested by earlier experiments on a smaller scale.',\n",
       "  'authors': ['Andrei Broder 1',\n",
       "   ' Ravi Kumar 2',\n",
       "   ' Farzin Maghoul 1',\n",
       "   ' Prabhakar Raghavan 2',\n",
       "   ' Sridhar Rajagopalan 2',\n",
       "   ' Raymie Stata 3',\n",
       "   ' Andrew Tomkins 2',\n",
       "   ' Janet Wiener 3'],\n",
       "  'date': '2000',\n",
       "  'identifier': '2175110005',\n",
       "  'references': ['2008620264',\n",
       "   '3013264884',\n",
       "   '2138621811',\n",
       "   '1976969221',\n",
       "   '2769133055',\n",
       "   '2006119904',\n",
       "   '2089192108',\n",
       "   '2079672501',\n",
       "   '2097147952',\n",
       "   '2037498077'],\n",
       "  'title': 'Graph structure in the Web'},\n",
       " {'abstract': 'This paper develops performance criteria for evaluating transform data coding schemes under computational constraints. Computational constraints that conform with the proposed basis-restricted model give rise to suboptimal coding efficiency characterized by a rate-distortion relation R(D) similar in form to the theoretical rate-distortion function. Numerical examples of this performance measure are presented for Fourier, Walsh, Haar, and Karhunen-Loeve transforms.',\n",
       "  'authors': ['J. Pearl ', ' H. Andrews ', ' W. Pratt'],\n",
       "  'date': '1972',\n",
       "  'identifier': '2069800184',\n",
       "  'references': ['2142901448',\n",
       "   '2061171222',\n",
       "   '2109808436',\n",
       "   '2160050411',\n",
       "   '2069267080',\n",
       "   '2026517766',\n",
       "   '2110380152',\n",
       "   '2145341540',\n",
       "   '2114715837',\n",
       "   '2127787769'],\n",
       "  'title': 'Performance Measures for Transform Data Coding'},\n",
       " {'abstract': 'In this paper, a general scheme for complete model-based decomposition of the polarimetric synthetic aperture radar (POLSAR) coherency matrix data is presented. We show that the POLSAR coherency matrix can be completely decomposed into three components contributed by volume scattering and two single scatterers (characterized by rank-1 matrices). Under this scheme, solving for the volume scattering power amounts to a generalized eigendecomposition problem, and the nonnegative power constraint uniquely determines the minimum eigenvalue as the volume scattering power. Furthermore, in order to discriminate the remaining components, we propose two approaches. One is based on eigendecomposition, and the other is based on model fitting, both of which are shown to properly resolve the surface and double-bounce scattering ambiguity. As a result, this paper in particular contributes to two pending needs for model-based POLSAR decomposition. First, it overcomes negative power problems, i.e., all the decomposed powers are strictly guaranteed to be nonnegative; and second, the three-component decomposition exactly accounts for every element of the observed coherency matrix, leading to a complete utilization of the fully polarimetric information.',\n",
       "  'authors': ['Yi Cui 1',\n",
       "   ' Yoshio Yamaguchi 1',\n",
       "   ' Jian Yang 2',\n",
       "   ' Hirokazu Kobayashi 1',\n",
       "   ' Sang-Eun Park 1',\n",
       "   ' Gulab Singh 1'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2096893798',\n",
       "  'references': ['2078985447',\n",
       "   '2097272115',\n",
       "   '2150059023',\n",
       "   '2141424348',\n",
       "   '2130762895',\n",
       "   '2131035246',\n",
       "   '1974024703',\n",
       "   '2139824394',\n",
       "   '2170481611',\n",
       "   '3022080488'],\n",
       "  'title': 'On Complete Model-Based Decomposition of Polarimetric SAR Coherency Matrix Data'},\n",
       " {'abstract': 'A guide to using S environments to perform statistical analyses providing both an introduction to the use of S and a course in modern statistical methods. The emphasis is on presenting practical problems and full analyses of real data sets.',\n",
       "  'authors': ['W. N. Venables ', ' B. D. Ripley'],\n",
       "  'date': '2010',\n",
       "  'identifier': '1513618424',\n",
       "  'references': ['2115709314',\n",
       "   '2115098571',\n",
       "   '2121211805',\n",
       "   '2006617902',\n",
       "   '2244729984',\n",
       "   '2116206245',\n",
       "   '2155988679',\n",
       "   '2134629862',\n",
       "   '2106525823',\n",
       "   '2119160928'],\n",
       "  'title': 'Modern Applied Statistics with S'},\n",
       " {'abstract': 'This paper presents a particle swarm optimization (PSO) for reactive power and voltage control (RPVC) in electric power systems. RPVC can be formulated as a mixed-integer nonlinear optimization problem (MINLP). The proposed method expands the original PSO to handle a MINLP and determines an RPVC strategy with continuous and discrete control variables such as automatic voltage regulator (AVR) operating values of generators, tap positions of on-load tap changer (OLTC) of transformers, and the amount of reactive power compensation equipment (RPCE). The feasibility of the proposed method is demonstrated and compared with reactive tabu search (RTS) and the enumeration method on practical power system models with promising results.',\n",
       "  'authors': ['Y. Fukuyama 1', ' H. Yoshida 2'],\n",
       "  'date': '2001',\n",
       "  'identifier': '2165713450',\n",
       "  'references': ['2152195021',\n",
       "   '2165299997',\n",
       "   '1875348914',\n",
       "   '2105102593',\n",
       "   '2141249818',\n",
       "   '1959606346',\n",
       "   '1989353689',\n",
       "   '2102954583',\n",
       "   '2157534820',\n",
       "   '2130071675'],\n",
       "  'title': 'A particle swarm optimization for reactive power and voltage control in electric power systems'},\n",
       " {'abstract': 'The spatiotemporal patterns of Rolandic mu and beta rhythms were studied during motor imagery with a dense array of EEG electrodes. The subjects were instructed to imagine movements of either the right or the left hand, corresponding to visual stimuli on a computer screen. It was found that unilateral motor imagery results in a short-lasting and localized EEG change over the primary sensorimotor area. The Rolandic rhythms displayed an event-related desynchronization (ERD) only over the contralateral hemisphere. In two of the three investigated subjects, an enhanced Rolandic rhythm was found over the ipsilateral side. The pattern of EEG desynchronization related to imagination of a movement was similar to the pattern during planing of a voluntary movement.',\n",
       "  'authors': ['Gert Pfurtscheller 1', ' Christa Neuper 2'],\n",
       "  'date': '1997',\n",
       "  'identifier': '2006082701',\n",
       "  'references': [],\n",
       "  'title': 'Motor imagery activates primary sensorimotor area in humans'},\n",
       " {'abstract': \"On considere un algorithme servant a optimiser le rapport signal/fouillis echo en faisant l'hypothese que le fouillis echo est independant de la cible. On montre que l'optimisation de ce rapport peut etre developpee via la connaissance partielle des matrices de coherence moyennes des cibles et du fouillis echo en utilisant comme parametre la puissance retrodiffusee totale\",\n",
       "  'authors': ['S.R. Cloude'],\n",
       "  'date': '1988',\n",
       "  'identifier': '2166202664',\n",
       "  'references': ['2519627623', '2054790031'],\n",
       "  'title': 'Optimisation of signal/clutter ratio using polarisation diversity'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Bojan Cestnik ', ' Igor Kononenko ', ' Ivan Bratko'],\n",
       "  'date': '1987',\n",
       "  'identifier': '177590838',\n",
       "  'references': ['1873332500',\n",
       "   '2147169507',\n",
       "   '1500895378',\n",
       "   '2136000097',\n",
       "   '2132166479',\n",
       "   '1999138184',\n",
       "   '2032026767',\n",
       "   '1604329830',\n",
       "   '2083780116'],\n",
       "  'title': 'ASSISTANT 86: a knowledge-elicitation tool for sophisticated users'},\n",
       " {'abstract': 'We introduce a new general framework for the recognition of complex visual scenes, which is motivated by biology: We describe a hierarchical system that closely follows the organization of visual cortex and builds an increasingly complex and invariant feature representation by alternating between a template matching and a maximum pooling operation. We demonstrate the strength of the approach on a range of recognition tasks: From invariant single object recognition in clutter to multiclass categorization problems and complex scene understanding tasks that rely on the recognition of both shape-based as well as texture-based objects. Given the biological constraints that the system had to satisfy, the approach performs surprisingly well: It has the capability of learning from only a few training examples and competes with state-of-the-art systems. We also discuss the existence of a universal, redundant dictionary of features that could handle the recognition of most object categories. In addition to its relevance for computer vision, the success of this approach suggests a plausibility proof for a class of feedforward models of object recognition in cortex',\n",
       "  'authors': ['T. Serre 1',\n",
       "   ' L. Wolf 1',\n",
       "   ' S. Bileschi 2',\n",
       "   ' M. Riesenhuber 3',\n",
       "   ' T. Poggio 4'],\n",
       "  'date': '2007',\n",
       "  'identifier': '2144982973',\n",
       "  'references': [],\n",
       "  'title': 'Robust Object Recognition with Cortex-Like Mechanisms'},\n",
       " {'abstract': \"In this paper we investigate the use of the area under the receiver operating characteristic (ROC) curve (AUC) as a performance measure for machine learning algorithms. As a case study we evaluate six machine learning algorithms (C4.5, Multiscale Classifier, Perceptron, Multi-layer Perceptron, k-Nearest Neighbours, and a Quadratic Discriminant Function) on six ''real world'' medical diagnostics data sets. We compare and discuss the use of AUC to the more conventional overall accuracy and find that AUC exhibits a number of desirable properties when compared to overall accuracy: increased sensitivity in Analysis of Variance (ANOVA) tests; a standard error that decreased as both AUC and the number of test samples increased; decision threshold independent; and it is invariant to a priori class probabilities. The paper concludes with the recommendation that AUC be used in preference to overall accuracy for ''single number'' evaluation of machine learning algorithms.\",\n",
       "  'authors': ['Andrew P. Bradley'],\n",
       "  'date': '1997',\n",
       "  'identifier': '2155653793',\n",
       "  'references': ['2125055259',\n",
       "   '2154642048',\n",
       "   '3085162807',\n",
       "   '1594031697',\n",
       "   '2157825442',\n",
       "   '1770825568',\n",
       "   '2117897510',\n",
       "   '2102150307',\n",
       "   '2135346934',\n",
       "   '1587362683'],\n",
       "  'title': 'The use of the area under the ROC curve in the evaluation of machine learning algorithms'},\n",
       " {'abstract': '',\n",
       "  'authors': ['David Donoho ',\n",
       "   ' Iain Johnstone ',\n",
       "   ' Peter Rousseeuw ',\n",
       "   ' Werner Stahel'],\n",
       "  'date': '1985',\n",
       "  'identifier': '2084439265',\n",
       "  'references': ['2128659236',\n",
       "   '2163490846',\n",
       "   '2144405862',\n",
       "   '2151745355',\n",
       "   '1526121309',\n",
       "   '1521406110',\n",
       "   '2120354217',\n",
       "   '2103474619',\n",
       "   '1918110294',\n",
       "   '2057433494'],\n",
       "  'title': 'Discussion: Projection Pursuit'},\n",
       " {'abstract': \"User studies are important for many aspects of the design process and involve techniques ranging from informal surveys to rigorous laboratory studies. However, the costs involved in engaging users often requires practitioners to trade off between sample size, time requirements, and monetary costs. Micro-task markets, such as Amazon's Mechanical Turk, offer a potential paradigm for engaging a large number of users for low time and monetary costs. Here we investigate the utility of a micro-task market for collecting user measurements, and discuss design considerations for developing remote micro user evaluation tasks. Although micro-task markets have great potential for rapidly collecting user measurements at low costs, we found that special care is needed in formulating tasks in order to harness the capabilities of the approach.\",\n",
       "  'authors': ['Aniket Kittur ', ' Ed H. Chi ', ' Bongwon Suh'],\n",
       "  'date': '2008',\n",
       "  'identifier': '2151401338',\n",
       "  'references': ['1896125358',\n",
       "   '2093753056',\n",
       "   '1903617842',\n",
       "   '2055694118',\n",
       "   '2049671505',\n",
       "   '2028126161'],\n",
       "  'title': 'Crowdsourcing user studies with Mechanical Turk'},\n",
       " {'abstract': \"Algorithms are presented for converting between different three-dimensional object representations: from a collection of cross section outlines to surface points, and from surface points to a collection of overlapping spheres. The algorithms effect a conversion from surface representations (outlines or surface points) to a volume representation (spheres). The spherical representation can be useful for graphical display, and perhaps as an intermediate representation for conversions to representations with other primitives. The spherical decomposition also permits the computation of points on the symmetric surface of an object, the three-dimensional analog of Blum's symmetric axis. The algorithms work in real coordinates rather than in a discrete space, and so avoid error introduced by the quantization of the space.\",\n",
       "  'authors': [\"Joseph O'Rourke \", ' Norman Badler'],\n",
       "  'date': '1979',\n",
       "  'identifier': '2000134108',\n",
       "  'references': ['2009723655',\n",
       "   '2077246452',\n",
       "   '96629785',\n",
       "   '2029982187',\n",
       "   '2083461522',\n",
       "   '2083043519',\n",
       "   '90153133'],\n",
       "  'title': 'Decomposition of Three-Dimensional Objects into Spheres'},\n",
       " {'abstract': 'In the interest of brevity we assume that the reader is familiar with the notion of a relational data base. In particular, we assume a familiarity with the work of Codd or Boyce and Chamberlin. The examples in this paper will be drawn from a data base which describes a department store and consists of three relations: EMP(NAME, SAL, MGR, DEPT) SALES(DEPT, ITEM, VOL) LOC(DEPT, FLOOR)',\n",
       "  'authors': ['D. D. Chamberlin ', ' J. N. Gray ', ' I. L. Traiger'],\n",
       "  'date': '1975',\n",
       "  'identifier': '2119285119',\n",
       "  'references': ['2988119170',\n",
       "   '2012056301',\n",
       "   '2232207765',\n",
       "   '2047499522',\n",
       "   '1505165377',\n",
       "   '2085772207',\n",
       "   '2043571746'],\n",
       "  'title': 'Views, authorization, and locking in a relational data base system'},\n",
       " {'abstract': 'An object recognition system based on the dynamic link architecture, an extension to classical artificial neural networks (ANNs), is presented. The dynamic link architecture exploits correlations in the fine-scale temporal structure of cellular signals to group neurons dynamically into higher-order entities. These entities represent a rich structure and can code for high-level objects. To demonstrate the capabilities of the dynamic link architecture, a program was implemented that can recognize human faces and other objects from video images. Memorized objects are represented by sparse graphs, whose vertices are labeled by a multiresolution description in terms of a local power spectrum, and whose edges are labeled by geometrical distance vectors. Object recognition can be formulated as elastic graph matching, which is performed here by stochastic optimization of a matching cost function. The implementation on a transputer network achieved recognition of human faces and office objects from gray-level camera images. The performance of the program is evaluated by a statistical analysis of recognition results from a portrait gallery comprising images of 87 persons. >',\n",
       "  'authors': ['M. Lades 1',\n",
       "   ' J.C. Vorbruggen 1',\n",
       "   ' J. Buhmann 2',\n",
       "   ' J. Lange 1',\n",
       "   ' C. von der Malsburg 1',\n",
       "   ' R.P. Wurtz ',\n",
       "   ' W. Konen 1'],\n",
       "  'date': '1993',\n",
       "  'identifier': '2095757522',\n",
       "  'references': ['2011039300',\n",
       "   '2176300081',\n",
       "   '1991848143',\n",
       "   '2135463994',\n",
       "   '2130259898',\n",
       "   '2079948225',\n",
       "   '2167034998',\n",
       "   '23758216',\n",
       "   '2051719061',\n",
       "   '1914401667'],\n",
       "  'title': 'Distortion invariant object recognition in the dynamic link architecture'},\n",
       " {'abstract': 'This book contains tutorial overviews and research papers on contemporary trends in the area of machine learning viewed from an AI perspective. Research directions covered include: learning from examples, modeling human learning strategies, knowledge acquisition for expert systems, learning heuristics, discovery systems, and conceptual data analysis.',\n",
       "  'authors': ['R. S. Michalski ', ' J. G. Carbonell ', ' T. M. Mitchell'],\n",
       "  'date': '2013',\n",
       "  'identifier': '1596324102',\n",
       "  'references': ['2149706766',\n",
       "   '1501500081',\n",
       "   '2126385963',\n",
       "   '2137130182',\n",
       "   '2008906462',\n",
       "   '2100677568',\n",
       "   '2040884411',\n",
       "   '2147169507',\n",
       "   '2019363670',\n",
       "   '2133462743'],\n",
       "  'title': 'Machine Learning: An Artificial Intelligence Approach'},\n",
       " {'abstract': 'Abstract An orthogonal basis of L2 which is also an unconditional basis of a functional space F is an optimal basis for compressing, estimating, and recovering functions in F . Simple thresholding operations, applied in the unconditional basis, work essentially better for compressing, estimating, and recovering than they do in any other orthogonal basis. In fact, simple thresholding in an unconditional basis works essentially better for recovery and estimation than other methods, period. (Performance is measured in an asymptotic minimax sense.) As an application, we formalize and prove Mallat′s Heuristic, which says that wavelet bases are optimal for representing functions containing singularities, when there may be an arbitrary number of singularities, arbitrarily distributed.',\n",
       "  'authors': ['David L. Donoho'],\n",
       "  'date': '1993',\n",
       "  'identifier': '2050880896',\n",
       "  'references': ['2296616510',\n",
       "   '2146842127',\n",
       "   '2101491865',\n",
       "   '2018332268',\n",
       "   '2069912449',\n",
       "   '2110505738',\n",
       "   '2020919250',\n",
       "   '59771946',\n",
       "   '2109504624',\n",
       "   '67772112'],\n",
       "  'title': 'Unconditional Bases Are Optimal Bases for Data Compression and for Statistical Estimation'},\n",
       " {'abstract': 'A nonparametric algorithm is presented for the hierarchical partitioning of the feature space. The algorithm is based on the concept of average mutual information, and is suitable for multifeature multicategory pattern recognition problems. The algorithm generates an efficient partitioning tree for specified probability of error by maximizing the amount of average mutual information gain at each partitioning step. A confidence bound expression is presented for the resulting classifier. Three examples, including one of handprinted numeral recognition, are presented to demonstrate the effectiveness of the algorithm.',\n",
       "  'authors': ['I. K. Sethi ', ' G. P. R. Sarvarayudu'],\n",
       "  'date': '1982',\n",
       "  'identifier': '1976123439',\n",
       "  'references': ['2017900294',\n",
       "   '193579291',\n",
       "   '2055203001',\n",
       "   '2143249365',\n",
       "   '1980043035',\n",
       "   '2001619934'],\n",
       "  'title': 'Hierarchical Classifier Design Using Mutual Information'},\n",
       " {'abstract': 'In medical science, studies are often designed to investigate changes in a specific parameter which is measured repeatedly over time in the participating subjects. This allows one to model the process of change within individuals. Although this process occurs in every individual, the inter subject variability can be high. For example, using data of 955 men, Brant et al showed that the average rates of increase of systolic blood pressure (SBP) are smallest in the younger age groups, and greatest in the older age groups, that obese individuals tend to have a higher SBP than non-obese individuals, and that individuals in more recent birth cohorts have lower SBP’s than those born before 1910. However, these factors are not sufficient to explain all the heterogeneity between individuals since, after correction for age, obesity and birth cohort, individuals with SBP’s above (below) average at initial examination, still have slower (faster) rates of longitudinal change in SBP.',\n",
       "  'authors': ['Geert Verbeke ', ' Geert Molenberghs'],\n",
       "  'date': '2001',\n",
       "  'identifier': '1980911127',\n",
       "  'references': ['2115709314',\n",
       "   '2626446274',\n",
       "   '2156267802',\n",
       "   '1587682423',\n",
       "   '2166190112',\n",
       "   '2098001563',\n",
       "   '2120220100',\n",
       "   '2127684760',\n",
       "   '2045443942',\n",
       "   '2126602143'],\n",
       "  'title': 'Linear Mixed Models for Longitudinal Data'},\n",
       " {'abstract': 'Principal component analysis (PCA) is a ubiquitous technique for data analysis and processing, but one which is not based upon a probability model. In this paper we demonstrate how the principal axes of a set of observed data vectors may be determined through maximum-likelihood estimation of parameters in a latent variable model closely related to factor analysis. We consider the properties of the associated likelihood function, giving an EM algorithm for estimating the principal subspace iteratively, and discuss the advantages conveyed by the definition of a probability density function for PCA.',\n",
       "  'authors': ['Michael E. Tipping ', ' Christopher M. Bishop'],\n",
       "  'date': '1999',\n",
       "  'identifier': '2125027820',\n",
       "  'references': ['2156909104',\n",
       "   '2148694408',\n",
       "   '2119821739',\n",
       "   '2117812871',\n",
       "   '2140095548',\n",
       "   '2132549764',\n",
       "   '2087347434',\n",
       "   '2044758663',\n",
       "   '2108995755',\n",
       "   '2147800946'],\n",
       "  'title': 'Probabilistic Principal Component Analysis'},\n",
       " {'abstract': '',\n",
       "  'authors': ['J. Rasmussen'],\n",
       "  'date': '1987',\n",
       "  'identifier': '1983186110',\n",
       "  'references': ['2007567868',\n",
       "   '1994594651',\n",
       "   '2079641210',\n",
       "   '2159397557',\n",
       "   '2184459361',\n",
       "   '2285097965',\n",
       "   '2127255315',\n",
       "   '1586012768',\n",
       "   '2057144864',\n",
       "   '2044013956'],\n",
       "  'title': 'Skills, rules, and knowledge; signals, signs, and symbols, and other distinctions in human performance models'},\n",
       " {'abstract': 'We study the recognition of surfaces made from different materials such as concrete, rug, marble or leather on the basis of their textural appearance. Such natural textures arise from spatial variation of two surface attributes: (1) reflectance and (2) surface normal. In this paper, we provide a unified model to address both these aspects of natural texture. The main idea is to construct a vocabulary of prototype tiny surface patches with associated local geometric and photometric properties. We call these 3D textons. Examples might be ridges, grooves, spots or stripes or combinations thereof Associated with each texton is an appearance vector, which characterizes the local irradiance distribution, represented as a set of linear Gaussian derivative filter outputs, under different lighting and viewing conditions. Given a large collection of images of different materials, a clustering approach is used to acquire a small (on the order of 100) 3D texton vocabulary. Given a few (1 to 4) images of any material, it can be characterized using these textons. We demonstrate the application of this representation for recognition of the material viewed under novel lighting and viewing conditions.',\n",
       "  'authors': ['T. Leung ', ' J. Malik'],\n",
       "  'date': '1999',\n",
       "  'identifier': '2029727948',\n",
       "  'references': ['2170120409',\n",
       "   '2130416410',\n",
       "   '1997063559',\n",
       "   '1634005169',\n",
       "   '3017143921',\n",
       "   '1490632837',\n",
       "   '2000123870',\n",
       "   '1506013575',\n",
       "   '2115738369',\n",
       "   '1528775006'],\n",
       "  'title': 'Recognizing surfaces using three-dimensional textons'},\n",
       " {'abstract': 'This report contains a small corpus of transcriptions of task oriented spoken conversations in the TRAINS domain. Included are 16 conversations, amounting to over 80 minutes of speech. Also included are a description of the task and collection situation and the conventions used in transcription and utterance segmentation.',\n",
       "  'authors': ['Derek Gross ', ' James F. Allen ', ' David R. Traum'],\n",
       "  'date': '1993',\n",
       "  'identifier': '2915157635',\n",
       "  'references': ['2141766660',\n",
       "   '2165487751',\n",
       "   '2569248448',\n",
       "   '1560781570',\n",
       "   '2159117246',\n",
       "   '2082802837',\n",
       "   '2148249921',\n",
       "   '2018897455',\n",
       "   '2152486776'],\n",
       "  'title': 'The Trains 91 Dialogues'},\n",
       " {'abstract': 'Statistics for Spatial Data GEOSTATISTICAL DATA Geostatistics Spatial Prediction and Kriging Applications of Geostatistics Special Topics in Statistics for Spatial Data LATTICE DATA Spatial Models on Lattices Inference for Lattice Models SPATIAL PATTERNS Spatial Point Patterns Modeling Objects References Author Index Subject Index.',\n",
       "  'authors': ['Noel A. C. Cressie'],\n",
       "  'date': '1991',\n",
       "  'identifier': '2143022286',\n",
       "  'references': ['1746819321',\n",
       "   '2118898434',\n",
       "   '2903158431',\n",
       "   '2144898279',\n",
       "   '1510052597',\n",
       "   '1969461599',\n",
       "   '2093229042',\n",
       "   '2130761473',\n",
       "   '2049244691',\n",
       "   '2131824593'],\n",
       "  'title': 'Statistics for spatial data'},\n",
       " {'abstract': 'The general problem of estimating the a posteriori probabilities of the states and transitions of a Markov source observed through a discrete memoryless channel is considered. The decoding of linear block and convolutional codes to minimize symbol error probability is shown to be a special case of this problem. An optimal decoding algorithm is derived.',\n",
       "  'authors': ['L. Bahl 1', ' J. Cocke 1', ' F. Jelinek 2', ' J. Raviv 1'],\n",
       "  'date': '1974',\n",
       "  'identifier': '2045407304',\n",
       "  'references': ['2142384583',\n",
       "   '1991133427',\n",
       "   '2007321142',\n",
       "   '2166851988',\n",
       "   '2023141412'],\n",
       "  'title': 'Optimal decoding of linear codes for minimizing symbol error rate (Corresp.)'},\n",
       " {'abstract': \"This paper addresses the problem of Face Alignment for a single image. We show how an ensemble of regression trees can be used to estimate the face's landmark positions directly from a sparse subset of pixel intensities, achieving super-realtime performance with high quality predictions. We present a general framework based on gradient boosting for learning an ensemble of regression trees that optimizes the sum of square error loss and naturally handles missing or partially labelled data. We show how using appropriate priors exploiting the structure of image data helps with efficient feature selection. Different regularization strategies and its importance to combat overfitting are also investigated. In addition, we analyse the effect of the quantity of training data on the accuracy of the predictions and explore the effect of data augmentation using synthesized data.\",\n",
       "  'authors': ['Vahid Kazemi ', ' Josephine Sullivan'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2087681821',\n",
       "  'references': ['1554944419',\n",
       "   '3097096317',\n",
       "   '2047508432',\n",
       "   '2038952578',\n",
       "   '1990937109',\n",
       "   '1963599662',\n",
       "   '2076017598',\n",
       "   '1796263212',\n",
       "   '2102512156',\n",
       "   '2136000821'],\n",
       "  'title': 'One Millisecond Face Alignment with an Ensemble of Regression Trees'},\n",
       " {'abstract': 'A new interpretation of the Viterbi decoding algorithm based on the state-space approach to dyamical systems is presented. In this interpretation the optimum decoder solves a generalized regulator control problem by dynamic programming techniques.',\n",
       "  'authors': ['J. Omura'],\n",
       "  'date': '1969',\n",
       "  'identifier': '2151814693',\n",
       "  'references': ['1991133427'],\n",
       "  'title': 'On the Viterbi decoding algorithm'},\n",
       " {'abstract': 'Abstract Remote sensing from airborne and spaceborne platforms provides valuable data for mapping, environmental monitoring, disaster management and civil and military intelligence. However, to explore the full value of these data, the appropriate information has to be extracted and presented in standard format to import it into geo-information systems and thus allow efficient decision processes. The object-oriented approach can contribute to powerful automatic and semi-automatic analysis for most remote sensing applications. Synergetic use to pixel-based or statistical signal processing methods explores the rich information contents. Here, we explain principal strategies of object-oriented analysis, discuss how the combination with fuzzy methods allows implementing expert knowledge and describe a representative example for the proposed workflow from remote sensing imagery to GIS. The strategies are demonstrated using the first object-oriented image analysis software on the market, eCognition, which provides an appropriate link between remote sensing imagery and GIS.',\n",
       "  'authors': ['Ursula C. Benz ',\n",
       "   ' Peter Hofmann ',\n",
       "   ' Gregor Willhauck ',\n",
       "   ' Iris Lingenfelder ',\n",
       "   ' Markus Heynen'],\n",
       "  'date': '2004',\n",
       "  'identifier': '2061240006',\n",
       "  'references': ['2912565176',\n",
       "   '1564419782',\n",
       "   '2078985447',\n",
       "   '2981849677',\n",
       "   '1622620102',\n",
       "   '2021751319',\n",
       "   '2111991188',\n",
       "   '1587235402',\n",
       "   '1981934656',\n",
       "   '1993456370'],\n",
       "  'title': 'Multi-resolution, object-oriented fuzzy analysis of remote sensing data for GIS-ready information'},\n",
       " {'abstract': 'We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.',\n",
       "  'authors': ['Pascal Vincent ',\n",
       "   ' Hugo Larochelle ',\n",
       "   ' Isabelle Lajoie ',\n",
       "   ' Yoshua Bengio ',\n",
       "   ' Pierre-Antoine Manzagol'],\n",
       "  'date': '2010',\n",
       "  'identifier': '2145094598',\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2072128103',\n",
       "   '2116064496',\n",
       "   '2025768430',\n",
       "   '2110798204',\n",
       "   '1652505363',\n",
       "   '3110653090',\n",
       "   '1479807131',\n",
       "   '1994197834'],\n",
       "  'title': 'Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion'},\n",
       " {'abstract': 'Abstract Especially in life-critical systems decision-making entails cognitive functions such as monitoring, as well as fault prevention and recovery. People involved in the control and management of such systems play two kinds of roles: positive thanks to their unique involvement and capacity to deal with the unexpected; and negative with their ability to make errors. But they are also able to detect and correct these mistakes and able to learn from them. Thus human-machine system designer can allow the humans an innovative behavior to be “aware” and to cope with unknown situations by enhancing Situation Awareness (SA). As humans are more and more involved in collective works the constructs of team-SA are important. But the literature shows a great variety and some incoherence in their definitions. That makes difficult to build a design methodology favoring human SA. In parallel, human machine cooperation models have been developed in the last two decades and validated in different dynamic application fields: Air Traffic Control, fighter aircraft cockpit, reconnaissance robot. These studies showed an increase of the problem solving capabilities and a decrease of workload when the tasks are performed by cooperative teams. In this paper we first synthesize main team-SA constructs, we then present principles of humans-machines cooperation and present a Common Work Space as a medium that allow cooperation. We propose to extend it in order to enrich team-SA constructs.',\n",
       "  'authors': ['Patrick Millot ', ' Marie-Pierre Pacaux-Lemoine'],\n",
       "  'date': '2013',\n",
       "  'identifier': '2079641210',\n",
       "  'references': ['2146948159',\n",
       "   '2069757512',\n",
       "   '1983186110',\n",
       "   '2007567868',\n",
       "   '1990414598',\n",
       "   '2086217951',\n",
       "   '1963494573',\n",
       "   '2154625376',\n",
       "   '2011643846',\n",
       "   '2140460278'],\n",
       "  'title': 'A Common Work Space for a Mutual Enrichment of Human-Machine Cooperation and Team-Situation Awareness'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Louisa Lam ', ' Seong-Whan Lee ', ' Ching Y. Suen'],\n",
       "  'date': '1995',\n",
       "  'identifier': '2178432768',\n",
       "  'references': ['2130072711',\n",
       "   '2051842524',\n",
       "   '2127222719',\n",
       "   '2139515308',\n",
       "   '2155700012',\n",
       "   '192646177',\n",
       "   '2047308964',\n",
       "   '2148535985',\n",
       "   '2130281324',\n",
       "   '2065914650'],\n",
       "  'title': 'Thinning methodologies—a comprehensive survey'},\n",
       " {'abstract': 'We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles, such as AGENT or PATIENT, or more domain-specific semantic roles, such as SPEAKER, MESSAGE, and TOPIC.The system is based on statistical classifiers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and its position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles. We used various lexical clustering algorithms to generalize across possible fillers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classifiers.Our system achieves 82% accuracy in identifying the semantic role of presegmented constituents. At the more difficult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.Our study also allowed us to compare the usefulness of different features and feature combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing and attempt to generalize to predicates unseen in the training data.',\n",
       "  'authors': ['Daniel Gildea 1', ' Daniel Jurafsky 2'],\n",
       "  'date': '2002',\n",
       "  'identifier': '2151170651',\n",
       "  'references': [],\n",
       "  'title': 'Automatic labeling of semantic roles'},\n",
       " {'abstract': '',\n",
       "  'authors': ['DC Washington'],\n",
       "  'date': '1994',\n",
       "  'identifier': '2296042795',\n",
       "  'references': [],\n",
       "  'title': 'Diagnostic and Statistical Manual of Mental Disorders, 4th Ed.'},\n",
       " {'abstract': 'The present research develops and tests a theoretical extension of the Technology Acceptance Model (TAM) that explains perceived usefulness and usage intentions in terms of social influence and cognitive instrumental processes. The extended model, referred to as TAM2, was tested using longitudinal data collected regarding four different systems at four organizations ( N = 156), two involving voluntary usage and two involving mandatory usage. Model constructs were measured at three points in time at each organization: preimplementation, one month postimplementation, and three months postimplementation. The extended model was strongly supported for all four organizations at all three points of measurement, accounting for 40%--60% of the variance in usefulness perceptions and 34%--52% of the variance in usage intentions. Both social influence processes (subjective norm, voluntariness, and image) and cognitive instrumental processes (job relevance, output quality, result demonstrability, and perceived ease of use) significantly influenced user acceptance. These findings advance theory and contribute to the foundation for future research aimed at improving our understanding of user adoption behavior.',\n",
       "  'authors': ['Viswanath Venkatesh 1', ' Fred D. Davis 2'],\n",
       "  'date': '2000',\n",
       "  'identifier': '2168569455',\n",
       "  'references': [],\n",
       "  'title': 'A Theoretical Extension of the Technology Acceptance Model: Four Longitudinal Field Studies'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Stuart J. Judge ', ' Barry J. Richmond ', ' Fred C. Chu'],\n",
       "  'date': '1980',\n",
       "  'identifier': '2093642963',\n",
       "  'references': [],\n",
       "  'title': 'Implantation of magnetic search coils for measurement of eye position: an improved method.'},\n",
       " {'abstract': 'Dialogue has its origins in joint activities, which it serves to coordinate. Joint activities, in turn, usually emerge in hierarchically nested projects and subprojects. We propose that participants use dialogue to coordinate two kinds of transitions in these joint projects: vertical transitions, or entering and exiting joint projects; and horizontal transitions, or continuing within joint projects. The participants help signal these transitions with project markers, words such as uh-huh, m-hm, yeah, okay, or all right. These words have been studied mainly as signals of listener feedback (back-channel signals) or turn-taking devices (acknowledgment tokens). We present evidence from several types of well-defined tasks that they are also part of a system of contrasts specialized for navigating joint projects. Uh-huh, m-hm and yeah are used for horizontal transitions, and okay and all right for vertical transitions.',\n",
       "  'authors': ['Adrian Bangerter ', ' Herbert H. Clark'],\n",
       "  'date': '2003',\n",
       "  'identifier': '2152486776',\n",
       "  'references': ['2085529605',\n",
       "   '216945601',\n",
       "   '2264742718',\n",
       "   '2166637769',\n",
       "   '2167702024',\n",
       "   '2153190547',\n",
       "   '1984218255',\n",
       "   '2333196491',\n",
       "   '2039207728',\n",
       "   '2169749310'],\n",
       "  'title': 'Navigating joint projects with dialogue'},\n",
       " {'abstract': 'From the Publisher: This book brings together - in an informal and tutorial fashion - the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems in many fields. Major concepts are illustrated with running examples, and major algorithms are illustrated by Pascal computer programs. No prior knowledge of GAs or genetics is assumed, and only a minimum of computer programming and mathematics background is required.',\n",
       "  'authors': ['David E. Goldberg'],\n",
       "  'date': '1989',\n",
       "  'identifier': '1639032689',\n",
       "  'references': [],\n",
       "  'title': 'Genetic algorithms in search, optimization, and machine learning'},\n",
       " {'abstract': 'In this paper, efficient preprocessing and feature extraction algorithms are presented to achieve high accuracy for handwritten numeral recognition. In preprocessing stage, a connectivity-preserving smoothing algorithm is proposed which is executed after normalization. Afterwards, multiresolution statistical features are extracted from stroke contour by directional decomposition and multiscale filtering. Meanwhile, structural features are represented by horizontal crossing counts and horizontal distances between boundary profiles and corresponding convex hulls. The extracted features not only have powerful discriminating ability, but also are less sensitive to shape variation. The extracted features are inputted into a multilayer neural network for training and classification. The efficiency of the proposed method has been demonstrated in recognition experiments on constrained and unconstrained handwritten numerals. The recognition rate on CENPARMI data is as high as 98.00%.',\n",
       "  'authors': ['Cheng-Lin Liu ', ' Ying-Jian Liu ', ' Ru-Wei Dai'],\n",
       "  'date': '1997',\n",
       "  'identifier': '192646177',\n",
       "  'references': ['2154642048',\n",
       "   '2017787659',\n",
       "   '2178432768',\n",
       "   '2040193698',\n",
       "   '1977582554',\n",
       "   '2135162882',\n",
       "   '1967395061',\n",
       "   '1977791751',\n",
       "   '2102922221',\n",
       "   '2077535401'],\n",
       "  'title': 'Preprocessing and statistical/structural feature extraction for handwritten numeral recognition'},\n",
       " {'abstract': \"As new technologies that support managerial communication become widely used, the question of how and why managers, especially senior managers, use them increases in importance. This paper examines how and why managers use electronic mail. Today, one of the more influential theories of media choice in organization and information science is information richness theory, which has stimulated much empirical research on media selection and has clear implications for how managers should use media. Despite numerous modifications and elaborations, information richness theory remains an individual-level rational choice explanation of behavior, and as such it differs fundamentally from theories that emphasize the social context of managers' communication and media choice behavior. While the weight of informed opinion seems to be shifting toward social theories of media selection and use, much empirical research continues to test individual-level rational choice models. A multi-method investigation was designed to assess the power of information richness theory, relative to alternative social theories, to explain and predict managers' use of email. Managers were found to perceive various media in ways that were relatively consistent with information richness theory, but to use email more and differently than the theory predicted. In particular, effective senior managers were found to use email heavily and even for equivocal communications tasks. These results cannot be explained by information richness theory or by simple modifications of the theory. Rather, they suggest that the adoption, use, and consequences of media in organizations can be powerfully shaped by social processes such as sponsorship, socialization, and social control, which require social perspectives to understand them. These processes can result in differences across organizations and other social units in the patterns of using traditional media like the telephone, but such differences are even more likely for new media, like electronic mail.\",\n",
       "  'authors': ['M. Lynne Markus'],\n",
       "  'date': '1994',\n",
       "  'identifier': '2042846381',\n",
       "  'references': ['1527311855',\n",
       "   '2004184632',\n",
       "   '2108752510',\n",
       "   '2125272611',\n",
       "   '2169736278',\n",
       "   '1499268907',\n",
       "   '2024372407',\n",
       "   '2041765503',\n",
       "   '2907899433',\n",
       "   '2128992662'],\n",
       "  'title': 'Electronic Mail as the Medium of Managerial Choice'},\n",
       " {'abstract': \"What makes people love and die for nations, as well as hate and kill in their name? While many studies have been written on nationalist political movements, the sense of nationality - the personal and cultural feeling of belonging to the nation - has not received proportionate attention. In this widely acclaimed work, Benedict Anderson examines the creation and global spread of the 'imagined communities' of nationality. Anderson explores the processes that created these communities: the territorialisation of religious faiths, the decline of antique kingship, the interaction between capitalism and print, the development of vernacular languages-of-state, and changing conceptions of time. He shows how an originary nationalism born in the Americas was modularly adopted by popular movements in Europe, by the imperialist powers, and by the anti-imperialist resistances in Asia and Africa. This revised edition includes two new chapters, one of which discusses the complex role of the colonialist state's mindset in the development of Third World nationalism, while the other analyses the processes by which all over the world, nations came to imagine themselves as old.\",\n",
       "  'authors': [\"Benedict Richard O'Gorman Anderson\"],\n",
       "  'date': '1983',\n",
       "  'identifier': '2121393013',\n",
       "  'references': ['2016563917',\n",
       "   '2150045067',\n",
       "   '1995088878',\n",
       "   '2118712147',\n",
       "   '2278772485',\n",
       "   '2056180069',\n",
       "   '1560831680',\n",
       "   '321383054',\n",
       "   '1728021200',\n",
       "   '1590562243'],\n",
       "  'title': 'Imagined Communities: Reflections on the Origin and Spread of Nationalism'},\n",
       " {'abstract': 'This paper describes TextTiling, an algorithm for partitioning expository texts into coherent multi-paragraph discourse units which reflect the subtopic structure of the texts. The algorithm uses domain-independent lexical frequency and distribution information to recognize the interactions of multiple simultaneous themes. Two fully-implemented versions of the algorithm are described and shown to produce segmentation that corresponds well to human judgments of the major subtopic boundaries of thirteen lengthy texts.',\n",
       "  'authors': ['Marti A. Hearst'],\n",
       "  'date': '1994',\n",
       "  'identifier': '2100873065',\n",
       "  'references': ['2102381086',\n",
       "   '1833785989',\n",
       "   '1570542661',\n",
       "   '2167702024',\n",
       "   '1710422233',\n",
       "   '2040004971',\n",
       "   '2015933299',\n",
       "   '1977182536',\n",
       "   '1516391399',\n",
       "   '2071188151'],\n",
       "  'title': 'MULTI-PARAGRAPH SEGMENTATION EXPOSITORY TEXT'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Kevin Clark ', ' Christopher D. Manning'],\n",
       "  'date': '2016',\n",
       "  'identifier': '2963167649',\n",
       "  'references': ['1665214252',\n",
       "   '2121863487',\n",
       "   '2155069789',\n",
       "   '2251035762',\n",
       "   '2158349948',\n",
       "   '2963695529',\n",
       "   '2098345921',\n",
       "   '2251064706',\n",
       "   '2252247041',\n",
       "   '1612381393'],\n",
       "  'title': 'Deep Reinforcement Learning for Mention-Ranking Coreference Models'},\n",
       " {'abstract': 'Summary. We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multifactor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.',\n",
       "  'authors': ['Ming Yuan 1', ' Yi Lin 2'],\n",
       "  'date': '2006',\n",
       "  'identifier': '2138019504',\n",
       "  'references': ['2135046866',\n",
       "   '1973948212',\n",
       "   '2063978378',\n",
       "   '2074682976',\n",
       "   '2007069447',\n",
       "   '2070094080',\n",
       "   '3105120000',\n",
       "   '2102760656',\n",
       "   '2080726496',\n",
       "   '2084089095'],\n",
       "  'title': 'Model selection and estimation in regression with grouped variables'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Joseph L. Doob'],\n",
       "  'date': '1953',\n",
       "  'identifier': '2799137445',\n",
       "  'references': ['2184024640',\n",
       "   '2111271983',\n",
       "   '2130761473',\n",
       "   '1883186006',\n",
       "   '1545211435',\n",
       "   '2028995298',\n",
       "   '2111616148',\n",
       "   '2044535354',\n",
       "   '2169071224',\n",
       "   '47957325'],\n",
       "  'title': 'Stochastic processes'},\n",
       " {'abstract': 'One of the major drawbacks of orthogonal wavelet transforms is their lack of translation invariance: the content of wavelet subbands is unstable under translations of the input signal. Wavelet transforms are also unstable with respect to dilations of the input signal and, in two dimensions, rotations of the input signal. The authors formalize these problems by defining a type of translation invariance called shiftability. In the spatial domain, shiftability corresponds to a lack of aliasing; thus, the conditions under which the property holds are specified by the sampling theorem. Shiftability may also be applied in the context of other domains, particularly orientation and scale. Jointly shiftable transforms that are simultaneously shiftable in more than one domain are explored. Two examples of jointly shiftable transforms are designed and implemented: a 1-D transform that is jointly shiftable in position and scale, and a 2-D transform that is jointly shiftable in position and orientation. The usefulness of these image representations for scale-space analysis, stereo disparity measurement, and image enhancement is demonstrated. >',\n",
       "  'authors': ['E.P. Simoncelli 1',\n",
       "   ' W.T. Freeman 1',\n",
       "   ' E.H. Adelson 1',\n",
       "   ' D.J. Heeger 2'],\n",
       "  'date': '1992',\n",
       "  'identifier': '2107790757',\n",
       "  'references': ['2170120409',\n",
       "   '2132984323',\n",
       "   '2098914003',\n",
       "   '1996021349',\n",
       "   '2103504761',\n",
       "   '2118877769',\n",
       "   '1991605728',\n",
       "   '2109863423',\n",
       "   '2166982406',\n",
       "   '1627054999'],\n",
       "  'title': 'Shiftable multiscale transforms'},\n",
       " {'abstract': 'Principal component analysis is a fundamental operation in computational data analysis, with myriad applications ranging from web search to bioinformatics to computer vision and image analysis. However, its performance and applicability in real scenarios are limited by a lack of robustness to outlying or corrupted observations. This paper considers the idealized \"robust principal component analysis\" problem of recovering a low rank matrix A from corrupted observations D = A + E. Here, the corrupted entries E are unknown and the errors can be arbitrarily large (modeling grossly corrupted observations common in visual and bioinformatic data), but are assumed to be sparse. We prove that most matrices A can be efficiently and exactly recovered from most error sign-and-support patterns by solving a simple convex program, for which we give a fast and provably convergent algorithm. Our result holds even when the rank of A grows nearly proportionally (up to a logarithmic factor) to the dimensionality of the observation space and the number of errors E grows in proportion to the total number of entries in the matrix. A by-product of our analysis is the first proportional growth results for the related problem of completing a low-rank matrix from a small fraction of its entries. Simulations and real-data examples corroborate the theoretical results, and suggest potential applications in computer vision.',\n",
       "  'authors': ['John Wright 1',\n",
       "   ' Arvind Ganesh 2',\n",
       "   ' Shankar Rao 2',\n",
       "   ' Yigang Peng 1',\n",
       "   ' Yi Ma 1'],\n",
       "  'date': '2009',\n",
       "  'identifier': '2131628350',\n",
       "  'references': [],\n",
       "  'title': 'Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Matrices via Convex Optimization'},\n",
       " {'abstract': 'Contents: Preface. Introduction. Part I: The Environment To Be Perceived.The Animal And The Environment. Medium, Substances, Surfaces. The Meaningful Environment. Part II: The Information For Visual Perception.The Relationship Between Stimulation And Stimulus Information. The Ambient Optic Array. Events And The Information For Perceiving Events. The Optical Information For Self-Perception. The Theory Of Affordances. Part III: Visual Perception.Experimental Evidence For Direct Perception: Persisting Layout. Experiments On The Perception Of Motion In The World And Movement Of The Self. The Discovery Of The Occluding Edge And Its Implications For Perception. Looking With The Head And Eyes. Locomotion And Manipulation. The Theory Of Information Pickup And Its Consequences. Part IV: Depiction.Pictures And Visual Awareness. Motion Pictures And Visual Awareness. Conclusion. Appendixes: The Principal Terms Used in Ecological Optics. The Concept of Invariants in Ecological Optics.',\n",
       "  'authors': ['James Jerome Gibson'],\n",
       "  'date': '1979',\n",
       "  'identifier': '1933657216',\n",
       "  'references': ['2123184471',\n",
       "   '2132454116',\n",
       "   '2153791616',\n",
       "   '2119112357',\n",
       "   '2102381086',\n",
       "   '2150375089',\n",
       "   '2058300759',\n",
       "   '2123179704',\n",
       "   '1520997877'],\n",
       "  'title': 'The Ecological Approach to Visual Perception'},\n",
       " {'abstract': 'Bacterial diversity among environmental samples is commonly assessed with PCR-amplified 16S rRNA gene (16S) sequences. Perceived diversity, however, can be influenced by sample preparation, primer selection, and formation of chimeric 16S amplification products. Chimeras are hybrid products between multiple parent sequences that can be falsely interpreted as novel organisms, thus inflating apparent diversity. We developed a new chimera detection tool called Chimera Slayer (CS). CS detects chimeras with greater sensitivity than previous methods, performs well on short sequences such as those produced by the 454 Life Sciences (Roche) Genome Sequencer, and can scale to large data sets. By benchmarking CS performance against sequences derived from a controlled DNA mixture of known organisms and a simulated chimera set, we provide insights into the factors that affect chimera formation such as sequence abundance, the extent of similarity between 16S genes, and PCR conditions. Chimeras were found to reproducibly form among independent amplifications and contributed to false perceptions of sample diversity and the false identification of novel taxa, with less-abundant species exhibiting chimera rates exceeding 70%. Shotgun metagenomic sequences of our mock community appear to be devoid of 16S chimeras, supporting a role for shotgun metagenomics in validating novel organisms discovered in targeted sequence surveys.',\n",
       "  'authors': ['Brian J. Haas ',\n",
       "   ' Dirk Gevers ',\n",
       "   ' Ashlee M. Earl ',\n",
       "   ' Mike Feldgarden ',\n",
       "   ' Doyle V. Ward ',\n",
       "   ' Georgia Giannoukos ',\n",
       "   ' Dawn Ciulla ',\n",
       "   ' Diana Tabbaa ',\n",
       "   ' Sarah K. Highlander ',\n",
       "   ' Erica Sodergren ',\n",
       "   ' Barbara Methé ',\n",
       "   ' Todd Z. DeSantis ',\n",
       "   ' Joseph F. Petrosino ',\n",
       "   ' Rob Knight ',\n",
       "   ' Bruce W. Birren'],\n",
       "  'date': '2011',\n",
       "  'identifier': '2101120467',\n",
       "  'references': ['2072970694',\n",
       "   '2108718991',\n",
       "   '2152885278',\n",
       "   '2155066423',\n",
       "   '1988925586',\n",
       "   '2045843097',\n",
       "   '2156125289',\n",
       "   '2091922579',\n",
       "   '2087671769',\n",
       "   '1969435169'],\n",
       "  'title': 'Chimeric 16S rRNA sequence formation and detection in Sanger and 454-pyrosequenced PCR amplicons'},\n",
       " {'abstract': 'Introduction to systems general properties of linear systems the geometric approach - analysis, synthesis, robustness optimality.',\n",
       "  'authors': ['Giuseppe Basile ', ' Giovanni Marro'],\n",
       "  'date': '1992',\n",
       "  'identifier': '1507624518',\n",
       "  'references': ['2798909945',\n",
       "   '1971947347',\n",
       "   '2128978199',\n",
       "   '2030033366',\n",
       "   '2130613450',\n",
       "   '1534520802',\n",
       "   '2013737143',\n",
       "   '1964262399',\n",
       "   '2135622428',\n",
       "   '114979488'],\n",
       "  'title': 'Controlled and conditioned invariants in linear system theory'},\n",
       " {'abstract': \"Abstract : To choose their actions, reasoning programs must be able to make assumptions and subsequently revise their beliefs when discoveries contradict these assumtions. The Truth Maintenance System (TMS) is a problem solver subsystem for performing these functions by recording and maintaining the reasons for program beliefs. Such recorded reasons are useful in constructing explanations for program actions and in guiding the course of action of a problem solver. This paper describes (1) the representations and structure of the TMS, (2) the mechanisms used to revise the current set of beliefs, (3) how dependency-directed backtracking changes the current set of assumptions, (4) techniques for summarizing explanations of beliefs, (5) how to organize problem solvers into 'dialectically arguing' modules, (6) how to revise models of the belief systems of others, and (7) methods for embedding control structures in patterns of assumptions. We stress the need of problem solvers to choose between alternate systems of beliefs, and outline a mechanism by which a problem solver can employ rules guiding choices of what to believe, what to want, and what to do. (Author)\",\n",
       "  'authors': ['Jon Doyle'],\n",
       "  'date': '1987',\n",
       "  'identifier': '2478175895',\n",
       "  'references': ['2121773050',\n",
       "   '1996347293',\n",
       "   '179611734',\n",
       "   '2038118137',\n",
       "   '2161531345',\n",
       "   '1533070959',\n",
       "   '1545815294',\n",
       "   '1494027264',\n",
       "   '1989483184',\n",
       "   '1485983381'],\n",
       "  'title': 'A truth maintenance system'},\n",
       " {'abstract': 'In a previous paper, the author presented a new velocity estimation algorithm, using orientation tensors and parametric motion models to provide both fast and accurate results. One of the tradeoffs between accuracy and speed was that no attempts were made to obtain regions of coherent motion when estimating the parametric models. In this paper we show how this can be improved by doing a simultaneous segmentation of the motion field. The resulting algorithm is slower than the previous one, but more accurate. This is shown by evaluation on the well-known Yosemite sequence, where already the previous algorithm showed an accuracy which was substantially better than for earlier published methods. This result has now been improved further.',\n",
       "  'authors': ['G. Farneback'],\n",
       "  'date': '2001',\n",
       "  'identifier': '1961031432',\n",
       "  'references': ['3003662786',\n",
       "   '2096579040',\n",
       "   '2100315781',\n",
       "   '1553200558',\n",
       "   '1491716407',\n",
       "   '2031912957',\n",
       "   '1542873368',\n",
       "   '2120507918',\n",
       "   '2116755564',\n",
       "   '1994764242'],\n",
       "  'title': 'Very high accuracy velocity estimation using orientation tensors, parametric motion, and simultaneous segmentation of the motion field'},\n",
       " {'abstract': 'It has previously been shown that gradient-descent learning algorithms for recurrent neural networks can perform poorly on tasks that involve long-term dependencies, i.e. those problems for which the desired output depends on inputs presented at times far in the past. We show that the long-term dependencies problem is lessened for a class of architectures called nonlinear autoregressive models with exogenous (NARX) recurrent neural networks, which have powerful representational capabilities. We have previously reported that gradient descent learning can be more effective in NARX networks than in recurrent neural network architectures that have \"hidden states\" on problems including grammatical inference and nonlinear system identification. Typically, the network converges much faster and generalizes better than other networks. The results in this paper are consistent with this phenomenon. We present some experimental results which show that NARX networks can often retain information for two to three times as long as conventional recurrent neural networks. We show that although NARX networks do not circumvent the problem of long-term dependencies, they can greatly improve performance on long-term dependency problems. We also describe in detail some of the assumptions regarding what it means to latch information robustly and suggest possible ways to loosen these assumptions.',\n",
       "  'authors': ['Tsungnan Lin ', ' B.G. Horne ', ' P. Tino ', ' C.L. Giles'],\n",
       "  'date': '1996',\n",
       "  'identifier': '2103452139',\n",
       "  'references': ['2064675550',\n",
       "   '2154642048',\n",
       "   '2138484437',\n",
       "   '2110485445',\n",
       "   '2107878631',\n",
       "   '2798813531',\n",
       "   '2128499899',\n",
       "   '2123716044',\n",
       "   '1674799117',\n",
       "   '2098398123'],\n",
       "  'title': 'Learning long-term dependencies in NARX recurrent neural networks'},\n",
       " {'abstract': \"Note: Includes bibliographical references, 3 appendixes and 2 indexes.- Diskette v 2.06, 3.5''[1.44M] for IBM PC, PS/2 and compatibles [DOS] Reference Record created on 2004-09-07, modified on 2016-08-08\",\n",
       "  'authors': ['William H. Press ',\n",
       "   ' Saul A. Teukolsky ',\n",
       "   ' William T. Vetterling ',\n",
       "   ' Brian P. Flannery'],\n",
       "  'date': '1994',\n",
       "  'identifier': '2170120409',\n",
       "  'references': ['2249139299',\n",
       "   '1746819321',\n",
       "   '1595159159',\n",
       "   '2103546861',\n",
       "   '2132103241',\n",
       "   '2183707334',\n",
       "   '1976969221',\n",
       "   '1998674867',\n",
       "   '2047205370',\n",
       "   '2121016876'],\n",
       "  'title': 'Numerical recipes in C'},\n",
       " {'abstract': 'The concept of situation awareness (SA) is frequently described in the literature. Theoretically, it remains predominantly an individual construct and the majority of the models presented describe SA from an individual perspective. In comparison, team SA has received less attention. SA in complex, collaborative environments thus remains a challenge for the human factors community, both in relation to the development of theoretical perspectives and of valid measures and to the development of guidelines for system, training and procedure design. This article presents a review and critique of what is currently known about SA and team SA, including a comparison of the most prominent individual and team models presented in the literature. In conclusion, it is argued that recently proposed systems level distributed SA approaches are the most suited to describing and assessing SA in real world, collaborative environments.',\n",
       "  'authors': ['Paul Matthew Salmon 1',\n",
       "   ' Neville Anthony Stanton 1',\n",
       "   ' Guy H Walker 1',\n",
       "   ' C Baber 2',\n",
       "   ' Daniel P Jenkins 1',\n",
       "   ' Richard McMaster 2',\n",
       "   ' Mark S Young 1'],\n",
       "  'date': '2008',\n",
       "  'identifier': '1963494573',\n",
       "  'references': ['2146948159',\n",
       "   '2085529605',\n",
       "   '1606313724',\n",
       "   '1555912304',\n",
       "   '2069757512',\n",
       "   '2149504281',\n",
       "   '2086217951',\n",
       "   '2024378172',\n",
       "   '1989989210',\n",
       "   '2049890080'],\n",
       "  'title': 'What really is going on? Review of situation awareness models for individuals and teams'},\n",
       " {'abstract': 'Abstract Oestrosis is a worldwide myiasis caused by larvae of the fly Oestrus ovis, which are obligate parasites of the nasal and sinus cavities of sheep and goats, especially in Mediterranean countries. The aim of this study was to compare the infection levels and seasonal patterns of oestrosis in sheep and goats from areas of similar climate and to explore some potential risk factors associated with this disease in Greece. Of 450 sheep and goat heads examined, 246 (54%) were infected with O. ovis larvae. Goats (76%, 95% CI 68–82) were more commonly infected than sheep (38%, 95% CI 37–49). Larval stages were recovered during all months of the year from both host species, with an overall mean infection intensity of 8.7 ± 13.1 (mean ± SD). Host species, month, area and age, but not nose colour, affected the chance of being infected. Parasite intensity varied but was not affected by these factors except for age and season. Seasonal variation in prevalence was much more pronounced for larval stage 1, than for total larvae. In sheep, infection was generally more common and intense in early spring and in autumn, whereas prevalence peaked in goats in winter. Both prevalence and larval intensity increased with age above 3–4 years in sheep, but not in goats. Patterns of aggregation were consistent with density-dependent constraints to development in the host, and suggested lower susceptibility to larval establishment in goats in spite of higher overall prevalence.',\n",
       "  'authors': ['Elias Papadopoulos 1',\n",
       "   ' Ilias Chaligiannis 1',\n",
       "   ' Eric R. Morgan 2'],\n",
       "  'date': '2010',\n",
       "  'identifier': '2152112501',\n",
       "  'references': ['2011162291',\n",
       "   '2043465320',\n",
       "   '2085277252',\n",
       "   '2015896194',\n",
       "   '1979114215',\n",
       "   '1977909163',\n",
       "   '2154773075',\n",
       "   '2065887985',\n",
       "   '2156765195',\n",
       "   '2262554405'],\n",
       "  'title': 'Epidemiology of Oestrus ovis L. (Diptera: Oestridae) larvae in sheep and goats in Greece'},\n",
       " {'abstract': 'During the past two decades, linkage analysis has been phenomenally successful in localizing Mendelian disease genes. Linkage disequilibrium (LD) analysis, which effectively incorporates the effects of many past generations of recombination, has often been instrumental in the final phases of gene localization (Feder et al. 1996; Hastbacka et al. 1994; Kerem et al. 1989). These successes have fueled hopes that similar approaches will be effective in localizing genes underlying susceptibility to common, complex diseases. With the exception of Mendelian subsets of common diseases (e.g., BRCA1 and BRCA2 for breast cancer, APC for colon cancer, the LDL receptor gene for heart disease), progress on this front has been limited. Typically, a nonparametric linkage analysis, such as a sib-pair analysis, will implicate several genetic regions as targets for further investigation. These regions, often 10–20 Mb in size, remain intractably large for effective positional cloning. It is now hoped that LD approaches, using hundreds of thousands of new polymorphic markers, will overcome this impasse (Risch and Merikangas 1996). The rationale underlying LD mapping of complex disease genes is straightforward and similar to the justification for LD mapping of Mendelian disease genes. With both types of disease genes, the primary advantage of LD analysis remains its ability to use the effects of dozens or hundreds of past generations of recombination to achieve fine-scale gene localization (Jorde 1995). An important difficulty, common to both types of disease genes, is that past historical events (admixture, genetic drift, multiple mutations, and natural selection) can disturb the relationship between LD and inter-locus physical distance. A major difference, of course, is that locus heterogeneity complicates the analysis of complex diseases and may be more extensive for these diseases than for most Mendelian diseases. Furthermore, allelic heterogeneity may be present at each locus. This heterogeneity, the scope of which is largely unknown, will limit the strength of association between a given polymorphism and an observable phenotype. Despite these challenges, LD mapping holds considerable appeal, and there is great demand to resolve the genetics of complex diseases. Consequently, many new techniques have been devised to carry out LD analysis, often with a view toward mapping complex disease loci. The purpose of this review is to summarize these techniques and some of the issues surrounding their application. In particular, the evolutionary factors that can confound or enhance disequilibrium analysis will be discussed, and some thoughts will be offered on the optimal choice of markers and populations for LD analysis.',\n",
       "  'authors': ['L.B. Jorde'],\n",
       "  'date': '2000',\n",
       "  'identifier': '2056713533',\n",
       "  'references': ['2042103448',\n",
       "   '1543868019',\n",
       "   '2002605005',\n",
       "   '15453418',\n",
       "   '2124277514',\n",
       "   '2062026889',\n",
       "   '2125391335',\n",
       "   '2116329478',\n",
       "   '2078625752',\n",
       "   '1711580613'],\n",
       "  'title': 'Linkage Disequilibrium and the Search for Complex Disease Genes'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Alberto Torchinsky'],\n",
       "  'date': '1986',\n",
       "  'identifier': '2149461673',\n",
       "  'references': ['2119372119',\n",
       "   '608533263',\n",
       "   '2289808822',\n",
       "   '2088277224',\n",
       "   '1866712883',\n",
       "   '2041509838',\n",
       "   '2018236137',\n",
       "   '1569973690',\n",
       "   '347227607'],\n",
       "  'title': 'Real-Variable Methods in Harmonic Analysis'},\n",
       " {'abstract': 'This book is a rigorous exposition of formal languages and models of computation, with an introduction to computational complexity. The authors present the theory in a concise and straightforward manner, with an eye out for the practical applications. Exercises at the end of each chapter, including some that have been solved, help readers confirm and enhance their understanding of the material. This book is appropriate for upper-level computer science undergraduates who are comfortable with mathematical arguments.',\n",
       "  'authors': ['John E. Hopcroft ',\n",
       "   ' Rajeev Motwani ',\n",
       "   ' Rotwani ',\n",
       "   ' Jeffrey D. Ullman'],\n",
       "  'date': '1979',\n",
       "  'identifier': '2002089154',\n",
       "  'references': ['2101508170',\n",
       "   '2340735175',\n",
       "   '1556566737',\n",
       "   '1527197079',\n",
       "   '2099529102',\n",
       "   '2092654472',\n",
       "   '2001496424',\n",
       "   '1845137714',\n",
       "   '1984052055',\n",
       "   '1944672'],\n",
       "  'title': 'Introduction to Automata Theory, Languages, and Computation'},\n",
       " {'abstract': 'The number of digits it takes to write down an observed sequence x\"1, ..., x\"N of a time series depends on the model with its parameters that one assumes to have generated the observed data. Accordingly, by finding the model which minimizes the description length one obtains estimates of both the integer-valued structure parameters and the real-valued system parameters.',\n",
       "  'authors': ['J. Rissanen'],\n",
       "  'date': '1978',\n",
       "  'identifier': '2054658115',\n",
       "  'references': [],\n",
       "  'title': 'Paper: Modeling by shortest data description'},\n",
       " {'abstract': '1. The extracellular patch clamp method, which first allowed the detection of single channel currents in biological membranes, has been further refined to enable higher current resolution, direct membrane patch potential control, and physical isolation of membrane patches. 2. A description of a convenient method for the fabrication of patch recording pipettes is given together with procedures followed to achieve giga-seals i.e. pipettemembrane seals with resistances of 109–1011Ω. 3. The basic patch clamp recording circuit, and designs for improved frequency response are described along with the present limitations in recording the currents from single channels. 4. Procedures for preparation and recording from three representative cell types are given. Some properties of single acetylcholine-activated channels in muscle membrane are described to illustrate the improved current and time resolution achieved with giga-seals. 5. A description is given of the various ways that patches of membrane can be physically isolated from cells. This isolation enables the recording of single channel currents with well-defined solutions on both sides of the membrane. Two types of isolated cell-free patch configurations can be formed: an inside-out patch with its cytoplasmic membrane face exposed to the bath solution, and an outside-out patch with its extracellular membrane face exposed to the bath solution. 6. The application of the method for the recording of ionic currents and internal dialysis of small cells is considered. Single channel resolution can be achieved when recording from whole cells, if the cell diameter is small (<20μm). 7. The wide range of cell types amenable to giga-seal formation is discussed.',\n",
       "  'authors': ['Owen P. Hamill ',\n",
       "   ' A. Marty ',\n",
       "   ' Erwin Neher ',\n",
       "   ' Bert Sakmann ',\n",
       "   ' F. Sigworth'],\n",
       "  'date': '1981',\n",
       "  'identifier': '2009667219',\n",
       "  'references': ['1971880089',\n",
       "   '2082345341',\n",
       "   '1990955686',\n",
       "   '2066914900',\n",
       "   '2131445842',\n",
       "   '1986909301',\n",
       "   '2108942635',\n",
       "   '2148071508',\n",
       "   '2171669937',\n",
       "   '2101218219'],\n",
       "  'title': 'Improved patch-clamp techniques for high-resolution current recording from cells and cell-free membrane patches.'},\n",
       " {'abstract': 'Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications.',\n",
       "  'authors': ['Christopher D. Manning 1', ' Hinrich Schütze 2'],\n",
       "  'date': '1999',\n",
       "  'identifier': '1574901103',\n",
       "  'references': ['182831726',\n",
       "   '1508165687',\n",
       "   '1994851566',\n",
       "   '1549026077',\n",
       "   '2108321481',\n",
       "   '2949237929',\n",
       "   '1795234945',\n",
       "   '1736036918',\n",
       "   '1746620543'],\n",
       "  'title': 'Foundations of Statistical Natural Language Processing'},\n",
       " {'abstract': 'The author reviews recent multichannel models developed in psychophysiology, computer vision, and image processing. In psychophysiology, multichannel models have been particularly successful in explaining some low-level processing in the visual cortex. The expansion of a function into several frequency channels provides a representation which is intermediate between a spatial and a Fourier representation. The author describes the mathematical properties of such decompositions and introduces the wavelet transform. He reviews the classical multiresolution pyramidal transforms developed in computer vision and shows how they relate to the decomposition of an image into a wavelet orthonormal basis. He discusses the properties of the zero crossings of multifrequency channels. Zero-crossing representations are particularly well adapted for pattern recognition in computer vision. >',\n",
       "  'authors': ['S.G. Mallat'],\n",
       "  'date': '1989',\n",
       "  'identifier': '2166982406',\n",
       "  'references': ['2132984323',\n",
       "   '2098914003',\n",
       "   '2103504761',\n",
       "   '2109863423',\n",
       "   '2003370853',\n",
       "   '1980149518',\n",
       "   '2096684483',\n",
       "   '2022735534',\n",
       "   '2139797453',\n",
       "   '1995756857'],\n",
       "  'title': 'Multifrequency channel decompositions of images and wavelet models'},\n",
       " {'abstract': 'A randomised approximation scheme for the permanent of a 0–1s presented. The task of estimating a permanent is reduced to that of almost uniformly generating perfect matchings in a graph; the latter is accomplished by simulating a Markov chain whose states are the matchings in the graph. For a wide class of 0–1 matrices the approximation scheme is fully-polynomial, i.e., runs in time polynomial in the size of the matrix and a parameter that controls the accuracy of the output. This class includes all dense matrices (those that contain sufficiently many 1’s) and almost all sparse matrices in some reasonable probabilistic model for 0–1 matrices of given density.For the approach sketched above to be computationally efficient, the Markov chain must be rapidly mixing: informally, it must converge in a short time to its stationary distribution. A major portion of the paper is devoted to demonstrating that the matchings chain is rapidly mixing, apparently the first such result for a Markov chain with genuinely c...',\n",
       "  'authors': ['M. Jerrum ', ' Alistair Sinclair'],\n",
       "  'date': '1989',\n",
       "  'identifier': '2106285343',\n",
       "  'references': ['2905110430',\n",
       "   '2005228957',\n",
       "   '2072211488',\n",
       "   '2047422346',\n",
       "   '2157211828',\n",
       "   '1552744309',\n",
       "   '2113750873',\n",
       "   '2074599161',\n",
       "   '2046097442',\n",
       "   '2058699951'],\n",
       "  'title': 'Approximating the permanent'},\n",
       " {'abstract': 'A technique is presented for the formal specification and modeling of multimedia composition with respect to intermedia timing. The proposed model is based on the logic of temporal intervals and timed Petri nets. A strategy is evinced for constructing a database schema to facilitate data storage and retrieval of media elements based on the temporal relationship established by the proposed modeling tool. An algorithm which allows the retrieval of media elements from the constructed database in a manner which preserves the temporal requirements of the initial specification is presented. Using the proposed model, the synchronization requirements of complex structures of temporally related objects can be easily specified. >',\n",
       "  'authors': ['T.D.C. Little ', ' A. Ghafoor'],\n",
       "  'date': '1990',\n",
       "  'identifier': '2054609588',\n",
       "  'references': ['2176300081',\n",
       "   '1524764420',\n",
       "   '2158046522',\n",
       "   '2047922146',\n",
       "   '2087311398',\n",
       "   '113037826',\n",
       "   '3003496299',\n",
       "   '2006415155',\n",
       "   '2001846247',\n",
       "   '2151786168'],\n",
       "  'title': 'Synchronization and storage models for multimedia objects'},\n",
       " {'abstract': 'Valiant (1984) and others have studied the problem of learning various classes of Boolean functions from examples. Here we discuss incremental learning of these functions. We consider a setting in which the learner responds to each example according to a current hypothesis. Then the learner updates the hypothesis, if necessary, based on the correct classification of the example. One natural measure of the quality of learning in this setting is the number of mistakes the learner makes. For suitable classes of functions, learning algorithms are available that make a bounded number of mistakes, with the bound independent of the number of examples seen by the learner. We present one such algorithm that learns disjunctive Boolean functions, along with variants for learning other classes of Boolean functions. The basic method can be expressed as a linear-threshold algorithm. A primary advantage of this algorithm is that the number of mistakes grows only logarithmically with the number of irrelevant attributes in the examples. At the same time, the algorithm is computationally efficient in both time and space.',\n",
       "  'authors': ['Nick Littlestone'],\n",
       "  'date': '1988',\n",
       "  'identifier': '2129113961',\n",
       "  'references': ['1652505363',\n",
       "   '3017143921',\n",
       "   '2019363670',\n",
       "   '2154952480',\n",
       "   '2139709458',\n",
       "   '2070902649',\n",
       "   '2009207944',\n",
       "   '2066789935',\n",
       "   '2061079066',\n",
       "   '2091401625'],\n",
       "  'title': 'Learning Quickly When Irrelevant Attributes Abound: A New Linear-Threshold Algorithm'},\n",
       " {'abstract': 'Computer techniques readily extract from the brainwaves an orderly sequence of brain potentials locked in time to sound stimuli. The potentials that appear 8 to 80 msec after the stimulus resemble 3 or 4 cycles of a 40-Hz sine wave; we show here that these waves combined to form a single, stable, composite wave when the sounds are repeated at rates around 40 per sec. This phenomenon, the 40-Hz event-related potential (ERP), displays several properties of theoretical and practical interest. First, it reportedly disappears with surgical anesthesia, and it resembles similar phenomena in the visual and olfactory system, facts which suggest that adequate processing of sensory information may require cyclical brain events in the 30- to 50-Hz range. Second, latency and amplitude measurements on the 40-Hz ERP indicate it may contain useful information on the number and basilar membrane location of the auditory nerve fibers a given tone excites. Third, the response is present at sound intensities very close to normal adult thresholds for the audiometric frequencies, a fact that could have application in clinical hearing testing.',\n",
       "  'authors': ['Robert Galambos ', ' Scott Makeig ', ' Peter J. Talmachoff'],\n",
       "  'date': '1981',\n",
       "  'identifier': '2085497225',\n",
       "  'references': ['1987211298',\n",
       "   '2048995850',\n",
       "   '2075095590',\n",
       "   '2470545476',\n",
       "   '2048412154',\n",
       "   '2403756422',\n",
       "   '2019507286',\n",
       "   '2022226708',\n",
       "   '2060534129',\n",
       "   '1976591718'],\n",
       "  'title': 'A 40-Hz auditory potential recorded from the human scalp'},\n",
       " {'abstract': \"ContextMost older adults with dementia will be cared for by primary care physicians, but the primary care practice environment presents important challenges to providing quality care.ObjectiveTo test the effectiveness of a collaborative care model to improve the quality of care for patients with Alzheimer disease.Design, Setting, and PatientsControlled clinical trial of 153 older adults with Alzheimer disease and their caregivers who were randomized by physician to receive collaborative care management (n = 84) or augmented usual care (n = 69) at primary care practices within 2 US university-affiliated health care systems from January 2002 through August 2004. Eligible patients (identified via screening or medical record) met diagnostic criteria for Alzheimer disease and had a self-identified caregiver.InterventionIntervention patients received 1 year of care management by an interdisciplinary team led by an advanced practice nurse working with the patient's family caregiver and integrated within primary care. The team used standard protocols to initiate treatment and identify, monitor, and treat behavioral and psychological symptoms of dementia, stressing nonpharmacological management.Main Outcome MeasuresNeuropsychiatric Inventory (NPI) administered at baseline and at 6, 12, and 18 months. Secondary outcomes included the Cornell Scale for Depression in Dementia (CSDD), cognition, activities of daily living, resource use, and caregiver's depression severity.ResultsInitiated by caregivers' reports, 89% of intervention patients triggered at least 1 protocol for behavioral and psychological symptoms of dementia with a mean of 4 per patient from a total of 8 possible protocols. Intervention patients were more likely to receive cholinesterase inhibitors (79.8% vs 55.1%; P = .002) and antidepressants (45.2% vs 27.5%; P = .03). Intervention patients had significantly fewer behavioral and psychological symptoms of dementia as measured by the total NPI score at 12 months (mean difference, −5.6; P = .01) and at 18 months (mean difference, −5.4; P = .01). Intervention caregivers also reported significant improvements in distress as measured by the caregiver NPI at 12 months; at 18 months, caregivers showed improvement in depression as measured by the Patient Health Questionnaire-9. No group differences were found on the CSDD, cognition, activities of daily living, or on rates of hospitalization, nursing home placement, or death.ConclusionsCollaborative care for the treatment of Alzheimer disease resulted in significant improvement in the quality of care and in behavioral and psychological symptoms of dementia among primary care patients and their caregivers. These improvements were achieved without significantly increasing the use of antipsychotics or sedative-hypnotics.Trial Registrationclinicaltrials.gov Identifier: NCT00246896\",\n",
       "  'authors': ['Christopher M. Callahan 1',\n",
       "   ' Malaz A. Boustani 2',\n",
       "   ' Frederick W. Unverzagt 3',\n",
       "   ' Mary G. Austrom 3',\n",
       "   ' Teresa M. Damush 2',\n",
       "   ' Anthony J. Perkins 2',\n",
       "   ' Bridget A. Fultz 2',\n",
       "   ' Siu L. Hui 2',\n",
       "   ' Steven R. Counsell 2',\n",
       "   ' Hugh C. Hendrie 2'],\n",
       "  'date': '2006',\n",
       "  'identifier': '2029421651',\n",
       "  'references': ['2132322340',\n",
       "   '2111963393',\n",
       "   '2147735030',\n",
       "   '2107820540',\n",
       "   '2132806643',\n",
       "   '2088361570',\n",
       "   '2075478727',\n",
       "   '2158766477',\n",
       "   '2082914293',\n",
       "   '2042353763'],\n",
       "  'title': 'Effectiveness of collaborative care for older adults with Alzheimer disease in primary care: a randomized controlled trial.'},\n",
       " {'abstract': 'An effective procedure for automatically acquiring a new set of disambiguation rules for an existing deterministic parser on the basis of tagged text is presented. Performance of the automatically acquired rules is much better than the existing hand-written disambiguation rules. The success of the acquired rules depends on using the linguistic information encoded in the parser; enhancements to various components of the parser improves the acquired rule set. This work suggests a path toward more robust and comprehensive syntactic analyzers.',\n",
       "  'authors': ['Donald Hindle'],\n",
       "  'date': '1989',\n",
       "  'identifier': '2121407024',\n",
       "  'references': ['2099247782',\n",
       "   '2017580301',\n",
       "   '1571096757',\n",
       "   '1981724541',\n",
       "   '1487155516',\n",
       "   '2158652440',\n",
       "   '2124102576',\n",
       "   '1590656471',\n",
       "   '2015773474'],\n",
       "  'title': 'ACQUIRING DISAMBIGUATION RULES FROM TEXT'},\n",
       " {'abstract': 'Alternative descriptions of a decision problem often give rise to different preferences, contrary to the principle of invariance that underlines the rational theory of choice. Violations of this theory are traced to the rules that govern the framing of decision and to the psychological principles of evaluation embodied in prospect theory. Invariance and dominance are obeyed when their application is transparent and often violated in other situations. Because these rules are normatively essential but descriptively invalid, no theory of choice can be both normatively adequate and descriptively accurate.',\n",
       "  'authors': ['A. Tversky 1', ' D. Kahneman 2'],\n",
       "  'date': '1990',\n",
       "  'identifier': '1986646649',\n",
       "  'references': ['2061592058',\n",
       "   '2133469585',\n",
       "   '2137358449',\n",
       "   '3011865677',\n",
       "   '2096452841',\n",
       "   '2120733867',\n",
       "   '1599449303',\n",
       "   '2024140722',\n",
       "   '1983084915',\n",
       "   '2042223112'],\n",
       "  'title': 'Rational choice and the framing of decisions'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Sidney Addelman'],\n",
       "  'date': '1978',\n",
       "  'identifier': '1968226729',\n",
       "  'references': ['2044771513',\n",
       "   '2073503722',\n",
       "   '1510052597',\n",
       "   '1484750607',\n",
       "   '149807329',\n",
       "   '2116483002',\n",
       "   '2108630796',\n",
       "   '2000836282',\n",
       "   '2105793531',\n",
       "   '2289748525'],\n",
       "  'title': 'Statistics for experimenters'},\n",
       " {'abstract': 'The subject of collective attention is central to an information age where millions of people are inundated with daily messages. It is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. We have analyzed the dynamics of collective attention among 1 million users of an interactive web site, digg.com, devoted to thousands of novel news stories. The observations can be described by a dynamical model characterized by a single novelty factor. Our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.',\n",
       "  'authors': ['Fang Wu 1', ' Bernardo A. Huberman 2'],\n",
       "  'date': '2007',\n",
       "  'identifier': '2058465497',\n",
       "  'references': ['1994473607',\n",
       "   '2019921249',\n",
       "   '2064786700',\n",
       "   '2094136133',\n",
       "   '2124073865',\n",
       "   '2017891730',\n",
       "   '2146395977',\n",
       "   '1585487328',\n",
       "   '1990588956'],\n",
       "  'title': 'Novelty and collective attention'},\n",
       " {'abstract': 'We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions.',\n",
       "  'authors': ['Peter Young ',\n",
       "   ' Alice Lai ',\n",
       "   ' Micah Hodosh ',\n",
       "   ' Julia Hockenmaier'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2185175083',\n",
       "  'references': ['2120779048',\n",
       "   '2248026759',\n",
       "   '1647729745',\n",
       "   '2525127255',\n",
       "   '1897761818',\n",
       "   '2066134726',\n",
       "   '2109586012',\n",
       "   '1984052055',\n",
       "   '2117805756',\n",
       "   '2251861449'],\n",
       "  'title': 'From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions'},\n",
       " {'abstract': 'In the last issue of this journal Mitchell, Keller, and Kedar-Cabelli presented a unifying framework for the explanation-based approach to machine learning. While it works well for a number of systems, the framework does not adequately capture certain aspects of the systems under development by the explanation-based learning group at Illinois. The primary inadequacies arise in the treatment of concept operationality, organization of knowledge into schemata, and learning from observation. This paper outlines six specific problems with the previously proposed framework and presents an alternative generalization method to perform explanation-based learning of new concepts.',\n",
       "  'authors': ['Gerald Dejong ', ' Raymond Mooney'],\n",
       "  'date': '1986',\n",
       "  'identifier': '2101602574',\n",
       "  'references': ['1596324102',\n",
       "   '2152475379',\n",
       "   '2121773050',\n",
       "   '2180885055',\n",
       "   '2020149918',\n",
       "   '2000900121',\n",
       "   '1608168306',\n",
       "   '2026319679',\n",
       "   '1981627423',\n",
       "   '38913427'],\n",
       "  'title': 'Explanation-Based Learning: An Alternative View'},\n",
       " {'abstract': 'For many computer vision and machine learning problems, large training sets are key for good performance. However, the most computationally expensive part of many computer vision and machine learning algorithms consists of finding nearest neighbor matches to high dimensional vectors that represent the training data. We propose new algorithms for approximate nearest neighbor matching and evaluate and compare them with previous algorithms. For matching high dimensional features, we find two algorithms to be the most efficient: the randomized k-d forest and a new algorithm proposed in this paper, the priority search k-means tree. We also propose a new algorithm for matching binary features by searching multiple hierarchical clustering trees and show it outperforms methods typically used in the literature. We show that the optimal nearest neighbor algorithm and its parameters depend on the data set characteristics and describe an automated configuration procedure for finding the best algorithm to search a particular data set. In order to scale to very large data sets that would otherwise not fit in the memory of a single machine, we propose a distributed nearest neighbor matching framework that can be used with any of the algorithms described in the paper. All this research has been released as an open source library called fast library for approximate nearest neighbors (FLANN), which has been incorporated into OpenCV and is now one of the most popular libraries for nearest neighbor matching.',\n",
       "  'authors': ['Marius Muja 1', ' David G. Lowe 2'],\n",
       "  'date': '2014',\n",
       "  'identifier': '2086504823',\n",
       "  'references': ['2151103935',\n",
       "   '2108598243',\n",
       "   '2901136733',\n",
       "   '2131846894',\n",
       "   '2117228865',\n",
       "   '2128017662',\n",
       "   '2145607950',\n",
       "   '2141362318',\n",
       "   '2097998348',\n",
       "   '1491719799'],\n",
       "  'title': 'Scalable Nearest Neighbor Algorithms for High Dimensional Data'},\n",
       " {'abstract': 'Abstract Synchronous 25- to 35-Hz oscillations were observed in local field potentials and unit activity in sensorimotor cortex of awake rhesus monkeys. The oscillatory episodes occurred often when the monkeys retrieved raisins from a Kluver board or from unseen locations using somatosensory feedback; they occurred less often during performance of repetitive wrist flexion and extension movements. The amplitude, duration, and frequency of oscillations were not directly related to movement parameters in behaviors studied so far. The occurrence of the oscillations was not consistently related to bursts of activity in forearm muscles, but cycle-triggered averages of electromyograms revealed synchronous modulation in flexor and extensor muscles. The phase of the oscillations changed continuously from the surface to the deeper layers of the cortex, reversing their polarity completely at depths exceeding 800 microns. The oscillations could become synchronized over a distance of 14 mm mediolaterally in precentral cortex. Coherent oscillations could also occur at pre- and postcentral sites separated by an estimated tangential intracortical distance of 20 mm. Activity of single units was commonly seen to burst in synchrony with field potential oscillations. These findings suggest that such oscillations may facilitate interactions between cells during exploratory and manipulative movements, requiring attention to sensorimotor integration.',\n",
       "  'authors': ['Venkatesh N. Murthy ', ' Eberhard E. Fetz'],\n",
       "  'date': '1992',\n",
       "  'identifier': '1990973912',\n",
       "  'references': ['2114104729',\n",
       "   '2116308679',\n",
       "   '1603307924',\n",
       "   '2162013747',\n",
       "   '2060589548',\n",
       "   '2035090004',\n",
       "   '2112332687',\n",
       "   '2046198005',\n",
       "   '2165892338',\n",
       "   '2040149530'],\n",
       "  'title': 'Coherent 25- to 35-Hz oscillations in the sensorimotor cortex of awake behaving monkeys.'},\n",
       " {'abstract': 'Data hiding, a form of steganography, embeds data into digital media for the purpose of identification, annotation, and copyright. Several constraints affect this process: the quantity of data to be hidden, the need for invariance of these data under conditions where a \"host\" signal is subject to distortions, e.g., lossy compression, and the degree to which the data must be immune to interception, modification, or removal by a third party. We explore both traditional and novel techniques for addressing the data-hiding process and evaluate these techniques in light of three applications: copyright protection, tamper-proofing, and augmentation data embedding.',\n",
       "  'authors': ['W. Bender 1', ' D. Gruhl 1', ' N. Morimoto 2', ' Aiguo Lu 1'],\n",
       "  'date': '1996',\n",
       "  'identifier': '2159390040',\n",
       "  'references': ['2116467012',\n",
       "   '2069501481',\n",
       "   '2084398124',\n",
       "   '1569879151',\n",
       "   '2191402453',\n",
       "   '1551265734',\n",
       "   '1655519805'],\n",
       "  'title': 'Techniques for data hiding'},\n",
       " {'abstract': 'We address the problem, fundamental to linguistics, bioinformatics, and certain other disciplines, of using corpora of raw symbolic sequential data to infer underlying rules that govern their production. Given a corpus of strings (such as text, transcribed speech, chromosome or protein sequence data, sheet music, etc.), our unsupervised algorithm recursively distills from it hierarchically structured patterns. The adios (automatic distillation of structure) algorithm relies on a statistical method for pattern extraction and on structured generalization, two processes that have been implicated in language acquisition. It has been evaluated on artificial context-free grammars with thousands of rules, on natural languages as diverse as English and Chinese, and on protein data correlating sequence with function. This unsupervised algorithm is capable of learning complex syntax, generating grammatical novel sentences, and proving useful in other fields that call for structure discovery from raw data, such as bioinformatics.',\n",
       "  'authors': ['Zach Solan ',\n",
       "   ' David Horn ',\n",
       "   ' Eytan Ruppin ',\n",
       "   ' Shimon Edelman'],\n",
       "  'date': '2009',\n",
       "  'identifier': '2143296986',\n",
       "  'references': ['1574901103',\n",
       "   '2115792525',\n",
       "   '2002089154',\n",
       "   '1980862600',\n",
       "   '1558866924',\n",
       "   '2107745473',\n",
       "   '1502101714',\n",
       "   '1485243506',\n",
       "   '2108321481',\n",
       "   '2050423830'],\n",
       "  'title': 'Unsupervised Learning of Natural Languages'},\n",
       " {'abstract': 'A dual mode facsimile data compression technique, called Combined Symbol Matching (CSM) , is presented. The CSM technique possesses the advantages of symbol recognition and extended run-length coding methods. In operation, a symbol blocking operator isolates valid alphanumeric characters and document symbols. The first symbol encountered is placed in a library, and as each new symbol is detected, it is compared with each entry of the library. If the comparison is within a tolerance, the library identification code is transmitted along with the symbol location coordinates. Otherwise, the new symbol is placed in the library and its binary pattern is transmitted. A scoring system determines which elements of the library are to be replaced by new prototypes once the library is filled. Non-isolated symbols are left behind as a residue, and are coded by a two-dimensional run-length coding method. Simulation results are presented for CCITT standard documents. With text-predominate documents, the CSM compression ratio exceeds that obtained with the best run-length coding techniques by a factor of two or more, and is comparable for graphics-predominate documents.© (1979) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.',\n",
       "  'authors': ['Wen-Hsiung Chen ',\n",
       "   ' John L. Douglas ',\n",
       "   ' William K. Pratt ',\n",
       "   ' Robert H. Wallis'],\n",
       "  'date': '1979',\n",
       "  'identifier': '1994606555',\n",
       "  'references': ['2015432638',\n",
       "   '2165564574',\n",
       "   '2042455828',\n",
       "   '2016318414',\n",
       "   '2182279964'],\n",
       "  'title': 'Dual-Mode Hybrid Compressor For Facsimile Images'},\n",
       " {'abstract': 'In speech translation, we are faced with the problem of how to couple the speech recognition process and the translation process. Starting from the Bayes decision rule for speech translation, we analyze how the interaction between the recognition process and the translation process can be modelled. In the light of this decision rule, we discuss the already existing approaches to speech translation. None of the existing approaches seems to have addressed this direct interaction. We suggest two new methods, the local averaging approximation and the monotone alignments.',\n",
       "  'authors': ['H. Ney'],\n",
       "  'date': '1999',\n",
       "  'identifier': '2113106066',\n",
       "  'references': ['2006969979',\n",
       "   '2146418175',\n",
       "   '2422872931',\n",
       "   '1603508585',\n",
       "   '2139647714',\n",
       "   '2012511220',\n",
       "   '2158164089',\n",
       "   '2166810516',\n",
       "   '2096312440',\n",
       "   '3088213079'],\n",
       "  'title': 'Speech translation: coupling of recognition and translation'},\n",
       " {'abstract': 'Abstract To take advantage of the high spectral resolution of Landsat TM images and the high spatial resolution of SPOT panchromatic images (SPOT PAN), we present a wavelet transform method to merge the two data types. In a pyramidal fashion, each TM reflective band or SPOT PAN image was decomposed into an orthogonal wavelet representation at a given coarser resolution, which consisted of a low frequency approximation image and a set of high frequency, spatially-oriented detail images. Band-by-band, the merged images were derived by performing an inverse wavelet transform using the approximation image from each TM band and detail images from SPOT PAN. The spectral and spatial features of the merged results of the wavelet methods were compared quantitatively with those of intensity-hue-saturation (IHS), principal component analysis (PCA), and the Brovey transform. It was found that multisensor data merging is a trade-off between the spectral information from a low spatial-high spectral resolution sensor an...',\n",
       "  'authors': ['J. Zhou ', ' D. L. Civco ', ' J. A. Silander'],\n",
       "  'date': '1998',\n",
       "  'identifier': '1991460509',\n",
       "  'references': ['2056346283',\n",
       "   '2136401825',\n",
       "   '2152254169',\n",
       "   '2097259623',\n",
       "   '2165651437',\n",
       "   '2462592242',\n",
       "   '2158537567',\n",
       "   '2123046940',\n",
       "   '2163677711'],\n",
       "  'title': 'A wavelet transform method to merge Landsat TM and SPOT panchromatic data'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Richard P. Brent'],\n",
       "  'date': '1975',\n",
       "  'identifier': '1964712205',\n",
       "  'references': ['2130116522',\n",
       "   '2135536698',\n",
       "   '2289748525',\n",
       "   '2201713963',\n",
       "   '2157098139',\n",
       "   '2038473742',\n",
       "   '2160960847',\n",
       "   '1990275569',\n",
       "   '2096739162',\n",
       "   '2008107087'],\n",
       "  'title': 'Table errata: Algorithms for minimization without derivatives (Prentice-Hall, Englewood Cliffs, N. J., 1973)'},\n",
       " {'abstract': '',\n",
       "  'authors': ['K. D Tocher'],\n",
       "  'date': '1964',\n",
       "  'identifier': '2116414161',\n",
       "  'references': [],\n",
       "  'title': 'The art of simulation'},\n",
       " {'abstract': \"A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the OCR'ed archives of the journal Science from 1880 through 2000.\",\n",
       "  'authors': ['David M. Blei 1', ' John D. Lafferty 2'],\n",
       "  'date': '2006',\n",
       "  'identifier': '2072644219',\n",
       "  'references': [],\n",
       "  'title': 'Dynamic topic models'},\n",
       " {'abstract': 'Abstract A versatile assembly system, using TV cameras and computer-controlled arm and moving table, is described. It makes simple assemblies such as a peg and rings and a toy car. It separates parts from a heap, recognizing them with an overhead camera, then assembles them by feel. It can be instructed to perform a new task with different parts by spending an hour or two showing it the parts and a day programming the assembly manipulations. A hierarchical description of parts, views, outlines, etc. is used to construct models, and a structure matching algorithm is used in recognition.',\n",
       "  'authors': ['A.P. Ambler ',\n",
       "   ' H.G. Barrow ',\n",
       "   ' C.M. Brown ',\n",
       "   ' R.M. Burstall ',\n",
       "   ' R.J. Popplestone'],\n",
       "  'date': '1975',\n",
       "  'identifier': '1993324373',\n",
       "  'references': [],\n",
       "  'title': 'A versatile system for computer-controlled assembly☆'},\n",
       " {'abstract': 'Abstract : It is known that for the pattern classification problem where only a finite number of training samples are available, in general performance improves, reaches a maximum, and then starts deteriorating as the number of measurements is increased. However, one of the authors has shown that for independent measurements of binary quantization, the measurement complexity can be arbitrarily increased without fear of this peaking of performance. In the paper the authors consider the case of independent measurements with arbitrary quantization. (Author)',\n",
       "  'authors': ['B. Chandrasekaran ', ' A. K. Jain'],\n",
       "  'date': '1972',\n",
       "  'identifier': '209146099',\n",
       "  'references': [],\n",
       "  'title': 'Quantization of Independent Measurements and Recognition Performance'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Robert Plutchik'],\n",
       "  'date': '1980',\n",
       "  'identifier': '2093862925',\n",
       "  'references': ['2156503193',\n",
       "   '2075456404',\n",
       "   '2096423181',\n",
       "   '2067302347',\n",
       "   '2135869619',\n",
       "   '1594610956',\n",
       "   '2584561145',\n",
       "   '2102770351',\n",
       "   '1544543982'],\n",
       "  'title': 'Emotion, a psychoevolutionary synthesis'},\n",
       " {'abstract': 'This paper describes a hierarchical estimation framework for the computation of diverse representations of motion information. The key features of the resulting framework (or family of algorithms) are a global model that constrains the overall structure of the motion estimated, a local model that is used in the estimation process, and a coarse-fine refinement strategy. Four specific motion models: affine flow, planar surface flow, rigid body motion, and general optical flow, are described along with their application to specific examples.',\n",
       "  'authors': ['James R. Bergen ',\n",
       "   ' P. Anandan ',\n",
       "   ' Keith J. Hanna ',\n",
       "   ' Rajesh Hingorani'],\n",
       "  'date': '1992',\n",
       "  'identifier': '1938714998',\n",
       "  'references': ['2911709767',\n",
       "   '2103504761',\n",
       "   '2620619910',\n",
       "   '2118877769',\n",
       "   '2130657708',\n",
       "   '2168564612',\n",
       "   '2043003144',\n",
       "   '2179278902',\n",
       "   '2168262846',\n",
       "   '1991518198'],\n",
       "  'title': 'Hierarchical Model-Based Motion Estimation'},\n",
       " {'abstract': 'Many systems have been developed for constructing decision trees from collections of examples. Although the decision trees generated by these methods are accurate and efficient, they often suffer the disadvantage of excessive complexity and are therefore incomprehensible to experts. It is questionable whether opaque structures of this kind can be described as knowledge, no matter how well they function. This paper discusses techniques for simplifying decision trees while retaining their accuracy. Four methods are described, illustrated, and compared on a test-bed of decision trees from a variety of domains.',\n",
       "  'authors': ['J. R. Quinlan'],\n",
       "  'date': '1987',\n",
       "  'identifier': '2128420091',\n",
       "  'references': ['3085162807',\n",
       "   '2149706766',\n",
       "   '2798643531',\n",
       "   '1874939829',\n",
       "   '1567276288',\n",
       "   '3021257214',\n",
       "   '2894813436',\n",
       "   '3022593335',\n",
       "   '98436501',\n",
       "   '2063990820'],\n",
       "  'title': 'Simplifying decision trees'},\n",
       " {'abstract': 'This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case.',\n",
       "  'authors': ['Michael I. Jordan 1',\n",
       "   ' Zoubin Ghahramani 2',\n",
       "   ' Tommi S. Jaakkola 3',\n",
       "   ' Lawrence K. Saul 4'],\n",
       "  'date': '1999',\n",
       "  'identifier': '1516111018',\n",
       "  'references': ['2099111195',\n",
       "   '2159080219',\n",
       "   '1573186872',\n",
       "   '2049633694',\n",
       "   '2171265988',\n",
       "   '2982720039',\n",
       "   '1746680969',\n",
       "   '1615454278',\n",
       "   '2567948266',\n",
       "   '1993845689'],\n",
       "  'title': 'An introduction to variational methods for graphical models'},\n",
       " {'abstract': 'While different optical flow techniques continue to appear, there has been a lack of quantitative evaluation of existing methods. For a common set of real and synthetic image sequences, we report the results of a number of regularly cited optical flow techniques, including instances of differential, matching, energy-based, and phase-based methods. Our comparisons are primarily empirical, and concentrate on the accuracy, reliability, and density of the velocity measurements; they show that performance can differ significantly among the techniques we implemented.',\n",
       "  'authors': ['J. L. Barron 1', ' D. J. Fleet 2', ' S. S. Beauchemin 1'],\n",
       "  'date': '1994',\n",
       "  'identifier': '3003662786',\n",
       "  'references': ['2911709767',\n",
       "   '2103504761',\n",
       "   '2620619910',\n",
       "   '2118877769',\n",
       "   '2003370853',\n",
       "   '2130657708',\n",
       "   '2108992228',\n",
       "   '2031912957',\n",
       "   '2146203184',\n",
       "   '1587794861'],\n",
       "  'title': 'Performance of optical flow techniques'},\n",
       " {'abstract': 'This tutorial paper provides a foundation for integrating probabilistic shaping (PS) and forward error correction (FEC). A layered PS-FEC architecture consisting of a PS encoder and an FEC encoder is introduced, of which probabilistic amplitude shaping is a practical instance. Achievable PS encoding rates and achievable FEC decoding rates are derived using information-theoretic arguments. The developed tools are applied to the design and performance assessment of optical transponders based on measurements from optical transmission experiments.',\n",
       "  'authors': ['Georg Bocherer 1', ' Patrick Schulte 2', ' Fabian Steiner 2'],\n",
       "  'date': '2019',\n",
       "  'identifier': '2910758021',\n",
       "  'references': ['2049633694',\n",
       "   '627952176',\n",
       "   '2126259959',\n",
       "   '1999631531',\n",
       "   '2296257523',\n",
       "   '2060093775',\n",
       "   '3098131962',\n",
       "   '2183415080',\n",
       "   '2952222053',\n",
       "   '2580864287'],\n",
       "  'title': 'Probabilistic Shaping and Forward Error Correction for Fiber-Optic Communication Systems'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Steven V. Earhart'],\n",
       "  'date': '1986',\n",
       "  'identifier': '38814960',\n",
       "  'references': ['2987803397',\n",
       "   '1789713128',\n",
       "   '2109507516',\n",
       "   '2099964107',\n",
       "   '2100648544',\n",
       "   '2086991199',\n",
       "   '2010876815',\n",
       "   '2070053315',\n",
       "   '111098712',\n",
       "   '1975770848'],\n",
       "  'title': \"UNIX programmer's manual\"},\n",
       " {'abstract': \"Recognizing lines of unconstrained handwritten text is a challenging task. The difficulty of segmenting cursive or overlapping characters, combined with the need to exploit surrounding context, has led to low recognition rates for even the best current recognizers. Most recent progress in the field has been made either through improved preprocessing or through advances in language modeling. Relatively little work has been done on the basic recognition algorithms. Indeed, most systems rely on the same hidden Markov models that have been used for decades in speech and handwriting recognition, despite their well-known shortcomings. This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies. In experiments on two large unconstrained handwriting databases, our approach achieves word recognition accuracies of 79.7 percent on online data and 74.1 percent on offline data, significantly outperforming a state-of-the-art HMM-based system. In addition, we demonstrate the network's robustness to lexicon size, measure the individual influence of its hidden layers, and analyze its use of context. Last, we provide an in-depth discussion of the differences between the network and HMMs, suggesting reasons for the network's superior performance.\",\n",
       "  'authors': ['A. Graves 1',\n",
       "   ' M. Liwicki 2',\n",
       "   ' S. Fernandez 3',\n",
       "   ' R. Bertolami 4',\n",
       "   ' H. Bunke 4',\n",
       "   ' J. Schmidhuber 1'],\n",
       "  'date': '2009',\n",
       "  'identifier': '2122585011',\n",
       "  'references': ['2064675550',\n",
       "   '2125838338',\n",
       "   '2127141656',\n",
       "   '3023071679',\n",
       "   '2107878631',\n",
       "   '2142069714',\n",
       "   '2131774270',\n",
       "   '1578856370',\n",
       "   '2079735306',\n",
       "   '2147568880'],\n",
       "  'title': 'A Novel Connectionist System for Unconstrained Handwriting Recognition'},\n",
       " {'abstract': 'It is widely acknowledged that many professionals suffer from \"e-mail overload.\" This article presents findings from in-depth fieldwork that examined this phenomenon, uncovering six key challenges of taskmanagement in e-mail. Analysis of qualitative and quantitative data suggests that it is not simply the quantity but also the collaborative quality of e-mail task and project management that causes this overload. We describe how e-mail becomes especially overwhelming when people use it for tasks that involve participation of others; tasks cannot be completed until a response is obtained and so they are interleaved. Interleaving means that the email user must somehow simultaneously keep track of multiple incomplete tasks, often with the only reminder for each one being an e-mail message somewhere in the inbox or a folder. This and other insights from our fieldwork led us to a new design philosophy for e-mail in which resources for task and project management are embedded directly within an e-mail client as opposed to being added on as separate components of the application. A client, TaskMaster, embodying these ideas, was developed and tested by users in managing their real e-mail over an extended period. The design of the client and results of its evaluation are also reported.',\n",
       "  'authors': ['Victoria Bellotti 1',\n",
       "   ' Nicolas Ducheneaut 1',\n",
       "   ' Mark Howard 1',\n",
       "   ' Ian Smith 1',\n",
       "   ' Rebecca E. Grinter 2'],\n",
       "  'date': '2005',\n",
       "  'identifier': '2147261473',\n",
       "  'references': ['1493688518',\n",
       "   '2137891816',\n",
       "   '2169736278',\n",
       "   '2024372407',\n",
       "   '2165990684',\n",
       "   '2026669942',\n",
       "   '156307932',\n",
       "   '1971167516',\n",
       "   '2402213046',\n",
       "   '2096418111'],\n",
       "  'title': 'Quality versus quantity: e-mail-centric task management and its relation with overload'},\n",
       " {'abstract': 'If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/',\n",
       "  'authors': ['Joseph Turian 1', ' Lev-Arie Ratinov 2', ' Yoshua Bengio 1'],\n",
       "  'date': '2010',\n",
       "  'identifier': '2158139315',\n",
       "  'references': ['1880262756',\n",
       "   '2117130368',\n",
       "   '2132339004',\n",
       "   '1662133657',\n",
       "   '2131462252',\n",
       "   '2296073425',\n",
       "   '168564468',\n",
       "   '2158997610',\n",
       "   '2156515921',\n",
       "   '2004763266'],\n",
       "  'title': 'Word Representations: A Simple and General Method for Semi-Supervised Learning'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Cameron R. Peterson ',\n",
       "   ' Z. J. Ulehla ',\n",
       "   ' Alan J. Miller ',\n",
       "   ' Lyle E. Bourne ',\n",
       "   ' Donald W. Stilson'],\n",
       "  'date': '1965',\n",
       "  'identifier': '2074854537',\n",
       "  'references': ['1967485420',\n",
       "   '2013645142',\n",
       "   '2057399',\n",
       "   '2018497029',\n",
       "   '1969354648',\n",
       "   '2067863856'],\n",
       "  'title': 'Internal consistency of subjective probabilities'},\n",
       " {'abstract': 'We describe and analyze a simple and effective iterative algorithm for solving the optimization problem cast by Support Vector Machines (SVM). Our method alternates between stochastic gradient descent steps and projection steps. We prove that the number of iterations required to obtain a solution of accuracy e is O(1/e). In contrast, previous analyses of stochastic gradient descent methods require Ω (1/e2) iterations. As in previously devised SVM solvers, the number of iterations also scales linearly with 1/λ, where λ is the regularization parameter of SVM. For a linear kernel, the total run-time of our method is O (d/(λe)), where d is a bound on the number of non-zero features in each example. Since the run-time does not depend directly on the size of the training set, the resulting algorithm is especially suited for learning from large datasets. Our approach can seamlessly be adapted to employ non-linear kernels while working solely on the primal objective function. We demonstrate the efficiency and applicability of our approach by conducting experiments on large text classification problems, comparing our solver to existing state-of-the-art SVM solvers. For example, it takes less than 5 seconds for our solver to converge when solving a text classification problem from Reuters Corpus Volume 1 (RCV1) with 800,000 training examples.',\n",
       "  'authors': ['Shai Shalev-Shwartz 1', ' Yoram Singer 1', ' Nathan Srebro 2'],\n",
       "  'date': '2007',\n",
       "  'identifier': '2142623206',\n",
       "  'references': ['2296319761',\n",
       "   '2148603752',\n",
       "   '1563088657',\n",
       "   '1512098439',\n",
       "   '1601740268',\n",
       "   '2035720976',\n",
       "   '2165966284',\n",
       "   '2160218441',\n",
       "   '3017143921',\n",
       "   '2113651538'],\n",
       "  'title': 'Pegasos: Primal Estimated sub-GrAdient SOlver for SVM'},\n",
       " {'abstract': 'We consider the problem of detecting a large number of different object classes in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, which can be slow and require much training data. We present a multi-class boosting procedure (joint boosting) that reduces both the computational and sample complexity, by finding common features that can be shared across the classes. The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required is observed to scale approximately logarithmically with the number of classes. In addition, we find that the features selected by independently trained classifiers are often specific to the class, whereas the features selected by the jointly trained classifiers are more generic features, such as lines and edges.',\n",
       "  'authors': ['A. Torralba ', ' K.P. Murphy ', ' W.T. Freeman'],\n",
       "  'date': '2004',\n",
       "  'identifier': '2171188998',\n",
       "  'references': ['3097096317',\n",
       "   '2124386111',\n",
       "   '2024046085',\n",
       "   '2017337590',\n",
       "   '1608462934',\n",
       "   '2123977795',\n",
       "   '2155511848',\n",
       "   '2101276256',\n",
       "   '2914746235',\n",
       "   '2168020168'],\n",
       "  'title': 'Sharing features: efficient boosting procedures for multiclass object detection'},\n",
       " {'abstract': 'Power-law distributions occur in many situations of scientific interest and have significant consequences for our understanding of natural and man-made phenomena. Unfortunately, the detection and characterization of power laws is complicated by the large fluctuations that occur in the tail of the distribution—the part of the distribution representing large but rare events—and by the difficulty of identifying the range over which power-law behavior holds. Commonly used methods for analyzing power-law data, such as least-squares fitting, can produce substantially inaccurate estimates of parameters for power-law distributions, and even in cases where such methods return accurate answers they are still unsatisfactory because they give no indication of whether the data obey a power law at all. Here we present a principled statistical framework for discerning and quantifying power-law behavior in empirical data. Our approach combines maximum-likelihood fitting methods with goodness-of-fit tests based on the Kolmogorov-Smirnov (KS) statistic and likelihood ratios. We evaluate the effectiveness of the approach with tests on synthetic data and give critical comparisons to previous approaches. We also apply the proposed methods to twenty-four real-world data sets from a range of different disciplines, each of which has been conjectured to follow a power-law distribution. In some cases we find these conjectures to be consistent with the data, while in others the power law is ruled out.',\n",
       "  'authors': ['Aaron Clauset 1',\n",
       "   ' 2',\n",
       "   ' Cosma Rohilla Shalizi 3',\n",
       "   ' M. E. J. Newman 4'],\n",
       "  'date': '2009',\n",
       "  'identifier': '2000042664',\n",
       "  'references': ['2331432542',\n",
       "   '2313307644',\n",
       "   '2165363188',\n",
       "   '1496357020',\n",
       "   '2950627632',\n",
       "   '1994702090',\n",
       "   '2175110005',\n",
       "   '2432517183',\n",
       "   '2981264952',\n",
       "   '2168175751'],\n",
       "  'title': 'Power-Law Distributions in Empirical Data'},\n",
       " {'abstract': 'We discuss the following problem given a random sample X = (X 1, X 2,…, X n) from an unknown probability distribution F, estimate the sampling distribution of some prespecified random variable R(X, F), on the basis of the observed data x. (Standard jackknife theory gives an approximate mean and variance in the case R(X, F) = \\\\(\\\\theta \\\\left( {\\\\hat F} \\\\right) - \\\\theta \\\\left( F \\\\right)\\\\), θ some parameter of interest.) A general method, called the “bootstrap”, is introduced, and shown to work satisfactorily on a variety of estimation problems. The jackknife is shown to be a linear approximation method for the bootstrap. The exposition proceeds by a series of examples: variance of the sample median, error rates in a linear discriminant analysis, ratio estimation, estimating regression parameters, etc.',\n",
       "  'authors': ['Bradley Efron'],\n",
       "  'date': '1979',\n",
       "  'identifier': '2117897510',\n",
       "  'references': ['2063698478',\n",
       "   '1981492171',\n",
       "   '2325429543',\n",
       "   '2079100340',\n",
       "   '1989584148',\n",
       "   '2061905897',\n",
       "   '2033260623',\n",
       "   '1971382239',\n",
       "   '1528761061',\n",
       "   '2069645522'],\n",
       "  'title': 'Bootstrap Methods: Another Look at the Jackknife'},\n",
       " {'abstract': 'In diary studies, people provide frequent reports on the events and experiences of their daily lives. These reports capture the particulars of experience in a way that is not possible using traditional designs. We review the types of research questions that diary methods are best equipped to answer, the main designs that can be used, current technology for obtaining diary reports, and appropriate data analysis strategies. Major recent developments include the use of electronic forms of data collection and multilevel models in data analysis. We identify several areas of research opportunities: 1. in technology, combining electronic diary reports with collateral measures such as ambulatory heart rate; 2. in measurement, switching from measures based on between-person differences to those based on within-person changes; and 3. in research questions, using diaries to (a) explain why people differ in variability rather than mean level, (b) study change processes during major events and transitions, and (c) study interpersonal processes using dyadic and group diary methods.',\n",
       "  'authors': ['Niall Bolger 1', ' Angelina Davis 2', ' Eshkol Rafaeli 2'],\n",
       "  'date': '2003',\n",
       "  'identifier': '2166190112',\n",
       "  'references': ['2324392187',\n",
       "   '1980911127',\n",
       "   '1592263067',\n",
       "   '2112778345',\n",
       "   '1571998446',\n",
       "   '2056434135',\n",
       "   '1565240145',\n",
       "   '1873057782',\n",
       "   '2170210994',\n",
       "   '2905732640'],\n",
       "  'title': 'Diary Methods: Capturing Life as it is Lived'},\n",
       " {'abstract': 'Abstract : This report describes a knowledge-base system in which the information is stored in a network of small parallel processing elements--node and link units--which are controlled by an external serial computer. Discussed is NETL, a language for storing real-world information in such a network. A simulator for the parallel network system has been implemented in MACLISP, and an experimental version of NETL is running on this simulator. A number of test-case results and simulated timings will be presented. (Author)',\n",
       "  'authors': ['Scott E. Fahlman'],\n",
       "  'date': '1979',\n",
       "  'identifier': '1533070959',\n",
       "  'references': ['2159080219',\n",
       "   '2102381086',\n",
       "   '2141312052',\n",
       "   '123527740',\n",
       "   '2478175895',\n",
       "   '2017668967',\n",
       "   '2112325651',\n",
       "   '2103537992'],\n",
       "  'title': 'NETL: A System for Representing and Using Real-World Knowledge'},\n",
       " {'abstract': 'This paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution-free (PAC) learning model. A concept class is learnable (or strongly learnable) if, given access to a source of examples of the unknown concept, the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances. The concept class is weakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing. In this paper, it is shown that these two notions of learnability are equivalent. A method is described for converting a weak learning algorithm into one that achieves arbitrarily high accuracy. This construction may have practical applications as a tool for efficiently converting a mediocre learning algorithm into one that performs extremely well. In addition, the construction has some interesting theoretical consequences, including a set of general upper bounds on the complexity of any strong learning algorithm as a function of the allowed error e.',\n",
       "  'authors': ['Robert E. Schapire'],\n",
       "  'date': '1990',\n",
       "  'identifier': '2093717447',\n",
       "  'references': ['2019363670',\n",
       "   '2154952480',\n",
       "   '2139709458',\n",
       "   '2070902649',\n",
       "   '2428981601',\n",
       "   '2117049614',\n",
       "   '2091401625',\n",
       "   '2066688546',\n",
       "   '1965415591',\n",
       "   '2157526632'],\n",
       "  'title': 'The Strength of Weak Learnability'},\n",
       " {'abstract': 'This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies. The procedure essentially involves the construction of functions of the observed proportions which are directed at the extent to which the observers agree among themselves and the construction of test statistics for hypotheses involving these functions. Tests for interobserver bias are presented in terms of first-order marginal homogeneity and measures of interobserver agreement are developed as generalized kappa-type statistics. These procedures are illustrated with a clinical diagnosis example from the epidemiological literature.',\n",
       "  'authors': ['J. R. Landis ', ' Gary G Koch'],\n",
       "  'date': '1977',\n",
       "  'identifier': '2164777277',\n",
       "  'references': ['2053154970',\n",
       "   '2798510847',\n",
       "   '2037789405',\n",
       "   '1975879668',\n",
       "   '1989774929',\n",
       "   '2021142183',\n",
       "   '2018385240',\n",
       "   '2133012565',\n",
       "   '2063803293',\n",
       "   '2088041869'],\n",
       "  'title': 'The measurement of observer agreement for categorical data'},\n",
       " {'abstract': 'The authors present an approach for the recognition of multiple 3-D object models from three 3-D scene data. The approach uses two different types of primitives for matching: small surface patches, where differential properties can be reliably computed, and lines corresponding to depth or orientation discontinuities. These are represented by splashes and 3-D curves, respectively. It is shown how both of these primitives can be encoded by a set of super segments, consisting of connected linear segments. These super segments are entered into a table and provide the essential mechanism for fast retrieval and matching. The issues of robustness and stability of the features are addressed in detail. The acquisition of the 3-D models is performed automatically by computing splashes in highly structured areas of the objects and by using boundary and surface edges for the generation of 3-D curves. The authors present results with the current system (3-D object recognition based on super segments) and discuss further extensions. >',\n",
       "  'authors': ['F. Stein ', ' G. Medioni'],\n",
       "  'date': '1992',\n",
       "  'identifier': '2033834476',\n",
       "  'references': ['2145023731',\n",
       "   '2103497972',\n",
       "   '2154204736',\n",
       "   '1532977286',\n",
       "   '1981154266',\n",
       "   '2068204893',\n",
       "   '2000048778',\n",
       "   '2105364438',\n",
       "   '1978578701',\n",
       "   '2140367930'],\n",
       "  'title': 'Structural indexing: efficient 3-D object recognition'},\n",
       " {'abstract': \"This article describes a new system for induction of oblique decision trees. This system, OC1, combines deterministic hill-climbing with two forms of randomization to find a good oblique split (in the form of a hyperplane) at each node of a decision tree. Oblique decision tree methods are tuned especially for domains in which the attributes are numeric, although they can be adapted to symbolic or mixed symbolic/numeric attributes. We present extensive empirical studies, using both real and artificial data, that analyze OC1's ability to construct oblique trees that are smaller and more accurate than their axis-parallel counterparts. We also examine the benefits of randomization for the construction of oblique decision trees.\",\n",
       "  'authors': ['Sreerama K. Murthy ', ' Simon Kasif ', ' Steven Salzberg'],\n",
       "  'date': '1994',\n",
       "  'identifier': '1515620500',\n",
       "  'references': ['2581275558',\n",
       "   '3085162807',\n",
       "   '2149706766',\n",
       "   '1583700199',\n",
       "   '2132166479',\n",
       "   '2128420091',\n",
       "   '2048997388',\n",
       "   '2159047538',\n",
       "   '1604329830',\n",
       "   '23418094'],\n",
       "  'title': 'A system for induction of oblique decision trees'},\n",
       " {'abstract': \"This case study examines the application of Quinlan's C4.5 to the task of diagnosing a subsystem of NASA's Space Shuttle. Hundreds of thousands of training instances were available from simulator runs and real flight data. The trees produced are highly accurate, moderately small, and after being converted to production rules, were judged by the expert to be not only comprehensible and acceptable, but to contain new knowledge that might otherwise have remained undiscovered. The training set's huge size contributes to the high accuracy. The lack of noise turned out not to be so critical to accuracy, but learning time looks infeasible if extrapolated to a million examples. The complexity of the concept could also grow too large. We point to methods of removing these two stumbling blocks of current machine learning technology.\",\n",
       "  'authors': ['Jason Catlett'],\n",
       "  'date': '1991',\n",
       "  'identifier': '1510806966',\n",
       "  'references': ['2128420091',\n",
       "   '2159047538',\n",
       "   '1597910678',\n",
       "   '1985624473',\n",
       "   '98436501',\n",
       "   '1552785817',\n",
       "   '1515227130'],\n",
       "  'title': 'Megainduction: a test flight'},\n",
       " {'abstract': 'This introduction to modern soil chemistry describes chemical processes in soils in terms of established principles of inorganic, organic, and physical chemistry. The text provides an understanding of the structure of the solid mineral and organic materials from which soils are formed, and explains such important processes as cation exchange, chemisorption and physical absorption of organic and inorganic ions and molecules, soil acidification and weathering, oxidation-reduction reactions, and development of soil alkalinity and swelling properties. Environmental rather than agricultural topics are emphasized, with individual chapters on such pollutants as heavy metals, trace elements, and inorganic chemicals.',\n",
       "  'authors': ['Murray B. Mcbride'],\n",
       "  'date': '1994',\n",
       "  'identifier': '1599334980',\n",
       "  'references': ['2149602337',\n",
       "   '2164998314',\n",
       "   '2201735116',\n",
       "   '2106412044',\n",
       "   '2096379616',\n",
       "   '1970143525',\n",
       "   '1977304780',\n",
       "   '1975320197',\n",
       "   '2167775473',\n",
       "   '2161053097'],\n",
       "  'title': 'Environmental Chemistry of Soils'},\n",
       " {'abstract': 'Training a support vector machine SVM leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples on the shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SVM light is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SVM light V 2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains.',\n",
       "  'authors': ['Thorsten Joachims'],\n",
       "  'date': '1998',\n",
       "  'identifier': '1576520375',\n",
       "  'references': ['2153635508',\n",
       "   '2161969291',\n",
       "   '1964357740',\n",
       "   '2108646579',\n",
       "   '2120419212',\n",
       "   '2172000360',\n",
       "   '2047221353',\n",
       "   '2132870739',\n",
       "   '2035720976',\n",
       "   '2108995755'],\n",
       "  'title': 'Making large scale SVM learning practical'},\n",
       " {'abstract': \"Presents a series of studies showing that the sources of innovation vary greatly; possible sources include innovation users, suppliers of innovation-related components, and product manufacturers. These types of roles are known as functional areas. Specific areas of innovation are marked by having innovators predominantly in one specific functional area. Using empirical data from industrial histories, the authors show that this innovation-function relationship has held in scientific instrument, semiconductor and printed circuit board assembly process innovations. Users are predominantly the innovators in these fields. Also identifies a few industries where manufacturers are typically the innovators and a few others where suppliers tend to be. Analysis of the economic rents of innovation expected by potential innovators can often, if not always, by itself predict the functional source of innovation. Innovating firms will do so only when these rents prove attractive. Two factors suggest that this will tend to limit exploitation of the innovation to a functional area. First, it is difficult for innovators to adopt new functional relationships to their innovations. Second, innovators face a poor ability to capture innovation rents by licensing their innovation-related knowledge to others. This hypothesis and its implications are tested against the empirical datasets used initially. The role of informal R&D know-how trading is also discussed and analyzed using the Prisoner's Dilemma. Guidance is given to innovation managers and policymakers. (CAR)\",\n",
       "  'authors': ['Eric A. von Hippel'],\n",
       "  'date': '1988',\n",
       "  'identifier': '2118243939',\n",
       "  'references': ['2342091124',\n",
       "   '2108795964',\n",
       "   '2132081716',\n",
       "   '2129444086',\n",
       "   '2087712586',\n",
       "   '2099330597',\n",
       "   '2012370212',\n",
       "   '2002779084',\n",
       "   '1983895294',\n",
       "   '1996293917'],\n",
       "  'title': 'The sources of innovation'},\n",
       " {'abstract': 'Olshausen and Field (1996) applied the principle of independence maximization by sparse coding to extract features from natural images. This leads to the emergence of oriented linear filters that have simultaneous localization in space and in frequency, thus resembling Gabor functions and simple cell receptive fields. In this article, we show that the same principle of independence maximization can explain the emergence of phase- and shift-invariant features, similar to those found in complex cells. This new kind of emergence is obtained by maximizing the independence between norms of projections on linear subspaces (instead of the independence of simple linear filter outputs). The norms of the projections on such “independent feature subspaces” then indicate the values of invariant features.',\n",
       "  'authors': ['Aapo Hyvärinen ', ' Patrik Hoyer'],\n",
       "  'date': '2000',\n",
       "  'identifier': '2124486835',\n",
       "  'references': [],\n",
       "  'title': 'Emergence of Phase- and Shift-Invariant Features by Decomposition of Natural Images into Independent Feature Subspaces'},\n",
       " {'abstract': 'This paper gives a unified theoretical view of the Dynamic Time Warping (DTW) and the Hidden Markov Model (HMM) techniques for speech recognition problems. The application of hidden Markov models in speech recognition is discussed. We show that the conventional dynamic time-warping algorithm with Linear Predictive (LP) signal modeling and distortion measurements can be formulated in a strictly statistical framework. It is further shown that the DTW/LP method is implicitly associated with a specific class of Markov models and is equivalent to the probability maximization procedures for Gaussian autoregressive multivariate probabilistic functions of the underlying Markov model. This unified view offers insights into the effectiveness of the probabilistic models in speech recognition applications.',\n",
       "  'authors': ['B.-H. Juang'],\n",
       "  'date': '1984',\n",
       "  'identifier': '2165253089',\n",
       "  'references': ['1966264494',\n",
       "   '2171850596',\n",
       "   '2137089646',\n",
       "   '2583466288',\n",
       "   '1575431606',\n",
       "   '1990005915',\n",
       "   '2021760654',\n",
       "   '2086699924',\n",
       "   '2164240509',\n",
       "   '2008536600'],\n",
       "  'title': 'On the hidden Markov model and dynamic time warping for speech recognition — A unified view'},\n",
       " {'abstract': 'This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.',\n",
       "  'authors': ['Alex Graves'],\n",
       "  'date': '2013',\n",
       "  'identifier': '1810943226',\n",
       "  'references': ['2064675550',\n",
       "   '1554663460',\n",
       "   '2143612262',\n",
       "   '44815768',\n",
       "   '1632114991',\n",
       "   '2131462252',\n",
       "   '196214544',\n",
       "   '2108677974',\n",
       "   '2120861206',\n",
       "   '3023071679'],\n",
       "  'title': 'Generating Sequences With Recurrent Neural Networks'},\n",
       " {'abstract': '',\n",
       "  'authors': [\"B. C. O'connor \", \" M. K. O'connor\"],\n",
       "  'date': '1999',\n",
       "  'identifier': '2612148268',\n",
       "  'references': ['2141282920', '2152027810'],\n",
       "  'title': 'Categories, photographs & predicaments : Exploratory research on representing pictures for access : Theory and practice in the organization of images and other visuo-spatial data for retrieval'},\n",
       " {'abstract': 'Relief algorithms are general and successful attribute estimators. They are able to detect conditional dependencies between attributes and provide a unified view on the attribute estimation in regression and classification. In addition, their quality estimates have a natural interpretation. While they have commonly been viewed as feature subset selection methods that are applied in prepossessing step before a model is learned, they have actually been used successfully in a variety of settings, e.g., to select splits or to guide constructive induction in the building phase of decision or regression tree learning, as the attribute weighting method and also in the inductive logic programming. A broad spectrum of successful uses calls for especially careful investigation of various features Relief algorithms have. In this paper we theoretically and empirically investigate and discuss how and why they work, their theoretical and practical properties, their parameters, what kind of dependencies they detect, how do they scale up to large number of examples and features, how to sample data for them, how robust are they regarding the noise, how irrelevant and redundant attributes influence their output and how different metrics influences them.',\n",
       "  'authors': ['Marko Robnik-Šikonja ', ' Igor Kononenko'],\n",
       "  'date': '2003',\n",
       "  'identifier': '1500895378',\n",
       "  'references': ['2125055259',\n",
       "   '3085162807',\n",
       "   '2149706766',\n",
       "   '1594031697',\n",
       "   '1601529450',\n",
       "   '1583700199',\n",
       "   '1808644423',\n",
       "   '3112020351',\n",
       "   '1496929357',\n",
       "   '190437827'],\n",
       "  'title': 'Theoretical and Empirical Analysis of ReliefF and RReliefF'},\n",
       " {'abstract': 'The article presents a review of the book “Organizational Culture and Leadership,” by Edgar H. Schein.',\n",
       "  'authors': ['Edgar H. Schein'],\n",
       "  'date': '1991',\n",
       "  'identifier': '2027320617',\n",
       "  'references': ['2137247419',\n",
       "   '1501241011',\n",
       "   '1561054077',\n",
       "   '2044858486',\n",
       "   '2150587481',\n",
       "   '2154435902',\n",
       "   '2131305193',\n",
       "   '2010812063',\n",
       "   '2009638102',\n",
       "   '2025579080'],\n",
       "  'title': 'Organizational Culture and Leadership'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Yves Meyer'],\n",
       "  'date': '1991',\n",
       "  'identifier': '2036144352',\n",
       "  'references': ['2078204800',\n",
       "   '2146842127',\n",
       "   '2158940042',\n",
       "   '2079724595',\n",
       "   '2020919250',\n",
       "   '2033484654',\n",
       "   '2163324644',\n",
       "   '2326057307',\n",
       "   '2019099900'],\n",
       "  'title': \"Ondelettes sur l'intervalle.\"},\n",
       " {'abstract': 'Trust and reputation systems represent a significant trend in decision support for Internet mediated service provision. The basic idea is to let parties rate each other, for example after the completion of a transaction, and use the aggregated ratings about a given party to derive a trust or reputation score, which can assist other parties in deciding whether or not to transact with that party in the future. A natural side effect is that it also provides an incentive for good behaviour, and therefore tends to have a positive effect on market quality. Reputation systems can be called collaborative sanctioning systems to reflect their collaborative nature, and are related to collaborative filtering systems. Reputation systems are already being used in successful commercial online applications. There is also a rapidly growing literature around trust and reputation systems, but unfortunately this activity is not very coherent. The purpose of this article is to give an overview of existing and proposed systems that can be used to derive measures of trust and reputation for Internet transactions, to analyse the current trends and developments in this area, and to propose a research agenda for trust and reputation systems.',\n",
       "  'authors': ['Audun Jøsang 1', ' Roslan Ismail 2', ' Colin Boyd 2'],\n",
       "  'date': '2007',\n",
       "  'identifier': '2097726984',\n",
       "  'references': ['1854214752',\n",
       "   '2156523427',\n",
       "   '2079041580',\n",
       "   '2337430557',\n",
       "   '2098685442',\n",
       "   '1601376565',\n",
       "   '1604936042',\n",
       "   '2090513801',\n",
       "   '2000368422',\n",
       "   '2110689325'],\n",
       "  'title': 'A survey of trust and reputation systems for online service provision'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Gary A. Kochenberger 1',\n",
       "   ' Bruce A. McCarl 2',\n",
       "   ' F. Paul Wyman 3'],\n",
       "  'date': '1974',\n",
       "  'identifier': '2022111106',\n",
       "  'references': ['1992428442', '2106433410', '2134857787', '2540171881'],\n",
       "  'title': 'A HEURISTIC FOR GENERAL INTEGER PROGRAMMING'},\n",
       " {'abstract': \"The Publication Manual of the American Psychological Association (American Psychological Association, 2001, American Psychological Association, 2010) calls for the reporting of effect sizes and their confidence intervals. Estimates of effect size are useful for determining the practical or theoretical importance of an effect, the relative contributions of factors, and the power of an analysis. We surveyed articles published in 2009 and 2010 in the Journal of Experimental Psychology: General, noting the statistical analyses reported and the associated reporting of effect size estimates. Effect sizes were reported for fewer than half of the analyses; no article reported a confidence interval for an effect size. The most often reported analysis was analysis of variance, and almost half of these reports were not accompanied by effect sizes. Partial η2 was the most commonly reported effect size estimate for analysis of variance. For t tests, 2/3 of the articles did not report an associated effect size estimate; Cohen's d was the most often reported. We provide a straightforward guide to understanding, selecting, calculating, and interpreting effect sizes for many types of data and to methods for calculating effect size confidence intervals and power analysis.\",\n",
       "  'authors': ['Catherine O. Fritz 1',\n",
       "   ' Peter E. Morris 1',\n",
       "   ' Jennifer J. Richler 2'],\n",
       "  'date': '2012',\n",
       "  'identifier': '2148540129',\n",
       "  'references': ['71228264',\n",
       "   '2897971917',\n",
       "   '2037124948',\n",
       "   '1484864026',\n",
       "   '1636536540',\n",
       "   '1999649023',\n",
       "   '2107031757',\n",
       "   '1948497384',\n",
       "   '2104818169',\n",
       "   '2030360178'],\n",
       "  'title': 'Effect size estimates: Current use, calculations, and interpretation.'},\n",
       " {'abstract': \"Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text, as is evidenced by their poor performance on domains like the Wall Street Journal, and by the movement away from parsing-based approaches to text-processing in general. In this paper, I describe SPATTER, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result. This work is based on the following premises: (1) grammars are too complex and detailed to develop manually for most interesting domains; (2) parsing models must rely heavily on lexical and contextual information to analyze sentences accurately; and (3) existing n-gram modeling techniques are inadequate for parsing models. In experiments comparing SPATTER with IBM's computer manuals parser, SPATTER significantly outperforms the grammar-based parser. Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures, SPATTER achieves 86% precision, 86% recall, and 1.3 crossing brackets per sentence for sentences of 40 words or less, and 91% precision, 90% recall, and 0.5 crossing brackets for sentences between 10 and 20 words in length.\",\n",
       "  'authors': ['David M. Magerman'],\n",
       "  'date': '1995',\n",
       "  'identifier': '2153439141',\n",
       "  'references': ['3085162807',\n",
       "   '1594031697',\n",
       "   '2121227244',\n",
       "   '2087165009',\n",
       "   '1575431606',\n",
       "   '2099345940',\n",
       "   '1924403233',\n",
       "   '1542847127'],\n",
       "  'title': 'Statistical Decision-Tree Models for Parsing'},\n",
       " {'abstract': 'Despite the apparent randomness of the Internet, we discover some surprisingly simple power-laws of the Internet topology. These power-laws hold for three snapshots of the Internet, between November 1997 and December 1998, despite a 45% growth of its size during that period. We show that our power-laws fit the real data very well resulting in correlation coefficients of 96% or higher.Our observations provide a novel perspective of the structure of the Internet. The power-laws describe concisely skewed distributions of graph properties such as the node outdegree. In addition, these power-laws can be used to estimate important parameters such as the average neighborhood size, and facilitate the design and the performance analysis of protocols. Furthermore, we can use them to generate and select realistic topologies for simulation purposes.',\n",
       "  'authors': ['Michalis Faloutsos ',\n",
       "   ' Petros Faloutsos ',\n",
       "   ' Christos Faloutsos'],\n",
       "  'date': '1999',\n",
       "  'identifier': '1976969221',\n",
       "  'references': ['2170120409',\n",
       "   '2078206416',\n",
       "   '2105818147',\n",
       "   '2148275477',\n",
       "   '2138543759',\n",
       "   '2408227189',\n",
       "   '2145721479',\n",
       "   '2171873915',\n",
       "   '2175892715',\n",
       "   '2128796442'],\n",
       "  'title': 'On power-law relationships of the Internet topology'},\n",
       " {'abstract': \"We report the results of a study of the reliability of anaphoric annotation which (i) involved a substantial number of naive subjects, (ii) used Krippendorff's α instead of K to measure agreement, as recently proposed by Passonneau, and (iii) allowed annotators to mark anaphoric expressions as ambiguous.\",\n",
       "  'authors': ['Massimo Poesio ', ' Ron Artstein'],\n",
       "  'date': '2005',\n",
       "  'identifier': '2082802837',\n",
       "  'references': ['1574901103',\n",
       "   '3021916629',\n",
       "   '1491947308',\n",
       "   '2002664886',\n",
       "   '1985463960',\n",
       "   '2102423300',\n",
       "   '1528859321',\n",
       "   '117665274',\n",
       "   '1980366261',\n",
       "   '2571812974'],\n",
       "  'title': 'The Reliability of Anaphoric Annotation, Reconsidered: Taking Ambiguity into Account'},\n",
       " {'abstract': 'Summary We examine the issue of population stratification in association-mapping studies. In case-control studies of association, population subdivision or recent admixture of populations can lead to spurious associations between a phenotype and unlinked candidate loci. Using a model of sampling from a structured population, we show that if population stratification exists, it can be detected by use of unlinked marker loci. We show that the case-control–study design, using unrelated control individuals, is a valid approach for association mapping, provided that marker loci unlinked to the candidate locus are included in the study, to test for stratification. We suggest guidelines as to the number of unlinked marker loci to use.',\n",
       "  'authors': ['Jonathan K. Pritchard 1', ' 2', ' Noah A. Rosenberg 2'],\n",
       "  'date': '1999',\n",
       "  'identifier': '2116329478',\n",
       "  'references': ['2042103448',\n",
       "   '2330519911',\n",
       "   '2105025553',\n",
       "   '1543868019',\n",
       "   '2123466824',\n",
       "   '1546828436',\n",
       "   '2169556112',\n",
       "   '1560012080',\n",
       "   '27851543',\n",
       "   '2112799266'],\n",
       "  'title': 'Use of unlinked genetic markers to detect population stratification in association studies.'},\n",
       " {'abstract': 'This paper demonstrates theoretically and empirically that a greedy algorithm called orthogonal matching pursuit (OMP) can reliably recover a signal with m nonzero entries in dimension d given O(m ln d) random linear measurements of that signal. This is a massive improvement over previous results, which require O(m2) measurements. The new results for OMP are comparable with recent results for another approach called basis pursuit (BP). In some settings, the OMP algorithm is faster and easier to implement, so it is an attractive alternative to BP for signal recovery problems.',\n",
       "  'authors': ['J.A. Tropp ', ' A.C. Gilbert'],\n",
       "  'date': '2007',\n",
       "  'identifier': '2127271355',\n",
       "  'references': ['2296319761',\n",
       "   '2296616510',\n",
       "   '2145096794',\n",
       "   '2129638195',\n",
       "   '2129131372',\n",
       "   '2063978378',\n",
       "   '2078204800',\n",
       "   '2798909945',\n",
       "   '2116148865',\n",
       "   '2151693816'],\n",
       "  'title': 'Signal Recovery From Random Measurements Via Orthogonal Matching Pursuit'},\n",
       " {'abstract': 'In nearest neighbor random walk on an n-dimensional cube a particle moves to one of its nearest neighbors (or stays fixed) with equal probability. the particle starts at 0. How long does it take to reach its stationary distribution? in fact, this occurs surprisingly rapidly. Previous analysis has shown that the total variation distance to stationarity is large if the number of steps N is 1/4n log n. This paper derives an explicit expression for the variation distance as n → ∞ in the transition region N ˜ 1/4n log n. This permits the first careful evaluation of a cutoff phenomenon observed in a wide variety of Markov chains. the argument involves Fourier analysis to express the probability as a contour integral and saddle point approximation. the asymptotic results are in good agreement with numerical results for n as small as 100.',\n",
       "  'authors': ['Persi Diaconis 1',\n",
       "   ' Ronald L. Graham 2',\n",
       "   ' John A. Morrison 2'],\n",
       "  'date': '1990',\n",
       "  'identifier': '2027808858',\n",
       "  'references': ['2120062331',\n",
       "   '2751862591',\n",
       "   '1556192255',\n",
       "   '2074599161',\n",
       "   '2138412601',\n",
       "   '2083120484',\n",
       "   '2018344665',\n",
       "   '1983403768',\n",
       "   '2001759372',\n",
       "   '2023642785'],\n",
       "  'title': 'Asymptotic analysis of a random walk on a hypercube with many dimensions'},\n",
       " {'abstract': \"An optimum system for transmitting a sample function from a stationary second order process over the BSC in real time is formulated under the mean-integral-square error criterion. The optimum number of channel bits and the optimum assignment of these bits to the samples is determined for the SGM process with R(\\\\tau) = \\\\exp(-|\\\\tau}) . The system error is evaluated and compared to the error performances of systems using time sampling with and without data compression. Shannon's information rate is also computed for these systems and compared to the theoretical limit and to Goblick's BSQC digitizer. We also present a waveform representation technique based on Hotelling's method of principal components. The waveform-to-vector converter consists of a time sampler followed by a digital filter, and the vector-to-waveform reconstructor consists of a digita filter followed by a linear interpolator. Performance is shown to be comparable to KL sampling.\",\n",
       "  'authors': ['P. Wintz ', ' A. Kurtenbach'],\n",
       "  'date': '1968',\n",
       "  'identifier': '2110380152',\n",
       "  'references': ['2109808436',\n",
       "   '2172074673',\n",
       "   '2129977719',\n",
       "   '2070865817',\n",
       "   '2087012806',\n",
       "   '2163624289',\n",
       "   '2071128523',\n",
       "   '2036949455',\n",
       "   '2122835713',\n",
       "   '1998472175'],\n",
       "  'title': 'Waveform error control in PCM telemetry'},\n",
       " {'abstract': 'SWITCHBOARD is a large multispeaker corpus of conversational speech and text which should be of interest to researchers in speaker authentication and large vocabulary speech recognition. About 2500 conversations by 500 speakers from around the US were collected automatically over T1 lines at Texas Instruments. Designed for training and testing of a variety of speech processing algorithms, especially in speaker verification, it has over an 1 h of speech from each of 50 speakers, and several minutes each from hundreds of others. A time-aligned word for word transcription accompanies each recording. >',\n",
       "  'authors': ['J.J. Godfrey ', ' E.C. Holliman ', ' J. McDaniel'],\n",
       "  'date': '1992',\n",
       "  'identifier': '2166637769',\n",
       "  'references': ['1643320849', '2077302143', '1976349544'],\n",
       "  'title': 'SWITCHBOARD: telephone speech corpus for research and development'},\n",
       " {'abstract': 'In this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an algorithm to track discourse context and bind pronouns. As described in [GJW86], the process of centering attention on entities in the discourse gives rise to the intersentential transitional states of continuing, retaining and shifting. We propose an extension to these states which handles some additional cases of multiple ambiguous pronouns. The algorithm has been implemented in an HPSG natural language system which serves as the interface to a database query application.',\n",
       "  'authors': ['Susan E. Brennan ',\n",
       "   ' Marilyn W. Friedman ',\n",
       "   ' Carl J. Pollard'],\n",
       "  'date': '1987',\n",
       "  'identifier': '2139243841',\n",
       "  'references': ['2167702024',\n",
       "   '1579095920',\n",
       "   '2146213370',\n",
       "   '2165046185',\n",
       "   '1963512417',\n",
       "   '2096979259',\n",
       "   '83905281',\n",
       "   '1606631862',\n",
       "   '2134984896'],\n",
       "  'title': 'A CENTERING APPROACH TO PRONOUNS'},\n",
       " {'abstract': \"Psychophysical functions describe the relationship between variations in the amplitude of a defined physical quantity and the psychological perception of these changes. Examples are brightness, loudness, and pain. The regularities of these relationships have been formulated into psychophysical laws. The measurement methodology of psychophysical scaling has been refined by the Harvard group led by S. S. Stevens, who proposed a power function as a general form for such laws. The main argument of the present article is that a similar scaling approach can be adapted to the measurement of workload and task demands based upon subjective estimates. The rationale is that these estimates, like other psychophysical judgments, reflect the individual's perception of the amount of processing resources that the subject invests to meet the demand imposed by a task. This approach was successfully applied to the assessment of 21 experimental conditions given to a group of 60 subjects. The paper discusses the main results ...\",\n",
       "  'authors': ['Daniel Gopher 1', ' Rolf Braune 2'],\n",
       "  'date': '1984',\n",
       "  'identifier': '1888386172',\n",
       "  'references': ['1987302113',\n",
       "   '2029654180',\n",
       "   '2517305112',\n",
       "   '1604337239',\n",
       "   '1642448760',\n",
       "   '1637563122',\n",
       "   '2101875783'],\n",
       "  'title': 'On the Psychophysics of Workload: Why Bother with Subjective Measures?'},\n",
       " {'abstract': 'Coreference systems are driven by syntactic, semantic, and discourse constraints. We present a simple approach which completely modularizes these three aspects. In contrast to much current work, which focuses on learning and on the discourse component, our system is deterministic and is driven entirely by syntactic and semantic compatibility as learned from a large, unlabeled corpus. Despite its simplicity and discourse naivete, our system substantially outperforms all unsupervised systems and most supervised ones. Primary contributions include (1) the presentation of a simple-to-reproduce, high-performing baseline and (2) the demonstration that most remaining errors can be attributed to syntactic and semantic factors external to the coreference phenomenon (and perhaps best addressed by non-coreference systems).',\n",
       "  'authors': ['Aria Haghighi ', ' Dan Klein'],\n",
       "  'date': '2009',\n",
       "  'identifier': '2139354869',\n",
       "  'references': ['2096765155',\n",
       "   '2097606805',\n",
       "   '1535015163',\n",
       "   '2092654472',\n",
       "   '2068737686',\n",
       "   '2164455818',\n",
       "   '2142086811',\n",
       "   '2149956050',\n",
       "   '2124741472',\n",
       "   '2050273484'],\n",
       "  'title': 'Simple Coreference Resolution with Rich Syntactic and Semantic Features'},\n",
       " {'abstract': 'In this paper, we present an approach to color image understanding that accounts for color variations due to highlights and shading. We demonstrate that the reflected light from every point on a dielectric object, such as plastic, can be described as a linear combination of the object color and the highlight color. The colors of all light rays reflected from one object then form a planar cluster in the color space. The shape of this cluster is determined by the object and highlight colors and by the object shape and illumination geometry. We present a method that exploits the difference between object color and highlight color to separate the color of every pixel into a matte component and a highlight component. This generates two intrinsic images, one showing the scene without highlights, and the other one showing only the highlights. The intrinsic images may be a useful tool for a variety of algorithms in computer vision, such as stereo vision, motion analysis, shape from shading, and shape from highlights. Our method combines the analysis of matte and highlight reflection with a sensor model that accounts for camera limitations. This enables us to successfully run our algorithm on real images taken in a laboratory setting. We show and discuss the results.',\n",
       "  'authors': ['Gudrun J. Klinker ', ' Steven A. Shafer ', ' Takeo Kanade'],\n",
       "  'date': '1992',\n",
       "  'identifier': '2119204143',\n",
       "  'references': ['2740373864',\n",
       "   '192761727',\n",
       "   '1530383550',\n",
       "   '1630728390',\n",
       "   '2415527960',\n",
       "   '2286014486',\n",
       "   '2010560725',\n",
       "   '2114162350',\n",
       "   '2075747175',\n",
       "   '1557590941'],\n",
       "  'title': 'The measurement of highlights in color images'},\n",
       " {'abstract': \"1. We studied the activity of single neurons in the monkey frontal eye fields during oculomotor tasks designed to assess the activity of these neurons when there was a dissonance between the spatial location of a target and its position on the retina. 2. Neurons with presaccadic activity were first studied to determine their receptive or movement fields and to classify them as visual, visuomovement, or movement cells with the use of the criteria described previously (Bruce and Goldberg 1985). The neurons were then studied by the use of double-step tasks that dissociated the retinal coordinates of visual targets from the dimensions of saccadic eye movements necessary to acquire those targets. These tasks required that the monkeys make two successive saccades to follow two sequentially flashed targets. Because the second target disappeared before the first saccade occurred, the dimensions of the second saccade could not be based solely on the retinal coordinates of the target but also depended on the dimensions of the first saccade. We used two versions of the double-step task. In one version neither target appeared in the cell's receptive or movement field, but the second eye movement was the optimum amplitude and direction for the cell (right-EM/wrong-RF task). In the other the second stimulus appeared in the cell's receptive field, but neither eye movement was appropriate for the cell (wrong-EM/right-RF task). 3. Most frontal-eye-field cells discharged in the right-EM/wrong-RF version of the double-step task. Their discharge began after the first saccade and continued until the second saccade was made. They usually discharged even on occasional trials in which the monkey failed to make the second saccade. They discharged much less, or not at all, in the wrong-EM/right-RF version of the double-step paradigm. Thus most presaccadic cells in the frontal eye fields were tuned to the dimensions of saccadic eye movements rather than to the coordinates of retinal stimulation. 4. Eleven movement cells (including 1 which also had independent postsaccadic activity for saccades opposite its presaccadic movement field) were studied, and all had significant activity in the right-EM/wrong-RF task. 5. Almost all (28/32) visuomovement cells, including 12 with independent postsaccadic activity, discharged in the right-EM/wrong-RF task. None of the four that failed had independent postsaccadic activity. 6. The majority (26/40) of visual cells were responsive in the right-EM/wrong-RF task.(ABSTRACT TRUNCATED AT 400 WORDS)\",\n",
       "  'authors': ['M. E. Goldberg ', ' C. J. Bruce'],\n",
       "  'date': '1990',\n",
       "  'identifier': '2269963436',\n",
       "  'references': ['2144200546',\n",
       "   '2123086804',\n",
       "   '2074854285',\n",
       "   '2042133853',\n",
       "   '2050605812',\n",
       "   '2099650267',\n",
       "   '2125132946',\n",
       "   '1966909626',\n",
       "   '1973176103',\n",
       "   '2038532558'],\n",
       "  'title': 'Primate frontal eye fields. III. Maintenance of a spatially accurate saccade signal'},\n",
       " {'abstract': 'Certain theoretical aspects of fuzzy decision trees and their applications are discussed. The main result is a branch-bound-backtrack algorithm which, by means of pruning subtrees unlikely to be traversed and installing tree-traversal pointers, has an effective backtracking mechanism leading to the optimal solution while still requiring usually only O(log n) time, where n is the number of decision classes.',\n",
       "  'authors': ['Robin L. P. Chang ', ' Theodosios Pavlidis'],\n",
       "  'date': '1977',\n",
       "  'identifier': '2077450947',\n",
       "  'references': ['2041280856',\n",
       "   '2799004609',\n",
       "   '2144931885',\n",
       "   '2045127810',\n",
       "   '2068373264',\n",
       "   '1979819178'],\n",
       "  'title': 'Fuzzy Decision Tree Algorithms'},\n",
       " {'abstract': 'Clustering is the division of data into groups of similar objects. In clustering, some details are disregarded in exchange for data simplification. Clustering can be viewed as a data modeling technique that provides for concise summaries of the data. Clustering is therefore related to many disciplines and plays an important role in a broad range of applications. The applications of clustering usually deal with large datasets and data with many attributes. Exploration of such data is a subject of data mining. This survey concentrates on clustering algorithms from a data mining perspective.',\n",
       "  'authors': ['Pavel Berkhin'],\n",
       "  'date': '2006',\n",
       "  'identifier': '1501500081',\n",
       "  'references': ['2140190241',\n",
       "   '2148694408',\n",
       "   '1639032689',\n",
       "   '2099111195',\n",
       "   '1679913846',\n",
       "   '1992419399',\n",
       "   '1673310716',\n",
       "   '2165874743',\n",
       "   '2156718197',\n",
       "   '2049633694'],\n",
       "  'title': 'A Survey of Clustering Data Mining Techniques'},\n",
       " {'abstract': 'Eye–hand coordination is complex because it involves the visual guidance of both the eyes and hands, while simultaneously using eye movements to optimize vision. Since only hand motion directly aff...',\n",
       "  'authors': ['J. D. Crawford ', ' W. P. Medendorp ', ' J. J. Marotta'],\n",
       "  'date': '2004',\n",
       "  'identifier': '2099650267',\n",
       "  'references': ['2109995759',\n",
       "   '2082627290',\n",
       "   '2152179757',\n",
       "   '2074854285',\n",
       "   '1926515346',\n",
       "   '1967533781',\n",
       "   '2074693532',\n",
       "   '1997392514',\n",
       "   '2117451014',\n",
       "   '1580261990'],\n",
       "  'title': 'Spatial Transformations for Eye–Hand Coordination'},\n",
       " {'abstract': 'This paper extends previous results about the classical information capacity of a noiseless quantum-mechanical communication channel to situations in which the final signal states are mixed states, that is, to channels with noise.',\n",
       "  'authors': ['Benjamin Schumacher 1', ' Michael D. Westmoreland 2'],\n",
       "  'date': '1997',\n",
       "  'identifier': '2001619599',\n",
       "  'references': ['1970948459',\n",
       "   '1574861711',\n",
       "   '245337057',\n",
       "   '2804172638',\n",
       "   '2154564336',\n",
       "   '2141412822',\n",
       "   '1785526227',\n",
       "   '2808319042',\n",
       "   '3103744872'],\n",
       "  'title': 'Sending classical information via noisy quantum channels'},\n",
       " {'abstract': 'In this paper, we provide a review of the different approaches used for target decomposition theory in radar polarimetry. We classify three main types of theorem; those based on the Mueller matrix and Stokes vector, those using an eigenvector analysis of the covariance or coherency matrix, and those employing coherent decomposition of the scattering matrix. We unify the formulation of these different approaches using transformation theory and an eigenvector analysis. We show how special forms of these decompositions apply for the important case of backscatter from terrain with generic symmetries.',\n",
       "  'authors': ['S.R. Cloude ', ' E. Pottier'],\n",
       "  'date': '1996',\n",
       "  'identifier': '2078985447',\n",
       "  'references': ['2152577388',\n",
       "   '1666853924',\n",
       "   '2105781415',\n",
       "   '2152269371',\n",
       "   '2142810222',\n",
       "   '2199513852',\n",
       "   '1986603680',\n",
       "   '2153148066',\n",
       "   '2044781968',\n",
       "   '2089940122'],\n",
       "  'title': 'A review of target decomposition theorems in radar polarimetry'},\n",
       " {'abstract': \"1. An oculomotor delayed-response task was used to examine the spatial memory functions of neurons in primate prefrontal cortex. Monkeys were trained to fixate a central spot during a brief presentation (0.5 s) of a peripheral cue and throughout a subsequent delay period (1-6 s), and then, upon the extinction of the fixation target, to make a saccadic eye movement to where the cue had been presented. Cues were usually presented in one of eight different locations separated by 45 degrees. This task thus requires monkeys to direct their gaze to the location of a remembered visual cue, controls the retinal coordinates of the visual cues, controls the monkey's oculomotor behavior during the delay period, and also allows precise measurement of the timing and direction of the relevant behavioral responses. 2. Recordings were obtained from 288 neurons in the prefrontal cortex within and surrounding the principal sulcus (PS) while monkeys performed this task. An additional 31 neurons in the frontal eye fields (FEF) region within and near the anterior bank of the arcuate sulcus were also studied. 3. Of the 288 PS neurons, 170 exhibited task-related activity during at least one phase of this task and, of these, 87 showed significant excitation or inhibition of activity during the delay period relative to activity during the intertrial interval. 4. Delay period activity was classified as directional for 79% of these 87 neurons in that significant responses only occurred following cues located over a certain range of visual field directions and were weak or absent for other cue directions. The remaining 21% were omnidirectional, i.e., showed comparable delay period activity for all visual field locations tested. Directional preferences, or lack thereof, were maintained across different delay intervals (1-6 s). 5. For 50 of the 87 PS neurons, activity during the delay period was significantly elevated above the neuron's spontaneous rate for at least one cue location; for the remaining 37 neurons only inhibitory delay period activity was seen. Nearly all (92%) neurons with excitatory delay period activity were directional and few (8%) were omnidirectional. Most (62%) neurons with purely inhibitory delay period activity were directional, but a substantial minority (38%) was omnidirectional. 6. Fifteen of the neurons with excitatory directional delay period activity also had significant inhibitory delay period activity for other cue directions. These inhibitory responses were usually strongest for, or centered about, cue directions roughly opposite those optimal for excitatory responses.(ABSTRACT TRUNCATED AT 400 WORDS)\",\n",
       "  'authors': ['Shintaro Funahashi ',\n",
       "   ' Charles J. Bruce ',\n",
       "   ' Patricia S. Goldman-Rakic'],\n",
       "  'date': '1989',\n",
       "  'identifier': '1939596644',\n",
       "  'references': ['597766098',\n",
       "   '1957364793',\n",
       "   '2093642963',\n",
       "   '2118622321',\n",
       "   '2031678850',\n",
       "   '1806855318',\n",
       "   '1914270110',\n",
       "   '2158601670',\n",
       "   '1999014668',\n",
       "   '2076139053'],\n",
       "  'title': \"Mnemonic coding of visual space in the monkey's dorsolateral prefrontal cortex\"},\n",
       " {'abstract': 'Subjectivity is a pragmatic, sentence-level feature that has important implications for text processing applications such as information extraction and information retrieval. We study the effects of dynamic adjectives, semantically oriented adjectives, and gradable adjectives on a simple subjectivity classifier, and establish that they are strong predictors of subjectivity. A novel trainable method that statistically combines two indicators of gradability is presented and evaluated, complementing existing automatic techniques for assigning orientation labels.',\n",
       "  'authors': ['Vasileios Hatzivassiloglou 1', ' Janyce M. Wiebe 2'],\n",
       "  'date': '2000',\n",
       "  'identifier': '1998442272',\n",
       "  'references': ['1632114991',\n",
       "   '2313581450',\n",
       "   '2199803028',\n",
       "   '2099247782',\n",
       "   '2070779353',\n",
       "   '2150098611',\n",
       "   '2062837929',\n",
       "   '1548013757',\n",
       "   '2138437366',\n",
       "   '2128669672'],\n",
       "  'title': 'Effects of adjective orientation and gradability on sentence subjectivity'},\n",
       " {'abstract': 'Agile software development represents a major departure from traditional, plan-based approaches to software engineering. A systematic review of empirical studies of agile software development up to and including 2005 was conducted. The search strategy identified 1996 studies, of which 36 were identified as empirical studies. The studies were grouped into four themes: introduction and adoption, human and social factors, perceptions on agile methods, and comparative studies. The review investigates what is currently known about the benefits and limitations of, and the strength of evidence for, agile methods. Implications for research and practice are presented. The main implication for research is a need for more and better empirical studies of agile software development within a common research agenda. For the industrial readership, the review provides a map of findings, according to topic, that can be compared for relevance to their own settings and situations.',\n",
       "  'authors': ['Tore Dybå ', ' Torgeir Dingsøyr'],\n",
       "  'date': '2008',\n",
       "  'identifier': '2168811232',\n",
       "  'references': ['1780382453',\n",
       "   '1556808170',\n",
       "   '1598602811',\n",
       "   '1493688518',\n",
       "   '1247968195',\n",
       "   '1730782591',\n",
       "   '2164777277',\n",
       "   '3022607203',\n",
       "   '1602304209',\n",
       "   '1562934601'],\n",
       "  'title': 'Empirical studies of agile software development: A systematic review'},\n",
       " {'abstract': '',\n",
       "  'authors': ['D. F. Roberts ', ' L. Luca Cavalli-Sforza'],\n",
       "  'date': '1996',\n",
       "  'identifier': '2330519911',\n",
       "  'references': ['2157752701',\n",
       "   '2152664025',\n",
       "   '2058401000',\n",
       "   '2108169091',\n",
       "   '2008047653',\n",
       "   '2112848098',\n",
       "   '2085876742',\n",
       "   '1793793548',\n",
       "   '2007807048',\n",
       "   '2166214412'],\n",
       "  'title': 'The History and Geography of Human Genes.'},\n",
       " {'abstract': 'A number of methods have been developed to classify ground terrain types from fully polarimetric synthetic aperture radar (SAR) images, and these techniques are often grouped into supervised and unsupervised approaches. Supervised methods have yielded higher accuracy than unsupervised techniques, but suffer from the need for human interaction to determine classes and training regions. In contrast, unsupervised methods determine classes automatically, but generally show limited ability to accurately divide terrain into natural classes. In this paper, a new terrain classification technique is introduced to determine terrain classes in polarimetric SAR images, utilizing unsupervised neural networks to provide automatic classification, and employing an iterative algorithm to improve the performance. Several types of unsupervised neural networks are first applied to the classification of SAR images, and the results are compared to those of more conventional unsupervised methods. Results show that one neural network method-Learning Vector Quantization (LVQ)-outperforms the conventional unsupervised classifiers, but is still inferior to supervised methods. To overcome this poor accuracy, an iterative algorithm is proposed where the SAR image is reclassified using a maximum likelihood (ML) classifier. It is shown that this algorithm converges, and significantly improves classification accuracy. >',\n",
       "  'authors': ['Y. Hara 1',\n",
       "   ' R.G. Atkins 1',\n",
       "   ' S.H. Yueh 2',\n",
       "   ' R.T. Shin 1',\n",
       "   ' J.A. Kong 1'],\n",
       "  'date': '1994',\n",
       "  'identifier': '2166960586',\n",
       "  'references': ['2152477898',\n",
       "   '2292279516',\n",
       "   '2144841545',\n",
       "   '2115226769',\n",
       "   '1518186130',\n",
       "   '1588783521',\n",
       "   '2155622525',\n",
       "   '2147439738',\n",
       "   '3015384695',\n",
       "   '2135768204'],\n",
       "  'title': 'Application of neural networks to radar image classification'},\n",
       " {'abstract': 'Responses toS_ (\"errors\") arenota necessary condition fortheformation ofan operant discrimination ofcolor. Errors donotoccurifdiscrimination training begins earlyinconditioning andifS+andS_ initially differ withrespect tobrightness, (luration andwavelength. Aftertraining starts, S-\\'sduration andbrightness isprogressively increased untilS+andSdiffer onlywithrespect towavelength. Errors dooccuriftraining starts after muchconditioninginthepresence ofS+hasoccurred orifS+andS_ differ onlywithrespect towavelength throughout training. Performance following discrimination learning without errors lacks three characteristics thatarefoundfollowing learning witherrors. Onlythosebirdsthatlearned thediscrimination witherrors showed(1)\"emotional\" responses inthepresence ofS-, (2)anincrease intherate(oradecrease inthelatency) ofitsresponse toS+,and(3)occasional bursts ofresponses toS-. Theacquisition ofanoperant discriminationmaybedefined astheprocess whereby an organism comestorespond morefrequently toa stimulus correlated withreinforcement (S+)thantoastimulus correlated withnonreinforcement (S-).Inpopularterminology, responses madetoS+ are\"correct responses\"',\n",
       "  'authors': ['H. S. Terrace'],\n",
       "  'date': '1963',\n",
       "  'identifier': '2103170504',\n",
       "  'references': ['1996847178',\n",
       "   '2091144676',\n",
       "   '2036824067',\n",
       "   '2090761728',\n",
       "   '2431473142',\n",
       "   '2082708185',\n",
       "   '2027945421',\n",
       "   '2058857604',\n",
       "   '2066156272',\n",
       "   '2021365331'],\n",
       "  'title': 'Discrimination learning with and without \"errors\".'},\n",
       " {'abstract': 'Rank/Select dictionaries are data structures for an ordered set S ⊂ {0,1, . . ., n − 1} to compute rank(x, S) (the number of elements in S that are no greater than x), and select(i, S) (the i-th smallest element in S), which are the fundamental components of succinct data structures of strings, trees, graphs, etc.. In these data structures, however, only asymptotic behavior has been considered and their performance for real data is not satisfactory. In this paper, we propose four novel Rank/Select dictionaries: esp, recrank, vcode and sdarray, each of which is small if the number of elements in S is small, and indeed close to nH0(S) (H0(S) ≤ 1 is the zero-th order empirical entropy of S) in practice. Furthermore, their query times are superior to those of existing structures. Experimental results reveal the characteristics of our data structures and also show that these data structures are superior to existing implementations, both in terms of size and query time.',\n",
       "  'authors': ['Daisuke Okanohara 1', ' Kunihiko Sadakane 2'],\n",
       "  'date': '2007',\n",
       "  'identifier': '1949346071',\n",
       "  'references': ['2159647614',\n",
       "   '2134696992',\n",
       "   '1985174631',\n",
       "   '2148113067',\n",
       "   '2046038806',\n",
       "   '2149243190',\n",
       "   '2114790712',\n",
       "   '1986970296',\n",
       "   '1975454935',\n",
       "   '2061916218'],\n",
       "  'title': 'Practical entropy-compressed rank/select dictionary'},\n",
       " {'abstract': '',\n",
       "  'authors': ['William T. Reeves'],\n",
       "  'date': '1998',\n",
       "  'identifier': '2346323361',\n",
       "  'references': ['2117595234',\n",
       "   '2113634802',\n",
       "   '2158037930',\n",
       "   '2149774527',\n",
       "   '2135844327',\n",
       "   '1555079265',\n",
       "   '1906058103',\n",
       "   '2135314751',\n",
       "   '2152363438',\n",
       "   '2098210391'],\n",
       "  'title': 'Particle systems—a technique for modeling a class of fuzzy objects'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Ani Nenkova ', ' Rebecca J. Passonneau'],\n",
       "  'date': '2004',\n",
       "  'identifier': '2102065370',\n",
       "  'references': ['2101105183',\n",
       "   '2150824314',\n",
       "   '3021916629',\n",
       "   '2138180048',\n",
       "   '1977747299',\n",
       "   '2160204597',\n",
       "   '2137591918',\n",
       "   '1985463960',\n",
       "   '2169401582',\n",
       "   '2795696357'],\n",
       "  'title': 'Evaluating Content Selection in Summarization: The Pyramid Method'},\n",
       " {'abstract': 'Abstract The (maximum) penalized-likelihood method of probability density estimation and bump-hunting is improved and exemplified by applications to scattering and chondrite data. We show how the hyperparameter in the method can be satisfactorily estimated by using statistics of goodness of fit. A Fourier expansion is found to be usually more expeditious than a Hermite expansion but a compromise is useful. The best fit to the scattering data has 13 bumps, all of which are evaluated by the Bayesian interpretation of the method. Eight bumps are well supported. The result for the chondrite data suggests that it is trimodal and confirms that there are (at least) three kinds of chondrite.',\n",
       "  'authors': ['I. J. Good 1', ' R. A. Gaskins 2'],\n",
       "  'date': '1980',\n",
       "  'identifier': '2167641446',\n",
       "  'references': ['2319794630',\n",
       "   '2905505698',\n",
       "   '1976611840',\n",
       "   '2135479785',\n",
       "   '2033135151',\n",
       "   '2089611415',\n",
       "   '2605948046',\n",
       "   '58740540',\n",
       "   '1533179050',\n",
       "   '2510099899'],\n",
       "  'title': 'Density Estimation and Bump-Hunting by the Penalized Likelihood Method Exemplified by Scattering and Meteorite Data'},\n",
       " {'abstract': 'A means to adapt the classical architecture of a Viterbi decoder to make it able to provide soft (weighted) decisions is presented. After a theoretical justification of the proposed method, based on Battail or Hagenauer-Hoeher algorithms, the new architecture is detailed. It leads to a real-time circuit, the size of which is roughly twice the size of the classical Viterbi decoder. In order to appreciate the quality of the weighting method, an application to the decoding of concatenated convolutional codes, with the proposed soft-output decoder as the inner decoder, is examined. >',\n",
       "  'authors': ['C. Berrou ', ' P. Adde ', ' E. Angui ', ' S. Faudeil'],\n",
       "  'date': '1993',\n",
       "  'identifier': '2154185977',\n",
       "  'references': ['2119411189', '2142384583', '1562979145', '2045407304'],\n",
       "  'title': 'A low complexity soft-output Viterbi decoder architecture'},\n",
       " {'abstract': 'The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above R_{0} , the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms.',\n",
       "  'authors': ['A. Viterbi'],\n",
       "  'date': '1967',\n",
       "  'identifier': '1991133427',\n",
       "  'references': ['2034274945',\n",
       "   '1993944611',\n",
       "   '2087362480',\n",
       "   '2005530146',\n",
       "   '1976797517',\n",
       "   '1527268325',\n",
       "   '1527096151'],\n",
       "  'title': 'Error bounds for convolutional codes and an asymptotically optimum decoding algorithm'},\n",
       " {'abstract': 'The success of the von Neumann model of sequential computation is attributable to the fact that it is an efficient bridge between software and hardware: high-level languages can be efficiently compiled on to this model; yet it can be effeciently implemented in hardware. The author argues that an analogous bridge between software and hardware in required for parallel computation if that is to become as widely used. This article introduces the bulk-synchronous parallel (BSP) model as a candidate for this role, and gives results quantifying its efficiency both in implementing high-level language features and algorithms, as well as in being implemented in hardware.',\n",
       "  'authors': ['Leslie G. Valiant'],\n",
       "  'date': '1990',\n",
       "  'identifier': '2045271686',\n",
       "  'references': ['2052207834',\n",
       "   '2143462372',\n",
       "   '1555673550',\n",
       "   '1989582918',\n",
       "   '1969008575',\n",
       "   '2107997203',\n",
       "   '2137239103',\n",
       "   '2069489095',\n",
       "   '2103012681',\n",
       "   '1544480906'],\n",
       "  'title': 'A bridging model for parallel computation'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Stephane G. Mallat'],\n",
       "  'date': '1989',\n",
       "  'identifier': '1980149518',\n",
       "  'references': ['2098914003',\n",
       "   '2096684483',\n",
       "   '2013987111',\n",
       "   '1975474302',\n",
       "   '2030464621',\n",
       "   '3021168970',\n",
       "   '2047340603',\n",
       "   '628476551',\n",
       "   '2899605173'],\n",
       "  'title': 'Multiresolution approximations and wavelet orthonormal bases of L^2(R)'},\n",
       " {'abstract': 'We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested.',\n",
       "  'authors': ['Marti A. Hearst'],\n",
       "  'date': '1992',\n",
       "  'identifier': '2068737686',\n",
       "  'references': ['2102381086',\n",
       "   '2046224275',\n",
       "   '1710422233',\n",
       "   '2123084125',\n",
       "   '2038585482',\n",
       "   '2162861726',\n",
       "   '2127009519',\n",
       "   '1973021928',\n",
       "   '1794966349',\n",
       "   '2108470480'],\n",
       "  'title': 'Automatic acquisition of hyponyms from large text corpora'},\n",
       " {'abstract': \"Csiszr and Krner's book is widely regarded as a classic in the field of information theory, providing deep insights and expert treatment of the key theoretical issues. It includes in-depth coverage of the mathematics of reliable information transmission, both in two-terminal and multi-terminal network scenarios. Updated and considerably expanded, this new edition presents unique discussions of information theoretic secrecy and of zero-error information theory, including the deep connections of the latter with extremal combinatorics. The presentations of all core subjects are self contained, even the advanced topics, which helps readers to understand the important connections between seemingly different problems. Finally, 320 end-of-chapter problems, together with helpful solving hints, allow readers to develop a full command of the mathematical techniques. It is an ideal resource for graduate students and researchers in electrical and electronic engineering, computer science and applied mathematics.\",\n",
       "  'authors': ['Imre Csiszar 1', ' Janos Korner 2'],\n",
       "  'date': '2011',\n",
       "  'identifier': '1549664537',\n",
       "  'references': ['1667950888',\n",
       "   '2133475491',\n",
       "   '2098567664',\n",
       "   '2147942702',\n",
       "   '2102617152',\n",
       "   '2107689535',\n",
       "   '2098257210',\n",
       "   '2072184935',\n",
       "   '2120085609',\n",
       "   '627952176'],\n",
       "  'title': 'Information Theory: Coding Theorems for Discrete Memoryless Systems'},\n",
       " {'abstract': '',\n",
       "  'authors': ['L. Baum'],\n",
       "  'date': '1972',\n",
       "  'identifier': '1575431606',\n",
       "  'references': ['2125838338',\n",
       "   '2156985047',\n",
       "   '2049633694',\n",
       "   '2006969979',\n",
       "   '1916559533',\n",
       "   '2166501262',\n",
       "   '2158195707',\n",
       "   '2121227244',\n",
       "   '2146871184'],\n",
       "  'title': 'An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process'},\n",
       " {'abstract': 'Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97% adversarial success rate while only modifying on average 4.02% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.',\n",
       "  'authors': ['Nicolas Papernot 1',\n",
       "   ' Patrick McDaniel 1',\n",
       "   ' Somesh Jha 2',\n",
       "   ' Matt Fredrikson 2',\n",
       "   ' Z. Berkay Celik 1',\n",
       "   ' Ananthram Swami 3'],\n",
       "  'date': '2016',\n",
       "  'identifier': '2180612164',\n",
       "  'references': ['2618530766',\n",
       "   '2097117768',\n",
       "   '2099471712',\n",
       "   '2136922672',\n",
       "   '2310919327',\n",
       "   '2963207607',\n",
       "   '2964153729',\n",
       "   '2072128103',\n",
       "   '2145287260',\n",
       "   '2117130368'],\n",
       "  'title': 'The Limitations of Deep Learning in Adversarial Settings'},\n",
       " {'abstract': 'with am a minimizing direction [or as is demonstrated easily a maximizing direction for E(gm)2]. Huber establishes weak L2(P) convergence of the procedure [rm -O 0 weakly in L2(P)]. Also in the Comments of [2] Donoho and Johnstone announce a proof of strong convergence for P uniforn on the unit ball or multivariate Gaussian. Huber mentions that mild smoothness assumptions are necessary to ensure the existence of a minimizing direction. To avoid this complication and also generalize the procedure we shall allow any direction at stage m to be chosen as long as',\n",
       "  'authors': ['Lee K. Jones'],\n",
       "  'date': '1987',\n",
       "  'identifier': '2033519565',\n",
       "  'references': [],\n",
       "  'title': 'On a conjecture of Huber concerning the convergence of projection pursuit regression'},\n",
       " {'abstract': '\"Analysis of Visual Behavior\" encompasses both theoretical and experimental research. It deals with the visual mechanisms of diverse vertebrate species from salamanders and toads to primates and humans and presents a stimulating interaction of the disciplines of anatomy, physiology, and behavioral science. Throughout, visual mechanisms are investigated from the point of view of the brain functioning at the organismic level, as opposed to the now more prevalent focus on the molecular and cellular levels. This approach allows researchers to deal with the patterns of visually guided behavior of animals in real-life situations.The twenty-six contributions in the book are divided among three sections: \"Indentification and Localization Processes in Nonmammalian Vertebrates,\" introduced by David J. Ingle; \"Visual Guidance of Motor Patterns: The Role of Visual Cortex and the Superior Colliculus,\" introduced by Melvyn A. Goodale; and \"Recognition and Transfer Processes,\" introduced by Richard J. W. Mansfield.The editors are all university researchers in psychology: David J. Ingle at Brandeis, Melvyn A. Goodale at the University of Western Ontario, and Richard J. W. Mansfield at Harvard.',\n",
       "  'authors': ['David Ingle ',\n",
       "   ' Melvyn A. Goodale ',\n",
       "   ' Richard J. W. Mansfield'],\n",
       "  'date': '1982',\n",
       "  'identifier': '576881103',\n",
       "  'references': [],\n",
       "  'title': 'Analysis of visual behavior'},\n",
       " {'abstract': 'We identify and validate from a large corpus constraints from conjunctions on the positive or negative semantic orientation of the conjoined adjectives. A log-linear regression model uses these constraints to predict whether conjoined adjectives are of same or different orientations, achieving 82% accuracy in this task when each conjunction is considered independently. Combining the constraints across many adjectives, a clustering algorithm separates the adjectives into groups of different orientations, and finally, adjectives are labeled positive or negative. Evaluations on real data and simulation experiments indicate high levels of performance: classification precision is more than 90% for adjectives that occur in a modest number of conjunctions in the corpus.',\n",
       "  'authors': ['Vasileios Hatzivassiloglou ', ' Kathleen R. McKeown'],\n",
       "  'date': '1997',\n",
       "  'identifier': '2199803028',\n",
       "  'references': ['2011039300',\n",
       "   '3015463134',\n",
       "   '1528905581',\n",
       "   '2102381086',\n",
       "   '3017143921',\n",
       "   '2121227244',\n",
       "   '2099247782',\n",
       "   '1987971958',\n",
       "   '2127314673',\n",
       "   '103650626'],\n",
       "  'title': 'Predicting the Semantic Orientation of Adjectives'},\n",
       " {'abstract': 'Enabling a computer to understand a document so that it can answer comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding its solution by machine learned systems is the limited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task. In this paper, we conduct a thorough examination of this new reading comprehension task. Our primary aim is to understand what depth of language understanding is required to do well on this task. We approach this from one side by doing a careful hand-analysis of a small subset of the problems and from the other by showing that simple, carefully designed systems can obtain accuracies of 72.4% and 75.8% on these two datasets, exceeding current state-of-the-art results by over 5% and approaching what we believe is the ceiling for performance on this task.1',\n",
       "  'authors': ['Danqi Chen 1', ' Jason Bolton 2', ' Christopher D. Manning 2'],\n",
       "  'date': '2016',\n",
       "  'identifier': '2962809918',\n",
       "  'references': [],\n",
       "  'title': 'A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task'},\n",
       " {'abstract': \"Simulated annealing (SA) presents an optimization technique with several striking positive and negative features. Perhaps its most salient feature, statistically promising to deliver an optimal solution, in current practice is often spurned to use instead modified faster algorithms, ''simulated quenching'' (SQ). Using the author's Adaptive Simulated Annealing (ASA) code, some examples are given which demonstrate how SQ can be much faster than SA without sacrificing accuracy.\",\n",
       "  'authors': ['L. Ingber'],\n",
       "  'date': '1993',\n",
       "  'identifier': '2040049280',\n",
       "  'references': ['1497256448',\n",
       "   '2581275558',\n",
       "   '1997063559',\n",
       "   '2133671888',\n",
       "   '1507849272',\n",
       "   '2171776966',\n",
       "   '2914275007',\n",
       "   '2154061444',\n",
       "   '2157350782',\n",
       "   '2059448777'],\n",
       "  'title': 'Simulated annealing: Practice versus theory'},\n",
       " {'abstract': 'The issue of how to support situation awareness among operators of complex systems or vehicles is a growing concern in a number of industries, especially when automation takes the operators partly ...',\n",
       "  'authors': ['Marilyn Jager Adams ', ' Yvette J. Tenney ', ' Richard W. Pew'],\n",
       "  'date': '1995',\n",
       "  'identifier': '2024378172',\n",
       "  'references': ['1570321471',\n",
       "   '1996728935',\n",
       "   '2020194695',\n",
       "   '1982888377',\n",
       "   '2029173882',\n",
       "   '2155156038',\n",
       "   '1999999075',\n",
       "   '1884489261',\n",
       "   '2015086821',\n",
       "   '2142403080'],\n",
       "  'title': 'Situation Awareness and the Cognitive Management of Complex Systems'},\n",
       " {'abstract': 'The modern science of networks has brought significant advances to our understanding of complex systems. One of the most relevant features of graphs representing real systems is community structure, or clustering, i.e. the organization of vertices in clusters, with many edges joining vertices of the same cluster and comparatively few edges joining vertices of different clusters. Such clusters, or communities, can be considered as fairly independent compartments of a graph, playing a similar role like, e.g., the tissues or the organs in the human body. Detecting communities is of great importance in sociology, biology and computer science, disciplines where systems are often represented as graphs. This problem is very hard and not yet satisfactorily solved, despite the huge effort of a large interdisciplinary community of scientists working on it over the past few years. We will attempt a thorough exposition of the topic, from the definition of the main elements of the problem, to the presentation of most methods developed, with a special focus on techniques designed by statistical physicists, from the discussion of crucial issues like the significance of clustering and how methods should be tested and compared against each other, to the description of applications to real networks.',\n",
       "  'authors': ['Santo Fortunato'],\n",
       "  'date': '2010',\n",
       "  'identifier': '3102641634',\n",
       "  'references': ['2331432542',\n",
       "   '2112090702',\n",
       "   '2008620264',\n",
       "   '1497256448',\n",
       "   '2148606196',\n",
       "   '2124637492',\n",
       "   '2009435671',\n",
       "   '3013264884',\n",
       "   '2103017472',\n",
       "   '1480376833'],\n",
       "  'title': 'Community detection in graphs'},\n",
       " {'abstract': 'The Basics of Target Tracking. Sensor and Source Characteristics. Kinematic State Estimation: Filtering and Prediction. Modelling and Tracking Dynamic Targets. Passive Sensor Tracking. Basic Methods for Data Association. Advanced Methods for MTT Data Association. Attribute Data Fusion. Multiple Sensor Tracking -- Issues and Methods. Multiple Sensor Tracking -- System Implementation and Applications. Reasoning Schemes for Situation Assessment and Sensor Management. Situation Assessment. Tracking System Performance Prediction, and Evaluation. Multi Target Tracking with an Agile Beam Radar. Sensor Management. Multiple Hypothesis Tracking System Design and Application. Detection and Tracking of Dim Targets in Clutter.',\n",
       "  'authors': ['Samuel S. Blackman ', ' Robert Populi'],\n",
       "  'date': '1999',\n",
       "  'identifier': '1568122762',\n",
       "  'references': [],\n",
       "  'title': 'Design and Analysis of Modern Tracking Systems'},\n",
       " {'abstract': \"ncovering the structure and function of communication networks has always been constrained by the practical difficulty of mapping out interactions among a large number of individuals. Indeed, most of our current understanding of com- munication and social networks is based on questionnaire data, reaching typically a few dozen individuals and relying on the individual's opinion to reveal the nature and the strength of the ties. The fact that currently an increasing fraction of human interactions are recorded, from e-mail (1-3) to phone records (4), offers unprecedented opportunities to uncover and explore the large scale characteristics of communication and social networks (5). Here we take a first step in this direction by exploiting the widespread use of mobile phones to construct a map of a society-wide communication network, capturing the mobile interaction patterns of millions of individuals. The data set allows us to explore the relationship between the topology of the network and the tie strengths between individuals, informa- tion that was inaccessible at the societal level before. We demonstrate a local coupling between tie strengths and network topology, and show that this coupling has important conse- quences for the network's global stability if ties are removed, as well as for the spread of news and ideas within the network. A significant portion of a country's communication network wasreconstructedfrom18weeksofallmobilephonecallrecords among 20% of the country's entire population, 90% of whose\",\n",
       "  'authors': ['J.-P. Onnela ',\n",
       "   ' J. Saramäki ',\n",
       "   ' J. Hyvönen ',\n",
       "   ' G. Szabó ',\n",
       "   ' D. Lazer ',\n",
       "   ' K. Kaski ',\n",
       "   ' J. Kertész ',\n",
       "   ' A.-L. Barabási'],\n",
       "  'date': '2007',\n",
       "  'identifier': '2141113219',\n",
       "  'references': ['2331432542',\n",
       "   '2008620264',\n",
       "   '1971421925',\n",
       "   '2070722739',\n",
       "   '2065769502',\n",
       "   '2061901927',\n",
       "   '2164928285',\n",
       "   '2038195874',\n",
       "   '3113109455',\n",
       "   '2136931666'],\n",
       "  'title': 'Structure and tie strengths in mobile communication networks'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Lotfi A. Zadeh'],\n",
       "  'date': '1996',\n",
       "  'identifier': '2912565176',\n",
       "  'references': ['2140190241',\n",
       "   '1992419399',\n",
       "   '2153233077',\n",
       "   '2186428165',\n",
       "   '2038420319',\n",
       "   '2143451122',\n",
       "   '2342408547',\n",
       "   '2151498684',\n",
       "   '2061240006'],\n",
       "  'title': 'Fuzzy sets'},\n",
       " {'abstract': 'In an earlier study by Julesz (1962) pairs of random textures were generated side-by-side using a Markov process with different third-order joint-probability distributions but identical first- and second-order distributions. Such texture pairs could not be discriminated from each other by the human visual system without scrutiny. Unfortunately, Markov processes are inherently one-dimensional while the general processes underlying visual texture discrimination are two-dimensional. Here three new methods are introduced that generate two-dimensional non-Markovian textures with different third-order but identical first- and second-order statistics. All three methods generate texture pairs that cannot be discriminated from each other.The lack of texture discrimination is the more astonishing since the individual elements that form the texture pair are clearly perceived as being very different. However, a counterexample was found that yields discrimination although the texture pair has approximately identical s...',\n",
       "  'authors': ['B Julesz 1', ' E N Gilbert 1', ' L A Shepp 1', ' H L Frisch 2'],\n",
       "  'date': '1973',\n",
       "  'identifier': '2066548055',\n",
       "  'references': [],\n",
       "  'title': 'Inability of Humans to Discriminate between Visual Textures That Agree in Second-Order Statistics—Revisited'},\n",
       " {'abstract': 'This paper introduces a novel algorithm to approximate the matrix with minimum nuclear norm among all matrices obeying a set of convex constraints. This problem may be understood as the convex relaxation of a rank minimization problem and arises in many important applications as in the task of recovering a large matrix from a small subset of its entries (the famous Netflix problem). Off-the-shelf algorithms such as interior point methods are not directly amenable to large problems of this kind with over a million unknown entries. This paper develops a simple first-order and easy-to-implement algorithm that is extremely efficient at addressing problems in which the optimal solution has low rank. The algorithm is iterative, produces a sequence of matrices $\\\\{\\\\boldsymbol{X}^k,\\\\boldsymbol{Y}^k\\\\}$, and at each step mainly performs a soft-thresholding operation on the singular values of the matrix $\\\\boldsymbol{Y}^k$. There are two remarkable features making this attractive for low-rank matrix completion problems. The first is that the soft-thresholding operation is applied to a sparse matrix; the second is that the rank of the iterates $\\\\{\\\\boldsymbol{X}^k\\\\}$ is empirically nondecreasing. Both these facts allow the algorithm to make use of very minimal storage space and keep the computational cost of each iteration low. On the theoretical side, we provide a convergence analysis showing that the sequence of iterates converges. On the practical side, we provide numerical examples in which $1,000\\\\times1,000$ matrices are recovered in less than a minute on a modest desktop computer. We also demonstrate that our approach is amenable to very large scale problems by recovering matrices of rank about 10 with nearly a billion unknowns from just about 0.4% of their sampled entries. Our methods are connected with the recent literature on linearized Bregman iterations for $\\\\ell_1$ minimization, and we develop a framework in which one can understand these algorithms in terms of well-known Lagrange multiplier algorithms.',\n",
       "  'authors': ['Jian-Feng Cai 1', ' Emmanuel J. Candès 2', ' Zuowei Shen 3'],\n",
       "  'date': '2010',\n",
       "  'identifier': '2103972604',\n",
       "  'references': ['2296319761',\n",
       "   '2296616510',\n",
       "   '2145096794',\n",
       "   '2129638195',\n",
       "   '2129131372',\n",
       "   '2124608575',\n",
       "   '2115706991',\n",
       "   '403935824',\n",
       "   '2109357213',\n",
       "   '2006262045'],\n",
       "  'title': 'A Singular Value Thresholding Algorithm for Matrix Completion'},\n",
       " {'abstract': 'Decoding algorithm is a crucial part in statistical machine translation. We describe a stack decoding algorithm in this paper. We present the hypothesis scoring method and the heuristics used in our algorithm. We report several techniques deployed to improve the performance of the decoder. We also introduce a simplified model to moderate the sparse data problem and to speed up the decoding process. We evaluate and compare these techniques/models in our statistical machine translation system.',\n",
       "  'authors': ['Ye-Yi Wang ', ' Alex Waibel'],\n",
       "  'date': '1997',\n",
       "  'identifier': '2012511220',\n",
       "  'references': [],\n",
       "  'title': 'Decoding Algorithm in Statistical Machine Translation'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Theodosios Pavlidis'],\n",
       "  'date': '1977',\n",
       "  'identifier': '1530383550',\n",
       "  'references': ['1999478155',\n",
       "   '2132549764',\n",
       "   '2116341502',\n",
       "   '2042316011',\n",
       "   '2109863423',\n",
       "   '1972544340',\n",
       "   '2294922303',\n",
       "   '2131673214',\n",
       "   '1970800786',\n",
       "   '2109268594'],\n",
       "  'title': 'Structural pattern recognition'},\n",
       " {'abstract': 'Many current neurophysiological, psychophysical, and psychological approaches to vision rest on the idea that when we see, the brain produces an internal representation of the world. The activation of this internal representation is assumed to give rise to the experience of seeing. The problem with this kind of approach is that it leaves unexplained how the existence of such a detailed internal representation might produce visual consciousness. An alternative proposal is made here. We propose that seeing is a way of acting. It is a particular way of exploring the environment. Activity in internal representations does not generate the experience of seeing. The outside world serves as its own, external, representation. The experience of seeing occurs when the organism masters what we call the governing laws of sensorimotor contingency. The advantage of this approach is that it provides a natural and principled way of accounting for visual consciousness, and for the differences in the perceived quality of sensory experience in the different sensory modalities. Several lines of empirical evidence are brought forward in support of the theory, in particular: evidence from experiments in sensorimotor adaptation, visual \"filling in,\" visual stability despite eye movements, change blindness, sensory substitution, and color perception. Language: en',\n",
       "  'authors': [\"J. Kevin O'Regan 1\", ' Alva Noë 2'],\n",
       "  'date': '2001',\n",
       "  'identifier': '2109995759',\n",
       "  'references': ['2339009915',\n",
       "   '2118615399',\n",
       "   '2120019338',\n",
       "   '1950864686',\n",
       "   '1989477529',\n",
       "   '3022778360',\n",
       "   '2029740044',\n",
       "   '2006633893',\n",
       "   '2152871995',\n",
       "   '1655654231'],\n",
       "  'title': 'A sensorimotor account of vision and visual consciousness'},\n",
       " {'abstract': 'From the Publisher: Pattern recognition has long been studied in relation to many different (and mainly unrelated) applications, such as remote sensing, computer vision, space research, and medical imaging. In this book Professor Ripley brings together two crucial ideas in pattern recognition; statistical methods and machine learning via neural networks. Unifying principles are brought to the fore, and the author gives an overview of the state of the subject. Many examples are included to illustrate real problems in pattern recognition and how to overcome them.This is a self-contained account, ideal both as an introduction for non-specialists readers, and also as a handbook for the more expert reader.',\n",
       "  'authors': ['Brian D. Ripley ', ' N. L. Hjort'],\n",
       "  'date': '1996',\n",
       "  'identifier': '2117812871',\n",
       "  'references': ['2156909104',\n",
       "   '1554663460',\n",
       "   '2119821739',\n",
       "   '1988790447',\n",
       "   '2912934387',\n",
       "   '1679913846',\n",
       "   '2112076978',\n",
       "   '1971784203',\n",
       "   '2046079134',\n",
       "   '2147800946'],\n",
       "  'title': 'Pattern recognition and neural networks'},\n",
       " {'abstract': '',\n",
       "  'authors': ['Mark Claypool ',\n",
       "   ' Anuja Gokhale ',\n",
       "   ' Tim Miranda ',\n",
       "   ' Paul Murnikov ',\n",
       "   ' Dmitry Netes ',\n",
       "   ' Matthew Sartin'],\n",
       "  'date': '1999',\n",
       "  'identifier': '1510348757',\n",
       "  'references': ['3093900339',\n",
       "   '2100235918',\n",
       "   '2025605741',\n",
       "   '281665770',\n",
       "   '2112430581',\n",
       "   '2116206254',\n",
       "   '2145360759',\n",
       "   '2158515176',\n",
       "   '1542664738',\n",
       "   '2113858518'],\n",
       "  'title': 'Combining Content-Based and Collaborative Filters in an Online Newspaper'},\n",
       " {'abstract': 'A computational model is presented for the visual recognition of three-dimensional objects based upon their spatial correspondence with two-dimensional features in an image. A number of components of this model are developed in further detail and implemented as computer algorithms. At the highest level, a verification process has been developed which can determine exact values of viewpoint and object parameters from hypothesized matches between three-dimensional object features and two-dimensional image features. This provides a reliable quantitative procedure for evaluating the correctness of an interpretation, even in the presence of noise or occlusion. Given a reliable method for final evaluation of correspondence, the remaining components of the system are aimed at reducing the size of the search space which must be covered. Unlike many previous approaches, this recognition process does not assume that it is possible to directly derive depth information from the image. Instead, the primary descriptive component is a process of perceptual organization, in which spatial relations are detected directly among two-dimensional image features. A basic requirement of the recognition process is that perceptual organization should accurately distinguish meaningful groupings from those which arise by accident of viewpoint or position. This requirement is used to derive a number of further constraints which must be satisfied by algorithms for perceptual grouping. A specific algorithm is presented for the problem of segmenting curves into natural descriptions. Methods are also presented for using the viewpoint-invariance properties of the perceptual groupings to infer three-dimensional relations directly from the image. The search process itself is described, both for covering the range of possible viewpoints and the range of possible objects. A method is presented for using evidential reasoning to combine information from multiple sources to determine the most efficient ordering for the search. This use of evidential reasoning allows a system to automatically improve its performance as it gains visual experience. In summary, spatial organization and recognition are shown to be a practical basis for current systems and to provide a promising path for further development of improved visual capabilities.',\n",
       "  'authors': ['David G. Lowe'],\n",
       "  'date': '2012',\n",
       "  'identifier': '1513966746',\n",
       "  'references': ['2154422044',\n",
       "   '1874027545',\n",
       "   '2075597533',\n",
       "   '2096077837',\n",
       "   '2063549868',\n",
       "   '2156406284',\n",
       "   '2080920426',\n",
       "   '2096600681',\n",
       "   '2134927309'],\n",
       "  'title': 'Perceptual Organization and Visual Recognition'},\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "f = open('/content/data.json', \"r\") \n",
    "papers = json.load(f) \n",
    "papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbxllYOOip7y"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "این قسمت برای لود کردن اطلاعات ذخیره شده است.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xYg3fQLbSOvW",
    "outputId": "4740d289-e4c7-43f3-fd60-bbc9c30c972e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_of_all_paper = set()\n",
    "for paper in papers:\n",
    "  set_of_all_paper.add(paper[\"identifier\"])\n",
    "list_of_all_paper = list(set_of_all_paper)\n",
    "size_of_matrix = len(set_of_all_paper)\n",
    "size_of_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qe0_GBRscTnY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "s = (size_of_matrix,size_of_matrix)\n",
    "adjacency_matrix = np.zeros(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3nNs4J-dlmU",
    "outputId": "c4e03a01-5447-46a9-b3a2-1d39d5ee030d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for paper in papers:\n",
    "  index_of_adj_matrix_i = list_of_all_paper.index(paper[\"identifier\"])\n",
    "  if len(paper[\"references\"]) != 0:\n",
    "    arr_of_ref_paper = []\n",
    "    for ref_paper in paper[\"references\"]:\n",
    "      if ref_paper in list_of_all_paper:\n",
    "        arr_of_ref_paper.append(ref_paper)\n",
    "    if len(arr_of_ref_paper) != 0:\n",
    "      p = 1/len(arr_of_ref_paper)\n",
    "      for ref_paper in arr_of_ref_paper:\n",
    "        index_of_adj_matrix_j = list_of_all_paper.index(ref_paper)\n",
    "        adjacency_matrix[index_of_adj_matrix_i][index_of_adj_matrix_j] = p\n",
    "    else:\n",
    "      p = 1/len(list_of_all_paper)\n",
    "      for j in range(len(list_of_all_paper)):\n",
    "        adjacency_matrix[index_of_adj_matrix_i][j] = p\n",
    "adjacency_matrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeGHLRuUi3DE"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "در قسمت های بالا ابتدا لیست رفرنس های مقالات را به رفرنس های ولید کوتاه می کنیم و سپس ماتریس همسایگی را برای هر مقاله می سازیم.\n",
    "\n",
    "رفرنس های ولید رفرنس هایی هستند که در مقالات بررسی شده آمده باشند.\n",
    "اگر یک مقاله ای هیچ رفرنس ولیدی نداشته باشد احتمال خیلی کمی برای رفتن از آن مقاله به همه مقالات ولید در نظر میگیریم.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HgV9QVKfT2N"
   },
   "outputs": [],
   "source": [
    "def pagerank(M, num_iterations: int = 100, d: float = 0.9):\n",
    "\n",
    "    N = M.shape[1]\n",
    "    v = np.random.rand(N, 1)\n",
    "    v = v / np.linalg.norm(v, 1)\n",
    "    M_hat = (d * M + (1 - d) / N)\n",
    "    for i in range(num_iterations):\n",
    "        v = M_hat @ v\n",
    "        v = v / np.linalg.norm(v, 1)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0KpKkGEjdT3"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "الگوریتم پیج رنک\n",
    "\n",
    "reference: https://en.wikipedia.org/wiki/PageRank\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iFqEmdJuflpb",
    "outputId": "319f98c1-772c-4063-f7b3-f81f2045caca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.59229753e-05],\n",
       "       [3.52014646e-04],\n",
       "       [1.07253610e-04],\n",
       "       ...,\n",
       "       [4.47067566e-05],\n",
       "       [2.07631377e-04],\n",
       "       [7.59290033e-04]])"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = pagerank(adjacency_matrix.T, 100, 0.9)\n",
    "pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0W23tMaMjn1Q"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "الگوریتم را با ضریب dump_factor=0.9 و تعداد iteration 100 اجرا می کنیم.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rhbFyzKBttTf",
    "outputId": "e93875ac-9895-42db-8f2d-57c899468f95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008754560817332125\n",
      "0.00789117864642011\n",
      "0.0041627468899100755\n",
      "0.003991958370139756\n",
      "0.003796091460487201\n",
      "0.0032912760993457284\n",
      "0.0030552708771908986\n",
      "0.0028612525268537146\n",
      "0.002720603216990728\n",
      "0.00261321279074422\n"
     ]
    }
   ],
   "source": [
    "top_ten_score = np.sort(pr,axis=None)[::-1]\n",
    "for i in range(10):\n",
    "  print(top_ten_score[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj3_jnJVj_f7"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "۱۰ امتیاز برتر در الگوریتم پیج رنک\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cqhl5XqSh7t-",
    "outputId": "50c23f9d-d6cc-4669-aa8b-43324ea7851d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 847, 2438, 3939, ...,  917, 3666,  692])"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top = np.argsort(pr,axis=None)[::-1]\n",
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ufPqR1A-pm7L",
    "outputId": "424ffd91-c210-43e8-ecb2-baba8a2be0a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: Receptive fields, binocular interaction and functional architecture in the cat's visual cortex\n",
      "number of references to it: 26\n",
      "page rank score: 0.008754560817332125\n",
      "\n",
      "title: Single unit activity in lateral geniculate body and optic tract of unrestrained cats.\n",
      "number of references to it: 3\n",
      "page rank score: 0.00789117864642011\n",
      "\n",
      "title: Receptive fields of single neurones in the cat's striate cortex\n",
      "number of references to it: 6\n",
      "page rank score: 0.0041627468899100755\n",
      "\n",
      "title: The Nature of Statistical Learning Theory\n",
      "number of references to it: 64\n",
      "page rank score: 0.003991958370139756\n",
      "\n",
      "title: Maximum likelihood from incomplete data via the EM algorithm\n",
      "number of references to it: 104\n",
      "page rank score: 0.003796091460487201\n",
      "\n",
      "title: Statistical Power Analysis for the Behavioral Sciences\n",
      "number of references to it: 15\n",
      "page rank score: 0.0032912760993457284\n",
      "\n",
      "title: A meta-analytic test of intergroup contact theory.\n",
      "number of references to it: 1\n",
      "page rank score: 0.0030552708771908986\n",
      "\n",
      "title: A mathematical theory of communication\n",
      "number of references to it: 48\n",
      "page rank score: 0.0028612525268537146\n",
      "\n",
      "title: Gradient-based learning applied to document recognition\n",
      "number of references to it: 71\n",
      "page rank score: 0.002720603216990728\n",
      "\n",
      "title: Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations\n",
      "number of references to it: 43\n",
      "page rank score: 0.00261321279074422\n",
      "\n",
      "title: Design of Planar Rectangular Microelectronic Inductors\n",
      "number of references to it: 2\n",
      "page rank score: 0.002588298216381832\n",
      "\n",
      "title: Deep learning in neural networks\n",
      "number of references to it: 32\n",
      "page rank score: 0.002553469520809111\n",
      "\n",
      "title: Optimization by simulated annealing\n",
      "number of references to it: 55\n",
      "page rank score: 0.0025037259147486224\n",
      "\n",
      "title: Learning internal representations by error propagation\n",
      "number of references to it: 61\n",
      "page rank score: 0.0024961493691787805\n",
      "\n",
      "title: Radio engineering handbook\n",
      "number of references to it: 1\n",
      "page rank score: 0.0024094339259059334\n",
      "\n",
      "title: Edge and Curve Detection for Visual Scene Analysis\n",
      "number of references to it: 16\n",
      "page rank score: 0.0022483453555240587\n",
      "\n",
      "title: Classification and Regression Trees.\n",
      "number of references to it: 63\n",
      "page rank score: 0.0020507730932255437\n",
      "\n",
      "title: Atlas of protein sequence and structure\n",
      "number of references to it: 10\n",
      "page rank score: 0.001984635339239481\n",
      "\n",
      "title: Pattern classification and scene analysis\n",
      "number of references to it: 61\n",
      "page rank score: 0.001953727400201079\n",
      "\n",
      "title: Collective dynamics of small-world networks\n",
      "number of references to it: 42\n",
      "page rank score: 0.0019436618534924032\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_ref = np.zeros(5000)\n",
    "for paper in papers:\n",
    "  for ref_paper in paper[\"references\"]:\n",
    "    if ref_paper in list_of_all_paper:\n",
    "      count_ref[list_of_all_paper.index(ref_paper)] += 1\n",
    "for i in range(20):\n",
    "  top_id = list_of_all_paper[top[i]]\n",
    "  for paper in papers:\n",
    "    if paper[\"identifier\"] == top_id:\n",
    "      print(\"title: \" + str(paper[\"title\"]))\n",
    "      print(\"number of references to it: \" + str(int(count_ref[top[i]])))\n",
    "      print(\"page rank score: \" + str(top_ten_score[i]))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osvCjMO6kHZi"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "در این قسمت هم ۲۰ مقاله برتر به همراه عنوان و تعداد رفرنس به آنها و امتیاز page_rank آورده شده اند.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "crawl (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
